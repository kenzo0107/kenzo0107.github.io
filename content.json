{"pages":[],"posts":[{"title":"Terraform ベストプラクティス 2020 春 ~moduleやめてみた~","text":"ToC 概要 結論 module やめてみた これ modules/stg, modules/common どっち？ 問題 そもそも何故分けた？ だから module やめてみた module が便利な例 ベストプラクティスを探す旅は続く 概要#InfraStudy に刺激を受け、書きます！ 2019年に以下記事を書いてから早1年、terraform 運用歴を重ね、2020年春のベストプラクティスを更新しました。 Terraform 運用ベストプラクティス 2019 ~workspace をやめてみた等諸々~2020-05-05 追記 2020年春のベストプラクティス更新しています。 Terraform ベストプラクティス 2020 春 ~moduleやめてみた~ToC 概要 結論 module やめてみた これ modules/stg, modules/common どっち… 例によって、まず結論、 結論 module やめてみた ↓これをやめて、 1234567891011.├──envs/│ ├── bootstrap/│ ├── prd/│ └── stg/│└──modules ├── bootstrap/ ├── common/ ├── prd/ └── stg/ ↓これにした！ 12345.└──envs/ ├── bootstrap/ ├── prd/ └── stg/ module やめてみた一般的な Web サービスを構築する際に module を利用した時は以下の様に構成にしていました。 1234567891011.├──envs/│ ├── bootstrap/│ ├── prd/│ └── stg/│└──modules ├── bootstrap/ ├── common/ ├── prd/ └── stg/ bootstrap は tfstate や Lock 管理用 DynamoDB を作成します。 modules/common は、 modules/stg, modules/prd で共通で作成するリソースが置かれます。 例: ログ保存用の S3 Bucket Don't Repeat Yourself の精神で重複を避ける狙いがありました。 ですが、この構成にした場合、運用上問題が生じました。 これ modules/stg, modules/common どっち？ 問題modules/common/s3.tf で以下の様に S3 Bucket を管理しているとします 1234567resource &quot;aws_s3_bucket&quot; &quot;assets&quot; { bucket = &quot;${var.env}.${var.service_name}.assets&quot; cors_rule { max_age_seconds = 3000 }} ここで stg のみ検証の為、 max_age_seconds = 0 にしたい、としたらどうすると良いでしょう？ var.env == stg ? 1 : 0 は以前にも書きましたが、脳内リソースの消費が激しくなるので使うのを避けたいです。 modules/stg/iam.tf, modules/prd/iam.tf で処理を分ける、という案ならできそうです。 modules/stg/s3.tf 1234567resource &quot;aws_s3_bucket&quot; &quot;assets&quot; { bucket = &quot;${var.env}.${var.service_name}.assets&quot; cors_rule { max_age_seconds = 0 }} modules/prd/s3.tf 1234567resource &quot;aws_s3_bucket&quot; &quot;assets&quot; { bucket = &quot;${var.env}.${var.service_name}.assets&quot; cors_rule { max_age_seconds = 3000 }} これで var.env == stg ? 1 : 0 を回避しコード上は回避できました。 但し、お気づきの通り、 tfstate の更新が必要です。 1234567cd envs/stgterraform state rm module.common.aws_s3_bucket.assetsterraform import module.stg.aws_s3_bucket.assets stg-hoge-assetscd envs/prdterraform state rm module.common.aws_s3_bucket.assetsterraform import module.prd.aws_s3_bucket.assets stg-hoge-assets 例では 1 resource のみなので、この程度ですが、複数リソースがある場合は、複数リソースの state 削除 × 2 (stg,prd) となり、 state の更新処理が非常に面倒です。 コード的にも var.env == stg ? 1 : 0 避けたものの DRY は避けられていない。 そもそも何故分けた？modules/common が再利用性がある、とし採用しました。 ですが、例題の様に、再利用性がなくなった時のコストが大きく、また、発生しやすいことが運用でわかりました。 だから module やめてみた同じサービスで stg, prd で再利用性を求める必要が少なく、むしろ、その構成に大きく差分が生じやすい方が運用しやすいことがわかった為、最初の結論の構成としました。 12345.└──envs/ ├── bootstrap/ ├── prd/ └── stg/ module が便利な例とはいえ、 module の再利用性が発揮される効果は十分にあります。 例: 複数 AWS Account での IAM 管理 AWS Account a,b,c,d,e と複数所持し、基本、開発者は a で IAM User を発行し、他 b ~ e は a からスイッチロールできる様、 Role に紐付けする際には便利です。 123456789101112.├──envs/│ ├── a/│ ├── b/│ ├── c/│ ├── d/│ └── e/│└──modules ├── backend/ ├── iam_user/ └── iam_role/ env/a/main.tf 123456789module &quot;backend&quot; { source = &quot;../../modules/backend&quot; ...}module &quot;iam_user&quot; { source = &quot;../../modules/iam_user&quot; ...} env/b/main.tf 12345678910module &quot;backend&quot; { source = &quot;../../modules/backend&quot; ...}// Switch Role へ紐付けmodule &quot;iam_role&quot; { source = &quot;../../modules/iam_user&quot; ...} 再利用性がむしろ求められ、 module と相性がとても良いことがわかりました。 ベストプラクティスを探す旅は続く運用してみて気付く問題があり、またそれを乗り越える度にまた新たな問題に出会います。 ここ最近は 「terraform プロジェクトで Pull Request のレビュー依頼する時についついファイル数盛り盛りになっちゃう問題」がありましたw 「お互いに知ってるいつもの構成だから」という甘い考えがあるとついつい攻撃的なファイル数になっちゃったり。 人のリソースを無為に奪ってしまう行為でもあるので、git cherry-pick して小分けにする様、レビューしやすさも大事な要素だなと考えています。 また、 GitHub Actions で terraform init, plan, fmt を実行し実行結果を PR コメントに追記することで、plan 情報の確認をしやすくしたり、 fmt の違反行為で失敗させたりしています。 .github/workflows/terraform.yml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556---name: Terraformon: [pull_request]env: TF_VERSION: 0.12.24 GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }} AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}jobs: plan: name: Plan strategy: matrix: env: [stg, prd] runs-on: ubuntu-latest steps: - name: Checkout Repo uses: actions/checkout@v2 - name: Terraform Init uses: hashicorp/terraform-github-actions@master with: tf_actions_version: ${{ env.TF_VERSION }} tf_actions_subcommand: 'init' tf_actions_working_dir: 'envs/${{ matrix.env }}' tf_actions_comment: 'true' - name: Terraform Plan uses: hashicorp/terraform-github-actions@master with: tf_actions_version: ${{ env.TF_VERSION }} tf_actions_subcommand: 'plan' tf_actions_working_dir: 'envs/${{ matrix.env }}' tf_actions_comment: 'true' args: '-lock=false' fmt: name: Format runs-on: ubuntu-latest steps: - name: Checkout Repo uses: actions/checkout@v2 - name: 'Terraform Format' uses: hashicorp/terraform-github-actions@master with: tf_actions_version: ${{ env.TF_VERSION }} tf_actions_subcommand: 'fmt' tf_actions_comment: 'true' 以上参考になれば幸いです。","link":"/2020/04/24/2020-04-25-terraform-bestpractice-2020/"},{"title":"Go でタイピングゲーム作った","text":"メルカリさんのご好意で「プログラミング言語Go完全入門」の期間限定公開のお知らせがあり、早速入門しました！ 【公開期間延長 7/31まで】「プログラミング言語Go完全入門」の期間限定公開のお知らせ - Mercari Engineering Blogメルペイ エキスパートチームの@tenntennです。 メルペイでは、社外の方向けにGopher道場という体系的にGoを学べる場を無償で提供してきました。Goの普及を目的にこれまでに7回開催し100人以上の方に参加していただきました。 自社でもGoを基幹技術として使っている背景が… とても良いなと思ったのは、各章毎、学んだことをハンズオン形式でプログラミングして都度使い方を試せるところかなと思います。 そこにお題としてあった Go でタイピングゲームを作りました♪ 時間制限内に出題されたワードをタイピングし正解する毎にすこアップする、というゲームです。 kenzo0107/typing-game-goTyping Game. Contribute to kenzo0107/typing-game-go development by creating an account on GitHub. やったことお題を出すtjarratt/babble でランダムにお題とするワードを取得し標準出力 1234567891011121314151617import ( ... \"github.com/tjarratt/babble\")func init() { ... babbler = babble.NewBabbler() babbler.Count = 1 // 1ワードに設定.}// 出題func q() { question = babbler.Babble() fmt.Println(\"\\ntype this: \", question) fmt.Print(\"&gt; \")} ゲーム開始前のカウントダウン1234567891011121314func _main() { // ゲーム開始前の 3,2,1 Go 表示 countdown() ...}func countdown() { for i := 3; i &gt; 0; i-- { fmt.Print(i) fmt.Print(\" \") time.Sleep(time.Second) } fmt.Println(\"Go !\")} context.WithTimeout でタイムアウト処理for-select パターンで無限にチャネルを作り、受け取れる様にしています。時間制限 12345678910111213141516// タイムアウト処理付き contextbc := context.Background()ctx, cancel := context.WithTimeout(bc, d)defer cancel()...for { select { case &lt;-ctx.Done(): // タイムアウト fmt.Println(\"\\n\\ntime up !\") fmt.Println(\"score:\", score) return ... }} 標準入力から文字列取得し chan に送信for-select パターンが待ち受けているので、無限に標準入力からの文字列取得処理が発行されます。タイムアウト context によって、これらの処理ができなくなる様、打ち切られます。 123456789101112func input(r io.Reader) &lt;-chan string { ch := make(chan string) go func() { // 標準入力から一行ずつ文字を読み込む s := bufio.NewScanner(r) for s.Scan() { ch &lt;- s.Text() } close(ch) }() return ch} こんなことやってみたい！ ゲーム中に音を出したい PCタイピングゲームの様に敵が現れ、正しくタイピングする度に敵がダメージを受けている様なアクションをさせたい こんなことを Go でやってみたいです。 ebiten の出番か！？ 引き続き入門続けます♪ 以上参考になれば幸いです。","link":"/2020/04/29/2020-04-30-typing-game-go/"},{"title":"「購入しようとしたアイテムが見つかりませんでした」 GooglePlay","text":"概要2014.05.30時点でリリース前のドラフト版での課金テストができない仕様になりました。 その為、今までのようにテスト課金しようとすると「購入しようとしたアイテムが見つかりませんでした」と表示されるようになりました。 対応ドラフト版 or ベータ版にapkをアップロードしテスターにのみ公開する 2015/02/20 追記apkをアップロードする場合はVersionに注意してください。Version 1 (1.0) であとは課金テストが完了すればリリースする、というアプリに対してapkアップロードすることが前提です。 ちょっと直したいなぁといって、直して同じバージョンをアップロードしようとしてもアップロードできないので注意してください。 詳細① アルファ版テスト ベータ版にAPKファイルをアップロードする。※2015/02 追記 ベータ版公開でないと課金テストはできなくなったようです。 ↓ ② テスターを登録する 「テスターのリストを管理」リンク押下し、グループE-mailアドレスを登録し対象端末を招待する ③製品版にAPKファイルがアップロードされていないことを確認する製品版にAPKファイルをアップロードしているとアプリが公開されてしまう！ ④アプリを公開するアルファベータ版として公開され、テスターだけにアプリが公開される。 無事購入テスト完了♪","link":"/2014/06/15/2014-06-16-googleplay/"},{"title":"Vagrant + Chef ⇒ LAMP環境構築","text":"目的Vagrant と Chef を利用して LAMP環境をローカルの仮想環境で実現する 経緯同一サーバ上で作業している場合にMySQLのバージョンアップしてパフォーマンステストしたりしたいなぁ、でも、他のみんなに迷惑かかっちゃうよ or サーバ買ってもらえないし、というときなんかにローカル環境で試せるね ってことで導入しました。 環境 MacOSX Marvericks Vagrant VirtualBox 前提以下がインストールされていること Vagrant VirtualBox Chef knife-solo ↑ 最下の「あとがき」にあるドットインストールで導入手順を参考にしました。 1. knife-solo初期化1$ knife configure 色々尋ねられますが、基本、[Enter]連打で問題ありません。 2. BOX追加BOXとは、仮想マシン起動の際にベースとなるイメージファイルです。 centos64 という名前でboxを追加します。 1$ vagrant box add centos64 http://developer.nrel.gov/downloads/vagrant-boxes/CentOS-6.4-i386-v20131103.box ※利用しているサーバ環境と合わせたい場合は、お使いのサーバにログインして以下実行で環境情報を調べるのが良いでしょう。 1$ uname -a boxは以下からURLを選べます。http://www.vagrantbox.es/ 3. 仮想環境 初期化123$ mkdir [vagrant用ディレクトリ]$ cd [vagrant用ディレクトリ]$ vagrant init centos64 成功するとVagrantfileができているのがわかります。 12$ ls$ Vagrantfile 4. Vagrantfile修正private networkの設定をしてローカル環境からアクセス出来る様にします。 MacOS → VirtualBoxへのアクセスです。 以下のようにコメントアウトを外すのみ！ 12#config.vm.network :private_network, ip: \"192.168.33.10\"config.vm.network :private_network, ip: \"192.168.33.10\" 5. 仮想環境 起動Vagrantfileのあるパスで以下実行 1$ vagrant up 以下実行で「running(virtualbox)」を実行すると、起動中であることを確認できる。 1$ vagrant status 12345678Current machine states:default running (virtualbox)The VM is running. To stop this VM, you can run `vagrant halt` toshut it down forcefully, or you can run `vagrant suspend` to simplysuspend the virtual machine. In either case, to restart it again,simply run `vagrant up`. 以下実行でログイン 1$ vagrant ssh ログインした後、ログアウトしたい場合に以下実行 1$ exit 6. sshエイリアス作成1$ vagrant ssh-config --host [sshエイリアス] &gt;&gt; ~/.ssh/config 以下実行でアクセスできる。 1$ ssh [sshエイリアス] また、以下実行でsshのconfigに書き込まれていることを確認できる。 1$ cat ~/.ssh/config 7. Chefリポジトリ作成以下のような構成が管理しやすいかなと思いますので、以下のように作ります。 1234|+--- [vagrant用ディレクトリ]|+--- [chef-repo] [vagrant用ディレクトリ]の１つ上の階層に移動しておいて以下実行 1$ knife solo init [Chefのリポジトリ名] 8. 仮想マシンをchef対応させる12$ cd [Chefのリポジトリ名]$ knife solo prepare [sshエイリアス] 9. cookbook作成1$ knife cookbook create [cookbook名] -o site_cookbooks/ 10. cookbookに構築する環境の設定記述1$ vim [Chefのリポジトリ名]/[cookbook名]/recipe/default.rb 11. 実行recipe指定1$ vim [Chefのリポジトリ名]/nodes/[sshエイリアス名].json 12345{ \"run_list\": [ \"recipe[[cookbook名]]\" ]} 12. テンプレートを作成1$ vim [Chefリポジトリ名]\\[cookbookの名前]\\template\\default\\index.html.erb index.html.erbの中身は自由に編集してください。 例として 1&lt;h1&gt;Hello, World&lt;/h1&gt; 13. cookbookをvagrant仮想環境へ反映1$ knife solo cook [sshエイリアス名] 13. 設定した仮想環境のWebサーバへアクセスするブラウザで http://192.168.33.10 へアクセス ブラウザに以下が表示されることを確認！ 1Hello, World また、仮想環境にアクセスすると以下ファイルができていることを確認できるので試してみてください。 12345$ vagrant ssh$ cd /var/www/html/$ lsindex.html あとがきドットインストールで以下さらっておくととっつきやすいです。 essons/basic_vagrant lessons/basic_chef","link":"/2014/05/29/2014-05-30-vagrant-chef/"},{"title":"Mac OS X に wget インストール","text":"ターミナルから以下実行 curlコマンドで圧縮されたwgetモジュール（wget-x.x.x.tar.gz）をダウンロードする。curl -Oオプション 出力をオリジナルのシステムと同名のファイルに保存する 1sudo curl -O http://ftp.gnu.org/pub/gnu/wget/wget-1.13.4.tar.gz 解凍1sudo tar zxvf wget-1.13.4.tar.gz ディレクトリ内へ移動1cd wget-1.13.4 Makefileを作成Makefile … インストール対象となるシステム特有の機能／情報のチェック状況を記述している 1sudo ./configure –with-ssl=openssl –with-ssl=opensslのオプションを付けないと以下のようなエラーが出てインストールできないconfigure: error: –with-ssl was given, but GNUTLS is not available. Makefileを元にソースファイルのコンパイル1sudo make インストール1sudo make install 試しに以下実行して、「index.html」がDLされていればOK ! 1wget http://yahoo.co.jp","link":"/2014/06/17/2014-06-18-install-wget-on-macosx/"},{"title":"Mac OS X に SonarQube 導入 - UnityのC#コーディング規約チェック","text":"[f:id:kenzo0107:20140620233131p:plain] 環境 Mac OS X 10.9.3 (Marvericks) SonarQube 4.3.1 SonarRunner 2.4 概要 C#のコーディング規約チェック管理ツールを導入します。 導入手順①SonarQube, SonarRunnerをダウンロード以下SonarQube公式サイトよりSonarQube, SonarRunnerをダウンロードして下さい。※フォルダはどこに配置しても構いません。 http://www.sonarqube.org/downloads/ ※2014.06.20現在、SonarQube4.2, MySQL5.6 では起動しないことを確認しています。Homebrewでのインストールはバージョンが古いアプリケーションをDLする可能性があり、動作保証が取れない場合があります。 ②MySQL MacOSにインストール以下ブログが非常にわかりやすかったです。 http://blog.hyec.jp/2014/02/mac-os-x-mavericksmysql.html 念の為、懸念箇所はここ！ 1234...Installing MySQL, administrator password required ...Password:（ログインユーザのパスワードを入力）... 上記で入力したパスワードを元にパスワードファイルが作成されます。 ※ちなみに「seaside99」(九十九里浜)と入力しました。 ファイルを開くと「seaside99」でなく文字化け？という文字列が並んでいますが、これがrootアカウントのパスワードです。[f:id:kenzo0107:20140621002405p:plain] MySQLインストールが完了できたでしょうか？以下実行し、MySQLを起動しましょう。 1sudo /usr/local/mysql/support-files/mysql.server start ③SonarQube用のDatabase作成 以下実行し、MySQLにログイン 1/usr/local/mysql/bin/mysql -u root -p(MYSQL_PASSWORD記載のパスワード) 「sonar」データベース作成以下実行でデータベース「sonar」作成 1CREATE DATABASE sonar; 以下実行し、Databaseに「sonar」が追加されていることを確認する。 1SHOW databases; 実行結果 1234567891011mysql&gt; SHOW databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sonar | | test | +--------------------+ 5 rows in set (0.00 sec) 「sonar」ユーザを作成し、全権限を委譲・ID:sonar PW:sonar のユーザ作成 1CREATE USER 'sonar'@'localhost' IDENTIFIED BY 'sonar'; ・sonarユーザに全権限付与 1GRANT ALL PRIVILEGES ON sonar.* TO 'sonar'@'localhost'; ・権限反映 1FLUSH PRIVILEGES; ・設定完了したので終わり 12mysql&gt; exit;Bye ④sonar.properties編集1vim (SonarQubeのパス)/conf/sonar.properties ※「4.2」ディレクトリはバージョンなのでバージョンアップ時は変更してください。 以下Sonarが参照するMySQLのDB設定の為、「sonar.jdbc.url」のコメントアウトを外して有効化してください。 12# Comment the embedded database and uncomment the following line to use MySQL&lt;span style=\"color: #e2241a\"&gt;sonar.jdbc.url=jdbc:mysql://localhost:3306/sonar?useUnicode=true&amp;characterEncoding=utf8&amp;rewriteBatchedStatements=true&lt;/span&gt; ⑤sonar-runner.properties編集1vi （Sonar-Runnerのパス）/conf/sonar-runner.properties 以下SonarRunnerが参照するDB設定、アカウント情報についてコメントアウトを外し有効化してください。 12345678910111213141516#Configure here general information about the environment, such as SonarQube DB details for example#No information about specific project should appear here#----- MySQLsonar.jdbc.url=jdbc:mysql://localhost:3306/sonar?useUnicode=true&amp;characterEncoding=utf8#----- Global database settingssonar.jdbc.username=sonarsonar.jdbc.password=sonar#----- Default source code encodingsonar.sourceEncoding=UTF-8#----- Security (when 'sonar.forceAuthentication' is set to 'true')sonar.login=adminsonar.password=admin 上記設定完了後、SonarQubeを起動します。 12345678$ (SonarQubeパス)/bin/macosx-universal-64/sonar.sh consoleRunning SonarQube...wrapper | --&gt; Wrapper Started as Consolewrapper | Launching a JVM...jvm 1 | Wrapper (Version 3.2.3) http://wrapper.tanukisoftware.orgjvm 1 | Copyright 1999-2006 Tanuki Software, Inc. All Rights Reserved.jvm 1 |jvm 1 | 2014.06.25 11:00:48 INFO Web server is started 以上で MySQL, SonarQube が起動している状態になりました。 ⑥SonarQubeでのプラグイン設定 以下URLにアクセス[http://localhost:9000] 以下のようなページが表示されます。右上のLoginリンク押下します。 LoginしますLogin : adminPassword : admin 規約チェックするプラグインを設定するSettingリンク押下します。 設定ページが表示されます。 Pluginをインストールします。・C#・ReSharper・StyleCop・Japanese Pack 各リンクを押下し、installボタン押下 上記全Pluginをインストールすると以下のように表示されます。 この状態では、まだインストールが完了していません。一度、SonarQubeを再起動させる必要があります。 「sonar.sh console」を実行したターミナルで一度Ctr+CでSonarQubeをストップさせると以下結果が出力されます。 123wrapper | INT trapped. Shutting down.jvm 1 | 2014.06.22 01:48:15 INFO Web server is stoppedwrapper | &lt;-- Wrapper Stopped 再度「sonar.sh console」を実行させSonarQubeにアクセス http://localhost:9000 アップデートセンター＞Installed Plugins先ほどのPluginがインストール済みであることが確認できます。※日本語化されてます。[f:id:kenzo0107:20140622015442p:plain] ⑦コーディング規約チェックをしたいプロジェクト決定例として、以下にUnityプロジェクトを配置しているとします。~/Desktop/UnityProject/ 同階層に「sonar-project.properties」作成します。 1$ vim sonar-project.properties 内容は以下を入力してください。※sonar.sources=（Unityのcsファイル格納パスを指定してください） 12345678910111213# Project identificationsonar.projectKey=Unity:DemoAppsonar.projectVersion=1.0sonar.projectName=AppliName# Info required for Sonarsonar.sources=~/Desktop/UnityProject/Assets/Scripts# Comma-separated paths to directories with sources (required)sonar.language=cs#----- Default source code encodingsonar.sourceEncoding=UTF-8 ⑧SonarRunner実行例として、以下にSonarRunnerを配置しているとします。~/Desktop/sonar-runner-dist-2.4/ sonar-project.properties作成した同階層でsonar-runnerを実行 1~/Desktop/sonar-runner-dist-2.4/bin/sonar-runner ターミナルに以下が表示されると成功 12345678910111213141516171819INFO - 37/37 source files analyzed02:37:00.544 INFO - Sensor org.sonar.plugins.csharp.squid.CSharpSquidSensor@625cb0bb done: 1476 ms02:37:01.246 INFO - Execute decorators...02:37:05.536 INFO - Store results in database02:37:05.839 INFO - ANALYSIS SUCCESSFUL, you can browse http://localhost:9000/dashboard/index/Unity:DemoApp02:37:05.945 INFO - Executing post-job class org.sonar.plugins.core.issue.notification.SendIssueNotificationsPostJob02:37:05.974 INFO - Executing post-job class org.sonar.plugins.core.batch.IndexProjectPostJob02:37:06.539 INFO - Executing post-job class org.sonar.plugins.dbcleaner.ProjectPurgePostJob02:37:06.552 INFO - -&gt; Keep one snapshot per day between 2014-05-25 and 2014-06-2102:37:06.553 INFO - -&gt; Keep one snapshot per week between 2013-06-23 and 2014-05-2502:37:06.554 INFO - -&gt; Keep one snapshot per month between 2009-06-28 and 2013-06-2302:37:06.554 INFO - -&gt; Delete data prior to: 2009-06-2802:37:06.560 INFO - -&gt; Clean AppliName [id=371]INFO: ------------------------------------------------------------------------INFO: EXECUTION SUCCESSINFO: ------------------------------------------------------------------------Total time: 24.508sFinal Memory: 6M/87MINFO: ------------------------------------------------------------------------ SonarQubeにアクセスするとProjects項目にsonar-project.propertiesで指定したsonar.projectNameリンクが表示されているのでリンク押下 コーディング規約のチェックが表示されます。 まずここまで出来たらあとは規約のカスタマイズになります。本件は以上までとします。 カスタマイズはメニューの品質プロファイル＞ Sonar Wayと進み、設定変更してみましょう。 補足元々UnityのC#コーディング規約チェック実施する為に導入を試みましたが、C#だけでなくJava, PHP, Python等かなり守備範囲が広いです。 他ブログではWindowsが多く、推奨環境としてはWindowsですがMacOS Xで利用したい場合を本記事はまとめました。 Jenkins用Plugin [Sonar Plugin]も用意されており定期チェックや他Jobと組み合わせてキックが可能です。","link":"/2014/06/21/2014-06-22-sonarqube-maxosx/"},{"title":"Apache + OpenSSL でSHA256対応CSR生成","text":"ドメイン登録者情報確認※特にドメイン登録者を明確に指定する必要がなければ、「手順」へ進んでください。 企業認証周りが必要な場合は事前にWHOIS(フーイズ)でドメイン登録者情報を確認しておくとスムーズです。 http://whois.jprs.jp/ 検索ワードにドメイン入力して検索すると登録者情報を確認できます。 CSR生成手順","link":"/2015/02/28/2015-03-01-apache-openssl-sha256/"},{"title":"Ruby インストール on CentOS7","text":"環境 AWS CentOS Linux release 7.0.1406 (Core) 必要モジュールインストール1$ sudo yum -y install git libffi libffi-dev gcc openssl-devel readline-devel zlib-devel rbenv, ruby-build ダウンロード12$ git clone https://github.com/sstephenson/rbenv.git ~/.rbenv$ git clone https://github.com/sstephenson/ruby-build.git ~/.rbenv/plugins/ruby-build rbenv PATH指定1234$ echo 'export PATH=&quot;$HOME/.rbenv/bin:$PATH&quot;' &gt;&gt; ~/.bash_profile$ echo 'eval &quot;$(rbenv init -)&quot;' &gt;&gt; ~/.bash_profile$ exec $SHELL$ source ~/.bash_profile 123456789101112$ rbenv install -listAvailable versions: : : 2.0.0-p643 2.0.0-p645 ← 2.0.0の最新 (※2015/08/06時点) 2.1.0-dev 2.1.0-preview1 2.1.0-preview2 2.1.0-rc1 : ruby インストール1$ rbenv install -v 2.0.0-p645 再読込1$ rbenv rehash インストールしたバージョンに切り替え1$ rbenv global 2.0.0-p645 確認12$ ruby -vruby 2.0.0p645 (2015-04-13 revision 50299) [x86_64-linux] 以上","link":"/2015/08/05/2015-08-06-install-ruby-on-centos7/"},{"title":"go get で 「exec: &quot;bzr&quot;: executable file not found in $PATH」エラー対策","text":"結論bzrモジュールインストールした後、再度go getして問題なく動作しました。 1# yum install -y bzr 概要EC2のCentOS7で以下のように go get した際にbzrが見当たらないというエラーが発生した。 12345$ go get launchpad.net/goamz/aws[centos@ip-xxx-xx-xx-xx src]$ go get launchpad.net/goamz/awsgo: missing Bazaar command. See http://golang.org/s/gogetcmdpackage launchpad.net/goamz/aws: exec: &quot;bzr&quot;: executable file not found in $PATH ちょこっとしたつまづきでした。 ちなみにbzrはgitと同様、分散バージョン管理システムです。 今回インストールしようとしたPackageがBazaarで管理していたから必要になったのでしょう。 以下参照Bazaarユーザーリファレンス","link":"/2015/08/18/2015-08-19-go-get-executable-file-not-found/"},{"title":"Golang Revelフレームワークエラー 「server.go:1775: http: TLS handshake error from 127.0.0.1:36799: tls: first record does not look like a TLS handshake」対応","text":"結論[prod]モード以外で http.ssl = true とし起動すると発生するエラーでした。その為、[prod]以外では http.ssl = false として起動することとしました。 概要テスト環境で app.conf にて[test]モードを追加しそこでオレオレSSL証明書作成してhttpsリンクの動作確認をしようとしてました。 ですが、起動こそするものの一向にアクセスできず汗 devモードではアクセス出来るのでその差分を見たところ、上記の結論に至りました。 エラー内容revel 起動しurlにアクセスすると以下のようなTLSハンドシェイクができずアクセスできない状況になる。 1234567$ revel run myapp test......INFO 2015/08/19 14:48:38 harness.go:165: Listening on :90002015/08/19 14:48:40 server.go:1775: http: TLS handshake error from 127.0.0.1:36799: tls: first record does not look like a TLS handshake","link":"/2015/08/18/2015-08-19-go-revel-fw-tls-handshake-error/"},{"title":"go-sql-driver&#x2F;mysqlでcreated_at (datetime) がUTCで登録されてしまう件","text":"環境 go-sql-driver/mysql version 1.2 結論以下のように parseTime=trueとloc=Asia%2FTokyoを設定する。 1db, err := sql.Open(&quot;mysql&quot;, &quot;user:passward@/dbname?parseTime=true&amp;loc=Asia%2FTokyo&quot;) loc=xxxxの指定がない場合、localが指定され、datetimeにtime.Now()を指定してINSERTしてもUTC時間に書き直されてしまう。 ちょっとハマりました。","link":"/2015/08/18/2015-08-19-gosql-driver-mysql-created_at-utc/"},{"title":"Go初心者におすすめ本","text":"概要身の回りであまりGoを積極的にやっている人がいないのでやってみたいなぁ、気になってはいる、という方に本をおすすめしてみたいと思いました。 この一冊！以下をチョイスしました！ WEB+DB PRESS Vol.82 | 山口 徹, Jxck, 佐々木 大輔, 横路 隆, 加来 純一, 山本 伶, 大平 武志, 米川 健一, 坂本 登史文, 若原 祥正, 和久田 龍, 平栗 遵宜, 伊藤 直也, 佐藤 太一, 高橋 俊幸, 海野 弘成, 五嶋 壮晃, 佐藤 歩, 吉村 総一郎, 橋本 翔, 舘野 祐一, 中島 聡, 渡邊 恵太, はまちや2, 竹原, 河合 宜文, WEB+DB PRESS編集部 |本 | 通販 | AmazonAmazonで山口 徹, Jxck, 佐々木 大輔, 横路 隆, 加来 純一, 山本 伶, 大平 武志, 米川 健一, 坂本 登史文, 若原 祥正, 和久田 龍, 平栗 遵宜, 伊藤 直也, 佐藤 太一, 高橋 俊幸, 海野 弘成, 五嶋 壮晃, 佐藤 歩, 吉村 総一郎, 橋本… よかった点 全部で34ページ！1日で読み終わり、逆引きとしても利用できる。(Goのオフィシャルのチュートリアルをまとめてくれてる感じ) Goの成り立ちを簡潔に説明しており、クロスコンパイルや並列処理サポートなどがありGoの使いどころがわかる。 ちょうど「Web API デザインの鉄則」という特集があり、API開発にはちょうど良い内容の一冊だった。 特に本を紹介して自分にお金が入ることはないので斜めから見ずにまず本見てみてください♪ Goを使いたくなる記事JSONシリアライズし秒間のレスポンス回数測定 以下Qiita記事でまとめてくれていました。 「最速」フルスタックWebフレームワーク「revel」の紹介 個人的に好きな所 while や do/while 、参考演算子がなくforのみ、とか冗長なコーディングを許さない gofmtでフォーマット化してくれる。いつもコミット前にやってます。 ライブラリ増えてる。 Pythonのpip installみたく go getでインストール可。 ネイティブなのでアプリケーションが大きくなってもさほど遅くならない。 以上Gopherとお話できる機会が増えると何よりです。","link":"/2015/08/19/2015-08-20-beginner-gopher/"},{"title":"Kibana4 インストール on CentOS7","text":"前提 Nginx インストール済み 環境 CentOS Linux release 7.0.1406 (Core) Nginx 1.9.3 Kibana 4.1.1 Kibana インストール123$ cd /usr/local/src$ sudo wget https://download.elastic.co/kibana/kibana/kibana-4.1.1-linux-x64.tar.gz$ sudo tar xvzf kibana-4.1.1-linux-x64.tar.gz Kibana 設定ファイル修正1$ vi /usr/local/src/kibana-4.1.1-linux-x64/config/kibana.yml host指定 デフォルトでは 0.0.0.0 となっています。今回は同サーバに構築するので localhost とし保存します。 12- host: &quot;0.0.0.0&quot;+ host: &quot;localhost&quot; 上記の設定により、kibanaがlocalhostにアクセスできるようになります。 Kibana 実行パス作成12$ sudo mkdir -p /opt/kibana$ sudo cp -R /usr/local/src/kibana-4.1.1-linux-x64/* /opt/kibana/ 上記設定により以下コマンドでkibana実行可能になります。 1/opt/kibana/bin/kibana 上記でも十分ですが、再起動した際にはkibanaは停止してしまいます。なので、起動スクリプトを作成し、サービス登録します。 起動スクリプト作成1$ sudo vi /etc/systemd/system/kibana4.service kibana4.service 123456789101112[Service]ExecStart=/opt/kibana/bin/kibanaRestart=alwaysStandardOutput=syslogStandardError=syslogSyslogIdentifier=kibana4User=rootGroup=rootEnvironment=NODE_ENV=production[Install]WantedBy=multi-user.target 上記保存します。 起動/サービス登録12$ sudo systemctl start kibana4$ sudo systemctl enable kibana4 Nginx インストール以下参考にしてください。 CentOS7 に Nginx インストール - 長生村本郷Engineers'BlogKibana構築用に書いときました。 環境 CentOS7 Nginxインストール Nginx用リポジトリ作成 # vim /etc/yum.repos.d/nginx.repo 以下追記 [nginx] name=nginx repo baseurl=http://nginx.… Nginx設定ファイル修正リーバスプロキシでKibana4用ポート(5601)へ向ける。 1$ sudo vi /etc/nginx/conf.d/default.conf 1234567891011121314151617server { listen 80; server_name ec2-xx-xx-xxx-xxx.ap-northeast-1.compute.amazonaws.com; auth_basic &quot;Restricted Access&quot;; auth_basic_user_file /etc/nginx/htpasswd.users; location / { proxy_pass http://localhost:5601; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; }} 以上設定保存後、Nginx再起動 1$ sudo systemctl restart nginx アクセスして確認以下のように表示されれば問題ありません。 以上 参考記事Systemd入門(4) - serviceタイプUnitの設定ファイル 起動スクリプトについて 各項目説明[Unit] について オプション 説明 Description Unitの説明文 Documentation ドキュメントのURI Requires このUnitが必要とする前提Unit Wants このUnitが必要とする前提Unit After このUnitより先に起動するべきUnit Before このUnitより後に起動するべきUnit [Install] について オプション 説明 WantedBy enable時にこのUnitの.wantsディレクトリにリンク作成 RequiredBy enable時にこのUnitの.requiredディレクトリにリンク作成 Also enable/disable時に同時にenable/disableするUnit Alias enable時にこのUnitの別名を用意 ↓この本とってもお世話になりました♪ サーバ/インフラエンジニア養成読本 ログ収集~可視化編 [現場主導のデータ分析環境を構築!] (Software Design plus) | 鈴木 健太, 吉田 健太郎, 大谷 純, 道井 俊介 |本 | 通販 | AmazonAmazonで鈴木 健太, 吉田 健太郎, 大谷 純, 道井 俊介のサーバ/インフラエンジニア養成読本 ログ収集~可視化編 [現場主導のデータ分析環境を構築!] (Software Design plus)。アマゾンならポイント還元本が多数。鈴木 健太, 吉田 健太郎, 大谷 純…","link":"/2015/08/19/2015-08-20-install-kibana-on-centos7/"},{"title":"CentOS7 に Nginx インストール","text":"Kibana構築用に書いときました。 環境 CentOS7 Nginxインストール Nginx用リポジトリ作成 1# vim /etc/yum.repos.d/nginx.repo 以下追記 12345[nginx]name=nginx repobaseurl=http://nginx.org/packages/mainline/centos/7/$basearch/gpgcheck=0enabled=1 Nginx モジュールインストール1$ sudo yum install --enablerepo=nginx nginx サービス登録サーバ起動時・再起動時にNginxが起動するようにします。 12345678910111213141516171819202122$ sudo systemctl disable httpd$ sudo systemctl enable nginx$ sudo systemctl start nginx$ systemctl status nginxnginx.service - nginx - high performance web server Loaded: loaded (/usr/lib/systemd/system/nginx.service; enabled) Active: active (running) since Mon 2015-08-03 06:07:44 UTC; 2s ago Docs: http://nginx.org/en/docs/ Process: 12642 ExecStart=/usr/sbin/nginx -c /etc/nginx/nginx.conf (code=exited, status=0/SUCCESS) Process: 12641 ExecStartPre=/usr/sbin/nginx -t -c /etc/nginx/nginx.conf (code=exited, status=0/SUCCESS) Main PID: 12645 (nginx) CGroup: /system.slice/nginx.service ├─12645 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.... └─12646 nginx: worker processAug 03 06:07:44 ip-172-31-19-253 systemd[1]: Starting nginx - high performan....Aug 03 06:07:44 ip-172-31-19-253 nginx[12641]: nginx: the configuration file...kAug 03 06:07:44 ip-172-31-19-253 nginx[12641]: nginx: configuration file /et...lAug 03 06:07:44 ip-172-31-19-253 systemd[1]: Failed to read PID from file /r...tAug 03 06:07:44 ip-172-31-19-253 systemd[1]: Started nginx - high performanc....Hint: Some lines were ellipsized, use -l to show in full. アクセス確認http://&lt;IPアドレス or ドメイン&gt; 以下のように表示されればOK 以上です。","link":"/2015/08/19/2015-08-20-install-nginx-on-centos7/"},{"title":"fluentd設定ハマった所あるある","text":"以下随時追加。 環境 CentOS Linux release 7.1.1503 (Core) Fluentd 0.12.12 Nginx 1.8.0 Permission denined パーミッションエラー！ 1234# tail -f /var/log/td-agent/td-agent.log2015-08-19 14:17:14 +0900 [error]: Permission denied @ xxxxxxx - /var/log/nginx/error.log 2015-08-19 14:17:14 +0900 [error]: suppressed same stacktrace 対策td-agent実行ユーザをrootに変更する。 1234567$ sudo vim /etc/init.d/td-agent- TD_AGENT_USER=td-agent- TD_AGENT_GROUP=td-agent+ TD_AGENT_USER=root+ TD_AGENT_GROUP=root デーモンリロード 1sudo systemctl daemon-reload 動作確認以下のようにtailが正しく実行できていることが確認できます。 1234# tail -f /var/log/td-agent/td-agent.log2015-08-19 14:17:15 +0900 [info]: following tail of /var/log/nginx/access.log2015-08-19 14:17:15 +0900 [info]: following tail of /var/log/nginx/error.log [warn]: pattern not matchこれかなりハマりました。 Nginxのlogを流すときに以下のようにfomatするように書かれている記事を多く見たので設定してみたらエラー発生(; &gt;_&lt;) /etc/td-agent/td-agent.conf 1234567&lt;source&gt; type tail format nginx path /var/log/nginx/access.log pos_file /var/log/td-agent/nginx-access.pos tag nginx.access&lt;/source&gt; 対策以下のように修正 /etc/td-agent/td-agent.conf 12345678&lt;source&gt; type tail format /^(?&lt;remote&gt;[^ ]*) (?&lt;host&gt;[^ ]*) (?&lt;user&gt;[^ ]*) \\[(?&lt;time&gt;[^\\]]*)\\] &quot;(?&lt;method&gt;\\S+)(?: +(?&lt;path&gt;[^ ]*) +\\S*)?&quot; (?&lt;code&gt;[^ ]*) (?&lt;size&gt;[^ ]*)(?: &quot;(?&lt;referer&gt;[^\\&quot;]*)&quot; &quot;(?&lt;agent&gt;[^\\&quot;]*)&quot; &quot;(?&lt;forwarder&gt;[^\\&quot;]*)&quot;)?/ time_format %d/%b/%Y:%H:%M:%S %z path /var/log/nginx/access.log pos_file /var/log/td-agent/nginx-access.pos tag nginx.access&lt;/source&gt; td-agent再起動 1$ sudo systemctl restart td-agent これで大丈夫。 buffer_path 重複1[error]: failed to configure sub output redshift: Other '' plugin already use same buffer_path: type = , buffer_path = * 元々の設定は以下の様にしていました。td-agent の送信先にトラブルがあり buffer が溜まり重複しエラーとなっていました。 1buffer_path /logs/td-agent/nginx/logs 例）以下の様な tag があった場合、buffer_path は同じく /logs/td-agent/nginx/logs となってしまう hogehoge.20170101.loghogehoge.20170102.log 対策tag_parts を用い、以下の様に tag 毎に buffer_path をユニークにすることで解決 1buffer_path /logs/td-agent/nginx/logs_${tag_parts[0]}_${tag_parts[1]} 引き続き何か発生したら追記していきます。","link":"/2015/08/20/2015-08-21-fluentd-failpattern/"},{"title":"Golang Revelフレームワーク ホットデプロイ方法","text":"概要Revel Officialサイトにあるデプロイ方法を検証しました。 Revel Deployment ローカルでアプリをビルドしサーバにコピーする サーバーで更新したコードをpullし、ビルド・起動する Heroku を利用しデプロイ管理する 1. ローカルビルド123456789101112131415# アプリを実行しテストする$ revel run import/path/to/app# パッケージ化$ revel package import/path/to/appYour archive is ready: app.tar.gz# 対象マシンへコピー$ scp app.tar.gz target:/srv/# ターゲットマシンで起動$ ssh target$ cd /srv/$ tar xzvf app.tar.gz$ bash run.sh 開発しているアーキテクチャと同じ環境へデプロイする場合、もしくは go インストールを設定し、デフォルトで希望するアーキテクチャへビルドするための唯一の方法です。 2. 追加デプロイメントフルセットアセット付きの静的にリンクされたバイナリが巨大になる可能性があるので、追加デプロイをサポートしています。 12345678# アプリを一時ディレクトリにビルド$ revel build import/path/to/app /tmp/app# サーバのhomeディレクトリにその一時ディレクトリにrsync$ rsync -vaz --rsh=&quot;ssh&quot; /tmp/app server# サーバーに接続しアプリを再起動... rsyncはsshでのコピーをサポートしているので、以下のように複雑にはなりますがデプロイ可能です。 12# カスタム証明書、ログイン名、対象ディレクトリを指定しrsync$ rsync -vaz --rsh=&quot;ssh -i .ssh/go.pem&quot; /tmp/myapp2 ubuntu@ec2-50-16-80-4.compute-1.amazonaws.com:~/rsync 3. サーバでビルドこの方法はバージョンコントロールシステムに依存します。Goをインストールしているサーバーが必要です。その代わり、クロスコンパイルを回避することができます。 12345678$ ssh server... install go ...... configure your app repository ...# appディレクトリに移動し pullしサーバー起動する$ cd gocode/src/import/path/to/app$ git pull$ revel run import/path/to/app prod 総評現在社内では、1. ローカルビルド のデプロイ方法を選択しています。 2. 追加デプロイメント は、再起動時の瞬断が懸念されます。大規模アクセスを捌く目的で利用するGoには向いていないと思います。 3. サーバでビルド は、運用方法にもよりますが、本番環境でコンフリクトが起きてしまったらコンパイルもストップしてしまう懸念があります。 1. ローカルビルド方法をJenkinsで管理して運用しており現状特に問題ないです。 Jenkinsの設定等まとめてから公開したいと思います。","link":"/2015/08/11/2015-08-21-go-revel-fw-hotdeploy/"},{"title":"Go Revelフレームワーク jQuery非同期処理","text":"概要Go RevelフレームワークでAjax非同期処理を実装します。 CSRF 対策以下コマンドによりRevelフレームワークでCSFR対策する為のライブラリをインストール 1$ go get github.com/cbonello/revel-csrf app/init.go ajax実行時にCSRFチェックする為、init.goでのチェックを外すように設定 init.goGitHub Gist: instantly share code, notes, and snippets. 以下CSRFのFilter設定 123456func init() revel.Filters = []revel.Filter{ ... ... csrf.CSRFFilter, // CSRF prevention. ... conf/routes で実行するAPIのURLはAjax実行時にCSRFチェックする為、init.goではチェックを対象外とする様に設定 1csrf.ExemptedFullPath(\"/api_execute\") View側の設定 views/header.html &lt;head&gt;〜&lt;/head&gt; 内にCSRFチェック用ハッシュ値をmeta情報として埋め込みます。 1&lt;meta name=\"csrf-token\" content=\"{{ .csrf_token }}\"&gt; jQueryファイル123456789101112131415161718192021222324252627282930313233function setAjaxToken( token ) { // ajax --- start -------------------------------------------------- $.ajaxSetup({ crossDomain: false, beforeSend: function(xhr, settings) { if (!csrfSafeMethod(settings.type)) { xhr.setRequestHeader(\"X-CSRFToken\", token ); } } });}$(document).ready(function () { $(document).on(\"click\", \".ajax_execute\", function (event) { event.preventDefault(); setAjaxToken( postData['_token'] ); var ajaxParamas = {}; ajaxParamas[\"type\"] = \"POST\"; ajaxParamas[\"url\"] = action; ajaxParamas[\"data\"] = postData; ajaxParamas[\"cache\"] = false; ajaxParamas[\"dataType\"] = \"json\"; $.ajax(ajaxParamas) .success( function(res) { console.log(\"(^-^) OK\") }).error ( function() { console.log(\"(&gt;_&lt;) NG\") }); return false;}) 簡単…とは行ってない気がしますがCSRF対策ができました。","link":"/2015/08/29/2015-08-30-go-revel-fw-jquery/"},{"title":"Nginx ssl.conf設定 特定ページのみhttpsへリダイレクトさせる設定","text":"概要Go RevelフレームをNginx上で利用しています。 その際に設定したssl設定、及び、特定ページのみhttpsへリダイレクトする様な設定をする必要があったので以下まとめました。 https通信したいのページは以下です。 /register /mypage /login /logout https通信したいページをlocationの正規表現(~)でprefixを引っ掛けてhttp通信の場合、https通信に301リダイレクトさせてます。 以上","link":"/2015/09/13/2015-09-14-nginx-redirect-https/"},{"title":"3分で出来る！ AWS EC2(CentOS7)に td-agent2インストール","text":"環境 AWS EC2 CentOS Linux release 7.1.1503 (Core) td-agent2 インストール1$ sudo curl -L http://toolbelt.treasuredata.com/sh/install-redhat-td-agent2.sh | sh 起動/サービス登録12$ sudo systemctl start td-agent$ sudo chkconfig td-agent on systemctl enableするとchkconfig使ってと怒られます。 1234567891011121314$ sudo systemctl enable td-agenttd-agent.service is not a native service, redirecting to /sbin/chkconfig.Executing /sbin/chkconfig td-agent onThe unit files have no [Install] section. They are not meant to be enabledusing systemctl.Possible reasons for having this kind of units are:1) A unit may be statically enabled by being symlinked from another unit's .wants/ or .requires/ directory.2) A unit's purpose may be to act as a helper for some other unit which has a requirement dependency on it.3) A unit may be started when needed via activation (socket, path, timer, D-Bus, udev, scripted systemctl call, ...).[root@ip-172-31-19-253 log]# 試験設定ファイル (/etc/td-agent/td-agent.conf)を見るとデフォルト設定では、httpプロトコルでport:8888からLoggingしtd-agent.log(/var/log/td-agent/td-agent.log)に流すようにしています。 123456789101112131415# HTTP input# POST http://localhost:8888/&lt;tag&gt;?json=&lt;json&gt;# POST http://localhost:8888/td.myapp.login?json={&quot;user&quot;%3A&quot;me&quot;}# @see http://docs.fluentd.org/articles/in_http&lt;source&gt; type http port 8888&lt;/source&gt;## live debugging agent&lt;source&gt; type debug_agent bind 127.0.0.1 port 24230&lt;/source&gt; 以下のようにコマンドを実行してtd-agent.logを確認してみる。 12345$ curl -X POST -d 'json={&quot;json&quot;:&quot;TEST!!&quot;}' http://localhost:8888/debug.test$ sudo tail -f /var/log/td-agent/td-agent.log2015-09-19 17:34:50 +0900 debug.test: {&quot;json&quot;:&quot;TEST!!&quot;} 上記のように正しくロギングされていることが確認できました。","link":"/2015/09/18/2015-09-19-install-td-agent2-centos7/"},{"title":"エラーログをSlack通知する","text":"環境 CentOS Linux release 7.1.1503 (Core) td-agent: 0.12.12 Nginx: 1.8.0 概要社内でSlackによる連携が進みログ管理もfluentdにまとめつつあるのでエラーログで何かあったらSlack通知させようと思いチャレンジ♪ 以下のような流れを想定しています。 Nginx error.log —&gt; fluentd —&gt; slack gemでも作るか！と思ったら既にSlack Pluginは豊富なのであやからせていただきます！ 準備fluentdが/var/log/nginx/error.log にアクセスできるようにしておいてください。 ログがtailできないなんてときは以下参照♪ fluentd設定ハマった所あるある - 長生村本郷Engineers'Blog以下随時追加。 環境 CentOS Linux release 7.1.1503 (Core) Fluentd 0.12.12 Nginx 1.8.0 Permission denined パーミッションエラー！ # tail -f /var/log/td-agent/td-ag… 利用するfluentd Plugin sowawa/fluent-plugin-slack sonots/fluent-plugin-record-reformer fluent/fluent-plugin-rewrite-tag-filter td-agent.conf設定 error.logフォーマット設定 Nginxのエラーログが以下のようにフォーマットされ出力されているとします。 12015/11/18 18:01:47 [error] 23029#0: *9086 open() &quot;/var/golang/src/img/tmp.png&quot; failed (2: No such file or directory), client: ***.**.**.****, server: hogehoge.jp, request: &quot;GET /img/tmp.png HTTP/2.0&quot;, host: &quot;hogehoge.jp&quot;, referrer: &quot;http://hogehoge.jp&quot; fluentdのformat設定 1format /^(?&lt;time&gt;.+) \\[(?&lt;level&gt;[^\\]]+)\\] *(?&lt;message&gt;.*)$/ 上記fomat設定によって以下のように key : value 構成で取得できます。 123time : 2015/11/18 18:01level : errormessage : or] 23029#0: *9086 open() &quot;/var/www/html/img/tmp.png&quot; failed (2: No such file or directory), client: ***.**.**.****, server: hogehoge.jp, request: &quot;GET /img/tmp.png HTTP/2.0&quot;, host: &quot;hogehoge.jp&quot;, referrer: &quot;http://hogehoge.jp&quot; 設定によってはうまく通知させずハマりました汗 tag名をリライト上記 で取得した key:value を元に tagを書き換えます。 以下の例だと、level が error の場合、 slack.error.${tag} (slack.error.nginx.error) にタグを書き換えてます。他、warn, fatal も同様です。 123456&lt;match nginx.error&gt; type rewrite_tag_filter rewriterule1 level error slack.error.${tag} rewriterule2 level warn slack.warn.${tag} rewriterule3 level fatal slack.fatal.${tag}&lt;/match&gt; また message で取得した値の中に特定文字列が含まれている場合等も可能です。 例) message に 「PHP Fatal Error」 で始まる文字列が含まれている場合にslack.fatal.${tag}に書き換える。 12345678&lt;match nginx.error&gt; type rewrite_tag_filter rewriterule1 level error slack.error.${tag} rewriterule2 level warn slack.warn.${tag} rewriterule3 level fatal slack.fatal.${tag} rewriterule4 message ^PHP Fatal Error.*$ slack.fatal.${fatal} # 追加&lt;/match&gt; フィールド追加 source_id 追加 time, level, message 以外に source_id を追加してます。以下の例では source_id` に tag_suffix[1] を指定しています。 1234567&lt;match slack.**&gt; type record_reformer tag reformed.${tag} &lt;record&gt; source_id ${tag_suffix[1]} &lt;/record&gt;&lt;/match&gt; tag_suffix についてtag が reformed.slack.error.nginx.error とすると以下のような仕様です。 123456tag_suffix[0] → reformed.slack.error.nginx.errortag_suffix[1] → slack.error.nginx.errortag_suffix[2] → error.nginx.errortag_suffix[-1] → errortag_suffix[-2] → nginx.error slack通知 incoming WebhookからWebhookURLを設定 12345678910&lt;match reformed.slack.**&gt; type slack webhook_url https://hooks.slack.com/services/xxxxxxxxx/xxxxxxxxx/xxxxxxxxxxxxxxxxxxxxxxxx channel flag_production username fluentd title_keys source_id title %s color danger flush_interval 5s&lt;/match&gt; 通知結果 余談Slackに通知しても休日で業務連絡を見ないということは往々にしてあるのでTwillioで電話通知するpluginもあります。 y-ken/fluent-plugin-twilio 株式会社KDDIウェブコミュニケーションズが提供する有料サービスです。比較的安価なので導入検討してみてください。 Twillio 料金表twilio price かつての「メール見てませんでした💦」なんてことがなくなりそうなのは良いですね 以上です。","link":"/2015/09/23/2015-09-24-errorlog-to-slack/"},{"title":"NginxでGeoIP設定しアクセスログにアクセスポイントを追加する","text":"概要GeoIP Libraryを設定後、access.logにアクセスポイントを表示 GeoIP データファイル取得GeoIPライブラリを提供しているMaxMindサイトから取得可能です。有料版もありますが、取り急ぎは無料版で試します。 123456# mkdir -p /usr/share/GeoIP/# cd /usr/share/GeoIP/# wget -N http://geolite.maxmind.com/download/geoip/database/GeoLiteCountry/GeoIP.dat.gz# wget http://geolite.maxmind.com/download/geoip/database/GeoLiteCity.dat.gz# gunzip GeoIP.dat.gz# gunzip GeoLiteCity.dat.gz nginx.conf設定※ログのフォーマットはltsvにしています。 1# vim /etc/nginx/nginx.conf 1234567891011121314151617181920212223242526272829303132333435363738394041http { ... ... geoip_country /usr/share/GeoIP/GeoIP.dat; geoip_city /usr/share/GeoIP/GeoLiteCity.dat; log_format ltsv 'time:$time_iso8601\\t' 'remote_addr:$remote_addr\\t' 'request_method:$request_method\\t' 'request_length:$request_length\\t' 'request_uri:$request_uri\\t' 'https:$https\\t' 'uri:$uri\\t' 'query_string:$query_string\\t' 'status:$status\\t' 'bytes_sent:$bytes_sent\\t' 'body_bytes_sent:$body_bytes_sent\\t' 'referer:$http_referer\\t' 'useragent:$http_user_agent\\t' 'forwardedfor:$http_x_forwarded_for\\t' 'request_time:$request_time\\t' 'upstream_response_time:$upstream_response_time\\t' 'host:$host\\t' # geoIP setting --- start --- 'geoip_country_name:$geoip_city_country_name\\t' # 国名 'geoip_country_code3:$geoip_city_country_code3\\t' # JPNとかUSAとか 'geoip_city:$geoip_city\\t' # 都市名 'geoip_latitude:$geoip_latitude\\t' # 緯度 'geoip_longitude:$geoip_longitude'; # 経度 # geoIP setting --- end --- access_log /var/log/nginx/access.log ltsv; ... ... include /etc/nginx/conf.d/*.conf;} Nginx再起動1# systemctl restart nginx アクセスログ確認123# tail -f /var/log/nginx/access.logtime:2015-10-01T18:01:48+09:00 remote_addr:xxx.xxx.xx.xx request_method:GET request_length:882 request_uri:/public/img/icon/favicon.ico https: uri:/public/img/icon/favicon.ico query_string:- status:200 bytes_sent:4791 body_bytes_sent:4096 referer:http://theflag.jp/ useragent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36 forwardedfor:- request_time:0.001 upstream_response_time:0.001 host:theflag.jp geoip_country_name:Japan geoip_country_code3:JPN geoip_city:Tokyo geoip_latitude:35.6850 geoip_longitude:139.7514 取得できました♪ これをfluentdで流してKibanaでかっこよく表示しましょう。 以上です。","link":"/2015/09/30/2015-10-01-nginx-geip/"},{"title":"インストール済みNginx にgeoIP Library追加","text":"概要元々yumでNignxをインストールしていましたがIP制限やfluentdでip情報を割り出す必要がありgeoIPが必要になりました。 その為には新たにNginxをソースからダウンロードしLibrary追加しコンパイルし直す必要があります。 geoIPインストール理由 ログにip情報を付加しfluentdで解析する。 IP許可制限が可能になる。 海外から不正アクセスがあった為対応しました。 環境 CentOS7 Nginx1.8 EC2 現状のNginxの利用可能モジュール確認1234567# nginx -Vnginx version: nginx/1.8.0built by gcc 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC)built with OpenSSL 1.0.1e-fips 11 Feb 2013TLS SNI support enabledconfigure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_random_index_module --with-http_secure_link_module --with-http_stub_status_module --with-http_auth_request_module --with-mail --with-mail_ssl_module --with-file-aio --with-ipv6 --with-http_spdy_module --with-cc-opt='-O2 -g -pipe -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic' --with-http_geoip_moduleがない！ ので geoIPをインストールしてNginxをコンパイルし直します。 GeoIP Libraryインストール1$ sudo yum install -y GeoIP GeoIP-devel 同バージョンのNginxのtar.gzをダウンロード 以下からダウンロードNginx Download ※今回はNginxは1.8.0なのでnginx-1.8.0.tar.gzをダウンロードします。 1234$ cd /usr/local/src$ wget http://nginx.org/download/nginx-1.8.0.tar.gz$ tar zxvf nginx-1.8.0.tar.gz$ cd nginx-1.8.0 先ほどnginx -Vで確認したconfigure argumentsをコピってそこに--with-http_geoip_moduleを追加してコンパイルする。 Nginx再コンパイル12345678910111213141516171819202122232425262728293031$ cd /usr/local/src/nginx-1.8.0$ ./configure --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_random_index_module --with-http_secure_link_module --with-http_stub_status_module --with-http_auth_request_module --with-mail --with-mail_ssl_module --with-file-aio --with-ipv6 --with-http_spdy_module --with-http_geoip_module --with-cc-opt='-O2 -g -pipe -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic'...checking for GeoIP library ... found ← ちゃんとインストール済み！checking for GeoIP IPv6 support ... found ← ちゃんとインストール済み！...Configuration summary + using system PCRE library + using system OpenSSL library + md5: using OpenSSL library + sha1: using OpenSSL library + using system zlib library nginx path prefix: &quot;/etc/nginx&quot; nginx binary file: &quot;/usr/sbin/nginx&quot; nginx configuration prefix: &quot;/etc/nginx&quot; nginx configuration file: &quot;/etc/nginx/nginx.conf&quot; nginx pid file: &quot;/var/run/nginx.pid&quot; nginx error log file: &quot;/var/log/nginx/error.log&quot; nginx http access log file: &quot;/var/log/nginx/access.log&quot; nginx http client request body temporary files: &quot;/var/cache/nginx/client_temp&quot; nginx http proxy temporary files: &quot;/var/cache/nginx/proxy_temp&quot; nginx http fastcgi temporary files: &quot;/var/cache/nginx/fastcgi_temp&quot; nginx http uwsgi temporary files: &quot;/var/cache/nginx/uwsgi_temp&quot; nginx http scgi temporary files: &quot;/var/cache/nginx/scgi_temp&quot;$ make$ make install 再度Nginxモジュール確認1234567nginx -Vnginx version: nginx/1.8.0built by gcc 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC)built with OpenSSL 1.0.1e-fips 11 Feb 2013TLS SNI support enabledconfigure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_random_index_module --with-http_secure_link_module --with-http_stub_status_module --with-http_auth_request_module --with-mail --with-mail_ssl_module --with-file-aio --with-ipv6 --with-http_spdy_module --with-http_geoip_module --with-cc-opt='-O2 -g -pipe -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic' --with-http_geoip_module が利用可能なモジュールとして設定されていることが確認できました。 以下設定ファイルも特に変更することなくsystemctl restart nginxしても問題なかったです。 12/etc/nginx/nginx.conf/etc/nginx/conf.d/*.conf 以上です。","link":"/2015/09/30/2015-10-01-nginx-geoip-library/"},{"title":"Nginxエラー調査 「duplicate MIME type &#39;text&#x2F;html&#39; in &#x2F;etc&#x2F;nginx&#x2F;nginx.conf」","text":"概要エラーログをslackに通知させるようにしてるとほんと便利。 エラーログをSlack通知する - 長生村本郷Engineers'Blog環境 CentOS Linux release 7.1.1503 (Core) td-agent: 0.12.12 Nginx: 1.8.0 概要 社内でSlackによる連携が進み ログ管理もfluentdにまとめつつあるので エラーログで何かあったらSlack通知させようと思い… 時たまなんですが、なんだこれ？というのが送られてくる。 その一つが掲題のエラー。 1duplicate MIME type &quot;text/html&quot; in /etc/nginx/nginx.conf nginx.confを見てみるとgzip_typesで設定した text/html でした。 直訳すると 1/etc/nginx/nginx.confでMIMEタイプ「text/html」が重複しています。 じゃ、消せばいいかなってことなので消せば解決しました。 /etc/nginx/nginx.conf 1234567891011121314151617gzip on;gzip_static on;gzip_http_version 1.0;gzip_types text/plain text/html text/xml text/css application/xml application/xhtml+xml application/rss+xml application/atom_xml application/javascript application/x-javascript;gzip_disable &quot;MSIE [1-11]\\.(?!.*SV1)&quot;;gzip_disable &quot;Mozilla/4&quot;;gzip_comp_level 9;gzip_vary on; 別にどこかで指定されているの？結論を言うと ngx_http_gzip_moduleをインストールしており gzip on としていると、デフォルトでtext/htmlがMIMEタイプが指定されます。 以下公式サイトを見るとわかります。 gzip_types text/htmlタイプは常に圧縮対象としているそうです。 なので、gzip で圧縮処理をする場合はtext/htmlが不要です。 ということでした。","link":"/2015/10/04/2015-10-05-nginx-error-duplicate-mime-type-text-html/"},{"title":"Kibana エラー対応 - Discover: An error occurred with your request. Reset your inputs and try again.","text":"問題ある日、Kibana &gt; Discover にアクセスすると以下のようなエラー表示でSearchingが完了できない状態に陥った。 ElasticSearchログ確認1234567891011121314151617181920tail -f /var/log/elasticsearch/elasticsearch.logFailed to execute [org.elasticsearch.action.search.SearchRequest@8307e49] while moving to second phasejava.lang.ClassCastException: java.lang.Long cannot be cast to org.apache.lucene.util.BytesRef at org.apache.lucene.search.FieldComparator$TermOrdValComparator.compareValues(FieldComparator.java:902) at org.apache.lucene.search.TopDocs$MergeSortQueue.lessThan(TopDocs.java:172) at org.apache.lucene.search.TopDocs$MergeSortQueue.lessThan(TopDocs.java:120) at org.apache.lucene.util.PriorityQueue.upHeap(PriorityQueue.java:225) at org.apache.lucene.util.PriorityQueue.add(PriorityQueue.java:133) at org.apache.lucene.search.TopDocs.merge(TopDocs.java:234) at org.elasticsearch.search.controller.SearchPhaseController.sortDocs(SearchPhaseController.java:239) at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.moveToSecondPhase(TransportSearchQueryThenFetchAction.java:89) at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.innerMoveToSecondPhase(TransportSearchTypeAction.java:403) at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:202) at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.onResult(TransportSearchTypeAction.java:178) at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.onResult(TransportSearchTypeAction.java:175) at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:568) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) キャストがうまくいってないバグ出てました。 マッピングがうまくいってないのかな？と問題を想定して色々設定変更していたらここにたどり着いた。 解決 Kibana &gt; Settings &gt; Advanced sort:options {“unmapped_type”: “boolean”} → {“ummapped_type”: “date” } に変更 これで治った！ GithubでもClose Issueとして残ってましたね。 Auto detect field type when sorting on unmapped_type fields","link":"/2015/10/06/2015-10-07-kibana-error-discover-an-error-occured-with-your-request/"},{"title":"Elasticsearch インデックス一覧・マッピング一覧・マッピング設定","text":"前提Elasticsearch のインストールされているサーバ内での作業を想定しています。 インデックス一覧1curl -XGET localhost:9200/_aliases?pretty 12345678{ &quot;logstash-2015.09.22&quot; : { &quot;aliases&quot; : { } }, &quot;logstash-2015.09.21&quot; : { &quot;aliases&quot; : { } },} prettyをクエリストリングに指定するとjsonが整形されて表示されます。 インデックスのマッピング一覧1$ curl -XGET localhost:9200/_mapping?pretty 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677{ \"logstash-2015.09.22\" : { \"mappings\" : { \"project_production_nginx\" : { \"properties\" : { \"@log_name\" : { \"type\" : \"string\" }, \"@timestamp\" : { \"type\" : \"date\", \"format\" : \"dateOptionalTime\" }, \"body_bytes_sent\" : { \"type\" : \"long\" }, \"bytes_sent\" : { \"type\" : \"long\" }, \"forwardedfor\" : { \"type\" : \"string\" }, \"host\" : { \"type\" : \"string\" }, \"https\" : { \"type\" : \"string\" }, \"log_level\" : { \"type\" : \"string\" }, \"message\" : { \"type\" : \"string\" }, \"pid\" : { \"type\" : \"string\" }, \"query_string\" : { \"type\" : \"string\" }, \"referer\" : { \"type\" : \"string\" }, \"remote_addr\" : { \"type\" : \"string\" }, \"request_length\" : { \"type\" : \"long\" }, \"request_method\" : { \"type\" : \"string\" }, \"request_time\" : { \"type\" : \"double\" }, \"request_uri\" : { \"type\" : \"string\" }, \"status\" : { \"type\" : \"long\" }, \"tid\" : { \"type\" : \"string\" }, \"upstream_response_time\" : { \"type\" : \"long\" }, \"uri\" : { \"type\" : \"string\" }, \"useragent\" : { \"type\" : \"string\" } } } } }} マッピング設定jsonファイル作成12$ cd /etc/elasticsearch/$ vim el_mapping.json el_mapping.json 以下のように設定し保存 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667{ \"template\": \"logstash-*\", \"mappings\" : { \"_default_\" : { \"dynamic_templates\":[{ \"string_template\":{ \"match\":\"*\", \"mapping\":{ \"type\":\"string\", \"index\":\"not_analyzed\" }, \"match_mapping_type\":\"string\" } }], \"properties\" : { \"@timestamp\" : { \"type\" : \"date\", \"format\" : \"dateOptionalTime\" }, \"body_bytes_sent\" : { \"type\" : \"long\" }, \"bytes_sent\" : { \"type\" : \"long\" }, \"geoip\" : { \"properties\" : { \"location\" : { \"type\" : \"geo_point\", \"lat_lon\" : true, \"geohash\" : true, \"geohash_prefix\" : true, \"geohash_precision\" : 10 } } }, \"request_length\" : { \"type\" : \"long\" }, \"request_time\" : { \"type\" : \"double\" }, \"status\" : { \"type\" : \"long\" }, \"upstream_response_time\" : { \"type\" : \"long\" }, \"coordinate\" : { \"type\" : \"double\" }, \"country\" : { \"type\" : \"string\" }, \"lat\" : { \"type\" : \"double\" }, \"location_properties\" : { \"type\" : \"geo_point\" }, \"lon\" : { \"type\" : \"double\" } } } }} マッピング設定実行1curl -X PUT localhost:9200/_template/template_1 --data @el_mapping.json マッピング設定一覧12345678910111213141516171819202122232425curl -X GET localhost:9200/_template/?pretty# 一覧が出力される......}, \"template_1\" : { \"order\" : 0, \"template\" : \"flag_prd.accesslog-*\", \"settings\" : { }, \"mappings\" : { \"_default_\" : { \"dynamic_templates\" : [ { \"string_template\" : { \"mapping\" : { \"index\" : \"not_analyzed\", \"type\" : \"string\" }, \"match_mapping_type\" : \"string\", \"match\" : \"*\" } } ], \"properties\" : {...... マッピング設定削除1curl -X DELETE localhost:9200/_template/template_1 string型はまとめてdynamic_templatesで定義してます。12345678910\"dynamic_templates\":[{ \"string_template\":{ \"match\":\"*\", \"mapping\":{ \"type\":\"string\", \"index\":\"not_analyzed\" }, \"match_mapping_type\":\"string\" }}], インデックス削除インデックスを指定し削除します。 123curl -X DELETE localhost:9200/logstash-2015.09.22{&quot;acknowledged&quot;:true} インデックス全削除123$ curl -XDELETE 'http://localhost:9200/*'{&quot;acknowledged&quot;:true} サービス開始前で試行錯誤してるときに何度か利用しました。極力使わない。 指定インデックス削除 例) .kibana インデックス削除 1$ curl -XDELETE 'http://localhost:9200/.kibana' 各フィールド設定は以下まとめていただいている方がいらっしゃいました。ありがとうございます♪ Kibana+Elasticsearchで文字列の完全一致と部分一致検索の両方を実現する","link":"/2015/10/07/2015-10-08-elasticsearch-index-mapping/"},{"title":"Nginx Basic認証設定、社内IPなど特定ipのみ許可","text":"概要サービス公開前にNginxでBasic認証を掛ける必要がありました ちょうど社内公開したときでもありBasic認証のポップアップが出るのが鬱陶しいのもあって社内だけオフりたい、というときに以下のような設定をしました。 Basic認証設定1# yum install -y httpd-tools 1234# cd /etc/nginx# htpasswd -c .htpasswd &lt;Basic認証ユーザ&gt;New password: &lt;Basic認証ユーザのパスワード入力&gt;Re-type new password: &lt;もう一度、Basic認証ユーザのパスワード入力&gt; Nginx configureファイル編集1# vim /etc/nginx/conf.d/default.conf 123456789location / { .... # Basic認証設定 auth_basic &quot;Restricted&quot;; auth_basic_user_file /etc/nginx/.htpasswd; ....} 特定IPのみ許可したい場合1# vim /etc/nginx/conf.d/default.conf 12345678910111213141516location / { .... # add start ----- satisfy any; allow &lt;許可IP&gt;; allow &lt;許可IP&gt;; deny all; # add end ----- # Basic認証設定 auth_basic &quot;Restricted&quot;; auth_basic_user_file /etc/nginx/.htpasswd; ....} あとは 許可IPを随時増やせば良いです。 Webアプリケーション開発におけるNginxあるあるかなと思います♪ 以上です。","link":"/2015/10/18/2015-10-19-allow-specified-ip-nginx/"},{"title":"Nginxエラー対策 a client request body is buffered to a temporary file","text":"概要Nginxで以下のようなエラーが発生 [warn]とあるしものものしさは否めない汗 要求されるバッファサイズが設定量より多く、一時ファイルを利用しますよ、という警告です。 1[warn] 1493#1493: *210 a client request body is buffered to a temporary file /var/cache/nginx/client_temp/0000000001 ... request: &quot;POST /article/save HTTP/1.1&quot;, そのバッファサイズの設定量を変更をしてまず対策しました。 取り急ぎ、t2.microで最小構成でとりあえず作れ！というミッションだったので50kくらいにしておきました。 123456789http { ... client_body_buffer_size 50k; ...} 取り急ぎは上記設定でwarn は吐き出されないようになりました。 もしまた吐き出されるようになった場合は、その数値を元にインスタンスのスケールアップも検討しようと思います。 ちなみにバッファのキャッシュ先ディレクトリ(client_body_temp_path)はデフォルトで/var/cache/nginx/clientとなっていたので設定しませんでした。 以上です。","link":"/2015/10/18/2015-10-19-nginx-error-a-client-request-body-is-buffered-to-a-temporary-file/"},{"title":"Ruby &amp; gem インストール","text":"備忘録です。 環境 CentOS 5.8 (Final) ruby 2.1.2 インストール1234567# cd /usr/local/src# wget http://cache.ruby-lang.org/pub/ruby/2.1/ruby-2.1.2.tar.gz# tar xvfz ruby-2.1.2.tar.gz# rm ruby-2.1.2.tar.gz# cd ruby-2.1.2# ./configure# make; make install gem インストール1234# wget http://production.cf.rubygems.org/rubygems/rubygems-2.2.2.zip# unzip rubygems-2.2.2.zip# cd rubygems-2.2.2# ruby setup.rb 以上","link":"/2015/10/21/2015-10-22-install-ruby-gem/"},{"title":"By grunt, uglify js &amp; minify css on MacOSX.","text":"environment MacOSX 10.11 grunt-cli v0.1.13 grunt v0.4.5 Install npm, Initilize npm123cd ~ # depending on your preference.brew install npmnpm init Result 1234567891011121314151617About to write to /Users/kenzo/go/src/github.com/flag/public/package.json:{ \"name\": \"public\", \"version\": \"1.0.0\", \"description\": \"gruntfile\", \"main\": \"Gruntfile.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" &amp;&amp; exit 1\" }, \"author\": \"xxxxxxxx@gmail.com\", \"license\": \"ISC\", \"devDependencies\": { }, \"dependencies\": { }} Install grunt-cli1npm install -g grunt-cli With --save-dev option, add install module infomation to package.json. Install grunt modules1234567891011npm install -g grunt --save-devnpm install grunt-contrib-watch --save-devnpm install grunt-contrib-copy --save-devnpm install grunt-contrib-clean --save-devnpm install grunt-contrib-cssmin --save-devnpm install grunt-contrib-uglify --save-devnpm install grunt-image --save-devnpm install grunt-contrib-htmlmin --save-devnpm install load-grunt-tasks --save-devnpm install grunt-jsbeautifier --save-devnpm install grunt-cssbeautifier --save-dev Module map. Module Detail grunt-contrib-watch Monitoring update files. grunt-contrib-copy Copy file or directory. grunt-contrib-clean Clean file or directory. grunt-contrib-cssmin Minify CSS files. grunt-contrib-uglify Uglify &amp; Compress Javascript files. grunt-contrib-image Optimize image files (jpeg, jpg, gif, png, swf, etc…). grunt-contrib-htmlmin Minify HTML files. grunt-jsbeautifier beautify Javascript files. grunt-cssbeautifier beautify Javascript files. Confirm package.json1234567891011121314151617181920212223242526cat package.json{ \"name\": \"public\", \"version\": \"1.0.0\", \"description\": \"grunt\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" &amp;&amp; exit 1\" }, \"author\": \"*********@gmail.com\", \"license\": \"ISC\", \"devDependencies\": { \"grunt\": \"^0.4.5\", \"grunt-contrib-clean\": \"^0.6.0\", \"grunt-contrib-copy\": \"^0.8.2\", \"grunt-contrib-cssmin\": \"^0.14.0\", \"grunt-contrib-htmlmin\": \"^0.6.0\", \"grunt-contrib-uglify\": \"^0.10.0\", \"grunt-contrib-watch\": \"^0.6.1\", \"grunt-cssbeautifier\": \"^0.1.2\", \"grunt-image\": \"^1.1.1\", \"grunt-jsbeautifier\": \"^0.2.10\", \"load-grunt-tasks\": \"^3.3.0\" }} Add dependencies of grunt modules to package.json ! move to parent directory of css, js folder1cd /path/to/project/public/ 1234567891011121314151617tree/public/│├─Gruntfile.coffee # make `Gruntfile.coffee` at next step.│├── css│ ├── bootstrap.css│ ├── img.css│ ├── style.css│ └── reset.css│└── js ├── jquery-1.9.1.js ├── img.js ├── login.js └── signup.js Create Gruntfile.coffee or Gruntfile.json Today, I create a Gruntfile.coffee. For example, Only Uglify js, Minify css, Beautify js, css. 1vim Gruntfile.coffee 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950module.exports = (grunt) -&gt; # current path. path = require('path') current = path.resolve('.') # load npm task. grunt.loadNpmTasks('grunt-contrib-cssmin'); grunt.loadNpmTasks('grunt-contrib-uglify'); grunt.loadNpmTasks('grunt-jsbeautifier'); grunt.loadNpmTasks('grunt-cssbeautifier'); grunt.initConfig # define directory. dir: js: 'js' css: 'css' img: 'img' # minify CSS. cssmin: all: expand: true cwd: current + '/&lt;%= dir.css %&gt;/' src: '*.css' dest: current + '/&lt;%= dir.css %&gt;/' # uglify js. uglify: options: mangle: true compress: true all: expand: true cwd: current + '/&lt;%= dir.js %&gt;/' src: '*.js' dest: current + '/&lt;%= dir.js %&gt;/' # beautify js. jsbeautifier: files: '**/*.js' options: [] # beautify css. cssbeautifier: files: '**/*.css' options: [] grunt.registerTask 'default', ['cssmin', 'uglify'] Make symbolilc Link of ‘node_modules’12345678910111213141516171819cd &lt;path where Gruntfile.coffee exist&gt;ln -s ~/node_modules .tree/public/├─Gruntfile.coffee├─node_modules -&gt; /Users/kenzo/node_modules│├── css│ ├── bootstrap.css│ ├── img.css│ ├── style.css│ └── reset.css│└── js ├── jquery-1.9.1.js ├── img.js ├── login.js └── signup.js Execute grunt command.12cd &lt;path where Gruntfile.coffee exist&gt;grunt # By no parameter, execute default task. If You want to execute only cssmin, excute command grunt cssmin Thakns.","link":"/2015/11/08/2015-11-09-grunt-uglify-js-minify-css-on-macos/"},{"title":"Elasticsearch curatorで不要Indexをまとめて削除","text":"概要fluentd + ElasticSearch + kibana を運用していますがある日ElasticSearchが動作しなくなる事象が発生しました。 過去indexが溜まりに溜まってメモリ不足というエラー。 logはS3にアップロードしているし、不要なIndexは適宜削除して対策しました。 環境 CentOS Linux release 7.0.1406 (Core) ElasticSearch 1.7.1 Python 2.7.5 pip 7.1.0 curator インストール ElasticSearchをインストールしているサーバにて以下実施 1# pip install curator curator コマンド実行 ElasticSearchをインストールしているサーバにて以下実施 12345678# 14日(2週間)経過でclosecurator --host localhost close indices --prefix logstash --older-than 14 --time-unit days --timestring %Y.%m.%d# 35日(4週間)経過でdeletecurator --host localhost delete indices --prefix logstash --older-than 35 --time-unit days --timestring %Y.%m.%d# 2日経過でbloom filter無効化curator --host localhost bloom indices --prefix logstash --older-than 2 --time-unit days --timestring %Y.%m.%d 上記をjenkinsで SSH plugin でリモートサーバにログインして実行するよう、設定して定期ポーリングで1日1回実行させてます。 以上","link":"/2015/11/12/2015-11-12-elasticsearch-curator-delete-norequired-index/"},{"title":"Use Redis via Python","text":"MemorandumThe below codes is how to use redis via Python. 123456789101112131415161718192021222324252627282930313233343536#!/usr/bin/python# coding: UTF-8import redisr = redis.StrictRedis(host='localhost', port=6379)# set/get string.r.set('test1', 'aiueo')r.expire('test1', 1000)print r.get('test1') # aiueo# set/get integer.r.set('test2', 2)print r.get('test2') # 2# Check key exit.print r.exists('test1') # Trueprint r.exists('test0') # False# pattern matchkeys = r.keys('test*')if len(keys) &gt; 0 : for key in keys: print '--------------------------------------------' print key # test1, test2 print r.get(key) # aiueo, 2 print r.type(key) # Type : string, string print r.ttl(key) # Expire : if not set, set \"-1\"r.append('test1', '_kkkkkkk')print r.get('test1') # aiueo_kkkkkkk# delete cache.r.delete('test1')print r.exists('test1') # False Thanks.","link":"/2015/11/09/2015-11-10-redis-python/"},{"title":"コンテンツがgzip圧縮されているかチェック","text":"本当にただの備忘録です。 Nginxで gzip on にしたけど設定反映されているかshellで確認","link":"/2015/11/12/2015-11-13-check-gzip-compress/"},{"title":"運用中のNginxをノーメンテでバージョンアップ&amp;HTTP2.0モジュールを導入","text":"概要運用中の Nginx に HTTP2.0モジュール http_v2_module を導入しサイトのパフォーマンス向上を図ります。 ※ Nginx 1.9.5 から http_spdy_module は http_v2_module に変更しています。 環境 CentOS Linux release 7.1.1503 (Core) Nginx 1.9.3 インストール済み/稼働中 したいこと Nginx のバージョンアップ (1.9.5以上) http_v2_module インストール 現状確認1234567# nginx -Vnginx version: nginx/1.9.3built by gcc 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC)built with OpenSSL 1.0.1e-fips 11 Feb 2013TLS SNI support enabledconfigure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_random_index_module --with-http_secure_link_module --with-http_stub_status_module --with-http_auth_request_module --with-threads --with-stream --with-stream_ssl_module --with-mail --with-mail_ssl_module --with-file-aio --with-ipv6 --with-http_spdy_module --with-cc-opt='-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic' ※moduleやlog, pidのパスは各環境に毎に異なります。 まずは 1.9.5以上にバージョンアップして http_v2_module を導入したいと思います。 Nginx 1.9.6 インストール今回は 2015.11.17 時点で最新の 1.9.6 をインストールします。 1234567891011121314151617# cd /usr/local/src# wget http://nginx.org/download/nginx-1.9.6.tar.gz# tar xvf load/nginx-1.9.6.tar.gz# cd nginx-1.9.6# ./configure --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_random_index_module --with-http_secure_link_module --with-http_stub_status_module --with-http_auth_request_module --with-threads --with-stream --with-stream_ssl_module --with-mail --with-mail_ssl_module --with-file-aio --with-ipv6 --with-http_v2_module --with-cc-opt='-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic'# make# make install ~~~ インストール完了 ~~~# nginx -Vnginx version: nginx/1.9.6built by gcc 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC)built with OpenSSL 1.0.1e-fips 11 Feb 2013TLS SNI support enabledconfigure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_random_index_module --with-http_secure_link_module --with-http_stub_status_module --with-http_auth_request_module --with-mail --with-mail_ssl_module --with-file-aio --with-ipv6 --with-http_v2_module --with-http_geoip_module --with-cc-opt='-O2 -g -pipe -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic' version が 1.9.6 となりconfigure arguments に --with-http_v2_module が追加されていることがわかります。 要点は元々導入済み http_spdy_module を http_v2_module に変更しビルドです。--with-http_spdy_module がなければ --with-http_v2_module 追加です。 nginx serverディレクティブ修正ssl http2.0対応する様、修正します 123server {- listen 443;+ listen 443 ssl http2; Nginx configure test &amp; reload configure test 実施します。 以下のように syntax is ok が出ない場合は設定に誤りがあるので修正してください。 1234# nginx -tnginx: the configuration file /etc/nginx/nginx.conf syntax is oknginx: configuration file /etc/nginx/nginx.conf test is successful 設定を再読み込みします。 1# nginx -s reload 上記で設定完了です。これまでノーメンテでバージョンアップし、http_v2_moduleインストールができました。 早速httpsスキーマとなるページにアクセスしてみましょう。 http2.0設定確認 Chrome ブラウザの Extension SPDYインディケータで確認 FireFox 開発ツール&gt; ネットワーク &gt; ヘッダから確認 SPDYインディケータで確認HTTP/2 and SPDY indicator 拡張モジュールをインストールして確認してみるとSPDYインディケータが青くなっていることが確認できます。 FireFox 開発ツール&gt; ネットワーク &gt; ヘッダから確認 参考サイト nginxでHTTP2接続(not spdy3.1)の検証 HTTP/2, SPDY 対応の負荷テストツール h2load","link":"/2015/11/16/2015-11-17-versionup-nginx-without-maintenance/"},{"title":"Kibana4 検索窓での検索 正規表現パターンマッチ等","text":"概要アクセスログをfluentdで集積(aggregate)してElasticSearch へ保存、そのデータをkibanaで表示しています。 ちょっとしたアクセスログ解析したい場合、かつては、SSHでサーバにログインしてコマンド実行し検索するという工程を踏んでいました。 ですが、Kibanaで検索することによりサーバログインすることなく、検索がスムーズになりました。 リモートログインして誤った操作等もなくなる、また本番環境アカウントの公開範囲を絞ることができ良いことが増えました。 実際構築しても使う側がどう検索したら良いかわからないということがちょいちょいあったのでKibana4 検索窓での検索方法を簡単にまとめました。 前提 Kibana4 ドメイン名を以下とする http(s)://hogehoge.jp 範囲指定 httpステータスコード 200 から 400 検索 status: [200 TO 400] 否定 例) 指定ドメイン以外のリファラ検索 referer という項目について 正規表現での否定(NOT)のパターンマッチで検索します。 1NOT referer:/http(s?)\\:\\/\\/hogehoge\\.jp\\/(.*)/ 複合検索 例) 指定ドメイン以外、且つ、200ステータス 1NOT referer:/http(s?)\\:\\/\\/hogehoge\\.jp\\/(.*)/ AND status:200 随時、事例があれば追記していきます。","link":"/2015/11/23/2015-11-24-kibana-regex-pattern-match/"},{"title":"意外と容量食ってた yum cache","text":"yum cache 容量12# du -sh /var/cache/yum155M /var/cache/yum 155MByteある汗 yum cache 削除12345# yum clean all読み込んだプラグイン:fastestmirrorリポジトリーを清掃しています: base epel extras mysql-connectors-community mysql-tools-community mysql56-community nginx treasuredata updatesCleaning up everythingCleaning up list of fastest mirrors yum cache容量確認12# du -sh /var/cache/yum8.0K /var/cache/yum スッキリ！ サーバから容量不足のアラートで少しでも容量減らしたいときに役立ちました。 潤沢にサーバスペックを用意できるクライアントでない場合もあるので地道に必要な知識だと感じました。","link":"/2015/11/25/2015-11-26-cleanup-yum-cache/"},{"title":"Twilio で電話通知","text":"概要障害検知をメールやSlackに流しても休日に業務系連絡を見ることは少ない。その為、より検知報告に気付きやすくする様、電話通知にするべくTwilioを導入する運びとなりました。 まずは利用するまでの簡単な手順をまとめました。 ※コードのサンプルがTwilioのサイト内にあるので導入しやすかったです。 手順1. Twilio にアクセス以下リンクからTwilioにアクセスします。 http://twilio.kddi-web.com 2. 新規登録名前、E-mail、パスワード(大小半角英数字)と 何にどの言語で利用するかを選択して 「始めましょう」をクリック。 「始めましょう」という日本語はユーザ目線でなく違和感ありますね。 3. アカウント認証以下２つの認証方法があります。 電話番号にSMSに確認コードを送信させる 電話番号にTwilioから着信があり確認コードのダイヤルキーを入力する。 後者はTwilioの音声読み上げロボットから電話がかかって来るのでどんな感じで通知されるか試してみたい方は是非後者を選択してみてください。 スムーズなイントネーションではないですが、伝えたい気持ちは誰よりもあります。 4. Twilioから電話をかけてみるアカウント認証確認後、Twilio製品の「プログラマブルVoice」ページTopに遷移しました。メニューから ツール をクリックしツールページに遷移してください。 To指定音声通話&gt;通話&gt;電話をかける のAPI Explorer にて必須 のTo に 電話番号認証した番号を入力してください。 但し、日本番号（+81）の場合、 090******** の番号ははじめの 0 を削除し、 +81 をprefixとして追加し、+8190******** となりますので注意してください。 URL指定条件 のUrl にTwilioからの電話を受け取った際、電話のキーダイヤルを押下した際の挙動をurl形式で指定可能です。 適当に準備できるUrlがない場合は、以下のような適当なUrlで良いです。 http:/hogehoge.hogehoge.co.jp 通知実行ページ下部の リクエストを発行する ボタンをクリックします。 ※ ※料金が発生します とありますが、Trialは料金を請求されるということはありません。※同アカウントでアップグレードする際は、発信した電話料金も合わせて請求されることになるので、テスト完了後は別途アカウントを取得することを推奨します。 電話を受け取るTwilioから電話がかかってきたかと思います。キーダイヤルを入力すると、 Urlで指定したプログラムが動作しますが、適当に入力したのでアプリケーションエラーが発生しました。電話を終了します。と通知されるはずです。 実際にプログラムから使用する際は、指定したUrlでキー入力を受け取ってその後の挙動を制御する、という流れになります。 実際に利用した例担当した業務では、Zabbixからの障害検知をTwilioで担当者に電話報告（エスカレーション）するという仕組みを構築しました。 例）以下のような挙動が実現できます。Zabbixで障害検知 ↓Twilio↓ 電話対応者 ↓ 対応者が「1」をクリック指定したUrlのプログラム実行↓「1」を受け取り、障害対応可能な旨をZabbixへ通知Zabbix (障害対応中) Zabbix &amp; Twilio 参考zabbix-twillio 上記サイトでのzabbix-twilio連携の手順で進めた場合、twilio でキーダイヤル入力後、 以下のようなエラーが発生し、イベント登録ができません。API error -32602: The &quot;user.login&quot; method must be called without the &quot;auth&quot; parameter zabbix-twilio.php内の Zabbix_APIクラスが古い為です。 zabbix-twilio.php PhpZabbixApi 上記をダウンロードし、 php build.php で 以下2ファイルを生成し、こちらを呼び出し Zabbix_Api と ZabbixApi とを変更してください。 ZabbixApi.class.php ZabbixApiAbstract.class.php /var/www/html/zabbix-twilio/zabbix-twilio.php 12345+ require_once 'ZabbixApi.class.php';+ use ZabbixApi\\ZabbixApi;- $api = new Zabbix_API ( $ZABBIX_API, $ZABBIX_USER, $ZABBIX_PASS );+ $api = new ZabbixApi ( $ZABBIX_API, $ZABBIX_USER, $ZABBIX_PASS); ZabbixApi は Basic認証を設定している場合にも対処しています。 12// 例)$api = new ZabbixApi ( $ZABBIX_API, $ZABBIX_USER, $ZABBIX_PASS, $BASIC_AUTH_USER, $BASIC_AUTH_PASS); 検証環境 Amazon Linux AMI release 2015.09 Zabbix 2.5 (3.0α) PHP 5.6.14 MySQL 5.5.46 以上","link":"/2015/12/07/2015-12-08-tellme-twillio/"},{"title":"Git で削除したブランチを復活させる","text":"概要以下コマンドでブランチを強制的に削除した後、やっぱり必要だったのに、となったときの対処 1$ git branch -D &lt;branch_name&gt; 復活手順 HEADの変更履歴を確認する HEADのログ番号からブランチ名作成 12$ git reflog$ git branch &lt;branch_name&gt; HEAD@{num} - 例) 123456$ git reflogc95c7e9 HEAD@{0}: merge release: Merge made by the 'recursive' strategy.ad5bed0 HEAD@{1}: checkout: moving from release to masterffe45df HEAD@{2}: merge develop: Merge made by the 'recursive' strategy.6aa536b HEAD@{3}: checkout: moving from develop to release 上記のHEAD@{3}が消してしまったブランチに対してのcommitだ！とわかれば、 1$ git branch hogehoge HEAD@{3} 上記コマンド完了後、git branchすると branch hogehogeが作成されたことがわかる。 なので、commitはこまめにしておくと良いです。","link":"/2015/12/08/2015-12-09-revive-deleted-git-branch/"},{"title":"今更ながらMacでドットファイルを表示する","text":"概要Finderでドットファイルを開きたいという際にやっておく設定 環境 MacOSX Yosemite 10.10.3 手順ターミナルを開き以下コマンド実行1$ defaults write com.apple.finder AppleShowAllFiles -boolean true Finderアプリ再起動1$ killall Finder これでドットファイルが反映されています。 元に戻したい場合ターミナルで以下コマンド実行1$ defaults delete com.apple.finder AppleShowAllFiles 再起動1$ killall Finder 以上です","link":"/2015/12/08/2015-12-09-show-dotfile-on-macos/"},{"title":"Redis - (error) NOAUTH Authentication required への対応","text":"概要経年運用し特にメモリも問題なかったRedisが突如接続エラーが発生したのでその際の対応をまとめました。 エラー内容1PHP Fatal error: Uncaught exception 'RedisException' with message 'Failed to AUTH connection' 認証接続に失敗して Exception になっている。 Redisの設定周りで特にrequirepass を設定していないし何故突然？という感じでした。 各環境でRedisの設定パスは異なると思いますが、自環境の場合以下です。/etc/redis/6379.conf 以下のようにrequirepassはコメントアウトされている。 1# requirepass 対応 process を kill する※ service redis restart のように redisを再起動しても状況は変わりませんでした。 123# ps aux | grep 'redis' | grep -v 'grep'root 12743 0.1 0.2 40604 2124 ? Ssl 10:50 0:00 /usr/local/bin/redis-server *:6379 再度 redis を起動させる。 1# service redis start requirepass 設定 運用中でアプリケーション自体のソースをいじりたくなかったので空のrequirepass を設定しました。 1redis-cli&gt; CONFIG SET REQUIREPASS '' 上記で取り急ぎ対応は問題なかったです。 総評改めて利用する際には認証設定をしてアプリケーションからもpasswordを指定して接続する仕組みが良いですね。 ただ何故急に発生したかは引き続き調査をしていきます。 理由がわかる方はコメントなどいただけましたら幸いです。","link":"/2015/12/11/2015-12-12-redis-error-noauth-authentication-required/"},{"title":"MySQLトラブルシューティング - ERROR 2006 (HY000) at line ***: MySQL server has gone away","text":"概要DBインポート時に掲題のエラーが発生しました。 インポートサイズが大きすぎる為です。 インポートデータサイズのデフォルト値は 1M です。 以下コマンドで確認できます。 1mysql&gt; show variables like 'max_allowed_packet'; 対策2点あります。 mysqlコマンドラインから設定 (一時的) my.cnf に設定 (恒久的) mysqlコマンドラインから一時的に引き上げる再起動の必要がなく影響範囲が少なく済みます。但し、再起動後、デフォルト値に戻るので恒久的な対応が必要な場合my.cnf に設定しmysqldを再起動する必要があります。 例) 10MB に設定 1mysql&gt; set global max_allowed_packet = 1000000; my.cnf に max_allowed_packet を引き上げる様設定my.cnf パス探索123# mysql --help | grep my.cnf order of preference, my.cnf, $MYSQL_TCP_PORT,/etc/my.cnf /etc/mysql/my.cnf /usr/etc/my.cnf ~/.my.cnf 以下の順で my.cnf を探しています。/etc/my.cnf → /etc/mysql/my.cnf → /usr/etc/my.cnf → ~/.my.cnf 個々の環境で異なるので本当に存在するかも含め設定ファイルを見定めてください。 おおよそ /etc/my.cnf が一般的かと思います。 my.cnf に [mysqld] に属する様に設定します。10MBに設定してみます。 12[mysqld]max_allowed_packet=10MB 以上設定して再起動で設定反映完了です。 確認設定が反映されているか確認します。 1234567mysql&gt; show variables like 'max_allowed_packet';+--------------------+----------+| Variable_name | Value |+--------------------+----------+| max_allowed_packet | 10485760 |+--------------------+----------+ 以上","link":"/2015/12/16/2015-12-17-mysql-error-2006-hy000-mysql-server-has-gone-away/"},{"title":"Outlook にメールが届かない件対応","text":"概要EC2を利用してますがSESかまさずにメール送信をしていた時に以下のようなエラーが発生しました。 エラー内容 Dec 18 17:24:11 ip-xxx-xx-xx-xx postfix/smtp[4827]: 380D2A27ED: to=hogehoge@xxxxxxx.com, relay=xxxxxxx-com.mail.protection.outlook.com[xxx.xx.xx.xxx]:25, delay=6.1, delays=0.01/0/0.88/5.2, dsn=5.7.1, status=bounced (host xxxxxxx-com.mail.protection.outlook.com[xxx.xx.xx.xxx] said: 550 5.7.1 Service unavailable; Client host [yy.yy.yy.yyy] blocked using FBLW15; To request removal from this list please forward this message to delist@messaging.microsoft.com (in reply to RCPT TO command)) 要約するとhogehoge@xxxxxxx.com 宛のメールが Outlook でブラックリスト扱いされて弾かれています ( status=bounced )。もしブラックリストから削除したい場合は、 delist@messaging.microsoft.com 宛に解除申請してください。 もうちょっと細かくrelay=xxxxxxx-com.mail.protection.outlook.com[xxx.xx.xx.xxx]:25とある通りMicroSoftがメールツールサービス Outlook が受信相手です。 送信元サーバIP yy.yy.yy.yyy のホストは FBLW15 基準でブラックリスト扱いされているのでサービスは利用できない = メールは受け取らないというものです。 FBLW15 … MicroSoft独自のブラックリスト 対応 宛先: 1delist@messaging.microsoft.com タイトル: 1Please Remove My IP yy.yy.yy.yyy from your BlockList. 内容1234Please remove this IP yy.yy.yy.yyy from your BlockList.Thanks.Kenzo Tanaka. 数分後に Microsoft Customer Support から返信が着た メール内容 12345678Hello ,Thank you for your delisting request SRX1318598611ID. Your ticket was received on (Dec 21 2015 08:14 AM UTC) and will be responded to within 24 hours.Our team will investigate the address that you have requested to be removed from our blocklist. If for any reason we are not able to remove your address, one of our technical support representatives will respond to you with additional information.Regards,Technical Support 和訳すると 12345678910こんにちはブラックリスト申請の削除依頼を受け取りました。24時間以内に回答します。削除要請頂いたアドレスをについて調査します。何らかの理由で削除依頼を引き受けられない場合は、当社の技術サポートから追加情報をお届け致します。以上宜しくお願い致します。テクニカルサポート 24時間待ってみるメール着た！ 1234567891011Hello,Thank you for contacting Microsoft Online Services Technical Support. This email is in reference to ticket number, 1318598611 which was opened in regards to your delisting request for yy.yy.yy.yyyThe IP address you submitted has been reviewed and removed from our block lists. Please note that there may be a 1-2 hour delay before this change propagates through our entire system.We apologize for any inconvenience this may have caused you. As long as our spam filtering systems do not mark a majority of email from the IP address as spam-like, your messages will be allowed to flow as normal through our network. However, should we detect an increase in spam-like activity, the IP address may be re-added to our block list.Should you have any further questions or concerns, please feel free to respond to this email.Thank you again for contacting Microsoft Online Services technical support and giving us the opportunity to serve you. 和訳 1234567891011121314151617こんにちはマイクロソフト・オンライン・サービス テクニカルサポートにお問い合わせいただきありがとうございます。このメールはチケット番号 1318598611、IPアドレス(yy.yy.yy.yyy) をブラックリストから削除する様申請頂いた件についてです。あなたが提出されたIPアドレスを見直し、ブロックリストから削除しました。この変更は私たちの全システムに行き届くには 1〜2時間程掛かる見込みです。ご不便お掛けしたことをお詫び申し上げます。スパムフィルタリングシステムがスパムとおぼしきIPアドレスからの多くのE-mailをマークしない限りあなたのメッセージは当社のネットワークを通じて通常通りに許可されるでしょう。しかし、スパム行為が増加していることを検知する必要があり、ブロックリストに追加する可能性があります。その他質問や確認をご希望の際は、こちらのメールにご返信ください。マイクロソフト・オンライン・サービス テクニカルサポートにご連絡いただきお力添えできましたこと、誠に感謝致します。 やや和訳が…アグレッシブということで受け入れてください。もっと良い和訳あったら頂きたいです。 以上でブラックリストから解除されたそうなので、マイクロソフト・テクニカル・サービスからメールが届いてから4〜5時間後に再度メール送信し問題ないことが確認できました。 IPアドレス/DNSのブラックリストチェック以下サイトで試しておくのが良いかと思います。何かの間違いで登録されていた、というスパムメールとして扱われているということになるので。 http://www.spamhaus.org/lookup/ http://mxtoolbox.com/blacklists.aspx","link":"/2015/12/21/2015-12-22-cannot-get-mail-in-outlook/"},{"title":"pingを通すiptableの設定","text":"環境 CentOS 5.8 実現したいことServer A が Server B からのみ ping を通す 12345+----------+ Ping Request +----------+| | &lt;-------------- | || Server A | | Server B || | --------------&gt; | |+----------+ Ping Response +----------+ 特定IPからの ping を通す許可設定以下設定を Server A で実施する。 123[Server A]# iptables -A INPUT -p icmp --icmp-type 8 -s &lt;Server B の IP Address&gt; -j ACCEPT[Server A]# iptables -A OUTPUT -p icmp --icmp-type 0 -s &lt;Server B の IP Address&gt; -j ACCEPT[Server A]# service iptables restart --icmp-type 8 は Echo request (エコー要求) を許可 --icmp-type 0 は Echo Reply (エコー応答) を許可 Server Bから ping実行12345678910[Server B]# ping &lt;Server A の IP Address&gt;PING (&lt;Server A の IP Address&gt;): 56 data bytes64 bytes from (&lt;Server A の IP Address&gt;): icmp_seq=0 ttl=58 time=4.411 ms64 bytes from (&lt;Server A の IP Address&gt;): icmp_seq=1 ttl=58 time=4.079 ms64 bytes from (&lt;Server A の IP Address&gt;): icmp_seq=2 ttl=58 time=4.027 ms^C--- (&lt;Server A の IP Address&gt;) ping statistics ---3 packets transmitted, 3 packets received, 0.0% packet lossround-trip min/avg/max/stddev = 4.027/4.172/4.411/0.170 ms icmp-type List以下asahi-net Appendix C. ICMPタイプより参照 TYPE CODE 意味 問合せ エラー 参照先 0 0 Echo Reply (エコー応答) x RFC792 3 0 Network Unreachable (ネットワーク到達不能) x RFC792 3 1 Host Unreachable (ホスト到達不能) x RFC792 3 2 Protocol Unreachable (プロトコル到達不能) x RFC792 3 3 Port Unreachable (ポート到達不能) x RFC792 3 4 Fragmentation needed but no frag. bit set (フラグメント必要だがフラグメント禁止ビットあり) x RFC792 3 5 Source routing failed (ソースルーティング失敗) x RFC792 3 6 Destination network unknown (宛先ネットワーク発見できず) x RFC792 3 7 Destination host unknown (宛先ホスト発見できず) x RFC792 3 8 Source host isolated (送信元ホストへのルートなし) (廃) x RFC792 3 9 Destination network administratively prohibited (宛先ネットワークは設定によりアクセス禁止) x RFC792 3 10 Destination host administratively prohibited (宛先ホストは設定によりアクセス禁止) x RFC792 3 11 Network unreachable for TOS (TOS種別によりネットワーク到達不能) x RFC792 3 12 Host unreachable for TOS (TOS種別によりホスト到達不能) x RFC792 3 13 Communication administratively prohibited by filtering (フィルタリング設定により通信禁止) x RFC1812 3 14 Host precedence violation (ホスト優先順位侵害) x RFC1812 3 15 Precedence cutoff in effect (優先順位により遮断発動) x RFC1812 4 0 Source quench (輻輳発生による発信抑制) RFC792 5 0 Redirect for network (指定ネットワークへのリダイレクト要求) RFC792 5 1 Redirect for host (指定ホストへのリダイレクト要求) 5 2 Redirect for TOS and network (TOSとネットワークのリダイレクト要求) RFC792 5 3 Redirect for TOS and host (TOSとホストのリダイレクト要求) RFC792 8 0 Echo request(エコー要求) x RFC792 9 0 Router advertisement - Normal router advertisement (ルータ広告 - 通常通知) RFC1256 9 16 Router advertisement - Does not route common traffic (ルータ広告 - 通常トラフィックはルーティング不可) RFC2002 10 0 Route selection (ルート選択) RFC1256 11 0 TTL equals 0 during transit (搬送中にTTLが0に) x RFC792 11 1 TTL equals 0 during reassembly (再構成時の欠損フラグメント待機中に時間超過) x RFC792 12 0 IP header bad (catchall error) (IPヘッダ異常) (あらゆるエラーに共通) x RFC792 12 1 Required options missing (必要なオプションが欠如) x RFC1108 12 2 IP Header bad length (IPヘッダ長の異常) x RFC792 13 0 Timestamp request (obsolete) (タイムスタンプ要求) (廃) x RFC792 14 Timestamp reply (obsolete) (タイムスタンプ応答) (廃) x RFC792 15 0 Information request (obsolete) (情報要求) (廃) x RFC792 16 0 Information reply (obsolete) (情報応答) (廃) x RFC792 17 0 Address mask request (ネットマスク通知要求) x RFC950 18 0 Address mask reply (ネットマスク通知応答) x RFC950 20-29 Reserved for robustness experiment (信頼性試験のための予約域) Zaw-Sing Su 30 0 Traceroute x RFC1393 31 0 Datagram Conversion Error (データグラム変換エラー) x RFC1475 32 0 Mobile Host Redirect (移動体ホストのリダイレクト) David Johnson 33 0 IPv6 Where-Are-You (IPv6位置確認要求) x Bill Simpson 34 0 IPv6 I-Am-Here (IPv6位置確認応答) x Bill Simpson 35 0 Mobile Registration Request (移動体登録要求) x Bill Simpson 36 0 Mobile Registration Reply (移動体登録応答) x Bill Simpson 39 0 SKIP Tom Markson 40 0 Photuris RFC2521","link":"/2015/12/23/2015-12-24-iptables-allow-ping/"},{"title":"AWS Multi-AZにおける Pacemaker + Corosync による Elastic IP の付け替え","text":"概要Pacemaker &amp; Corosync によるAWS での Multi-AZ 間のEIP付け替えによるフェイルオーバーについて実装したのでまとめます。 以下イメージです。 通常状態 Avalavility Zone A に配置された Instance A で障害が発生した場合Avalavility Zone B に配置された Instance B に EIPを付け替え ToDo VPC, Subnet 設定 Pacemaker &amp; Corosync インストール / 設定 Cluster 構築 EIP付け替えスクリプト作成 フェイルオーバー試験 環境 CentOS 7 (x86_64) with Updates HVM (t2.micro)※ 検証用の為、 t2.microで実施しています。 VPC, Subnet 構築以下記事にて非常によくまとめて頂いているので参考にしていただきこの設定を以降そのまま利用します。 0から始めるAWS入門①：VPC編 念のため、以下 VPC, Subnet 設定です。 VPC 設定 項目 値 Name tag 任意 CIDR 10.0.0.0/16 tenancy Default Subnet 設定 項目 Subnet 1 Subnet 2 Name tag 任意(VPCのタグ名と関連付けたほうが管理しやすい) 任意(VPCのタグ名と関連付けたほうが管理しやすい) VPC 上記で作成したVPCを選択する 上記で作成したVPCを選択する Availability Zone ap-northeast-1a ap-northeast-1c CIDR 10.0.0.0/24 10.0.1.0/24 上記VPC設定に基づき以下設定していきます。 構築するイメージは以下になります。 セキュリティグループ作成今回作成するインスタンス2つにアタッチするセキュリティグループを事前に作成します。 マイIPからのSSHログイン許可 項目 設定値 セキュリティグループ名 VPC-for-EIP(任意) 説明 VPC-for-EIP(任意) VPC 上記で作成したVPCを選択する 作成したセキュリティグループ編集 フィルターで検索 ※以下は環境により変更してください。 送信元を作成したグループIDとし以下追加し保存 タイプ プロトコル ポート範囲 送信元 用途 すべてのTCP TCP 0 - 65535 作成したセキュリティグループID 今回は検証用の為、全解放。適宜設定変更してください。 すべてのICMP ICMP 0 - 65535 作成したセキュリティグループID ping疎通確認用。今回は検証用の為、全解放 適宜設定変更してください。 すべてのUDP UDP 0 - 65535 作成したセキュリティグループID corosyncで必要なポートはデフォルトで 5404 - 5405。 環境により設定変更する場合は注意が必要です。今回は検証用の為、全解放。適宜設定変更してください。 SSH TCP 20 マイIP 自PC端末からSSHログイン用。実環境で設定する必要はありません。 HTTP TCP 80 マイIP FailOver検証用。実環境で設定する必要はありません。 以上でインスタンスに適用するセキュリティグループの作成が完了しました。 ポリシー作成今回、以下コマンドを実行する必要があります。 コマンド 用途 aws ec2 associate-address ElasticIPをインスタンスに関連付ける aws ec2 disassociate-address ElasticIPをインスタンス関連付け解除 aws ec2 describe-addresses IPアドレスについて詳細取得 Identity &amp; Access Management ページにアクセス 「ポシリー作成」クリック 独自ポリシー作成 独自ポリシー情報入力 ポリシー名(任意) floatingElasticIP ポリシードキュメント 12345678910111213141516{ &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;ec2:Describe*&quot;, &quot;ec2:DisassociateAddress&quot;, &quot;ec2:AssociateAddress&quot; ], &quot;Resource&quot;: [ &quot;*&quot; ] } ]} 確認 IAMロール作成ElasticIPの付け替え権限を持ったロールを作成します。 新しいロールの作成 をクリック ロール名の設定 ロールタイプの選択Amazon EC2 の「選択」ボタンをクリック ポリシーのアタッチ 登録内容を確認しロールの作成 作成されたか確認 以上でインスタンスに適用するIAMロールの作成が完了しました。 ユーザ作成Identity &amp; Access Management ページにアクセス メニューからユーザクリック &amp; 新規ユーザの作成 ボタンクリック ユーザ名入力し作成ボタンクリック 認証情報をダウンロードクリックAccess Key Id と Secret Access Key が記載されたCSVがダウンロードされます。大切に保管しましょう。 作成されたユーザーにアクセス ポリシーのアタッチ開始 ポリシーにチェックを入れアタッチ 以上でAmazonEC2FullAccess 権限を持ったユーザ floatingIP ユーザが作成されました。 上記ユーザの認証情報は 手順 aws-cli インストール で利用します。 インスタンスの作成 上記で作成したVPCのSubnet (ap-northeast-1a)にインスタンス (以降Instance A)を作成します。 「インスタンスの作成」をクリック マシンイメージ選択今回は CentOS 7 (x86_64) with Updates HVM を選択します。 インスタンスタイプ選択今回は検証用で無料枠として利用したいので t2.micro を選択します。 インスタンスの詳細の設定ap-northeast-1a に作成する Instance A のプライマリIPを10.0.0.20 とします。 ストレージの追加特に変更することなく次の手順へ インスタンスのタグ付けName タグに Instance A と指定します。※ 任意なのでわかりやすいテキストであれば良いです。 セキュリティグループの設定 事前に作成したセキュリティグループを選択 インスタンス作成の確認 以上で Insntace A の作成が完了しました。 同様に Instance B 作成Instance A との主な変更点 Subnet 10.0.1.0/24 を選択 インスタンスのタグは Instance B とする Instance B 設定時注意点 セキュリティグループ は Instance B でも同様 Instance A で設定したセキュリティグループ を選択する。 送信元/送信先の変更チェックを無効化※上記 Instance A, B 共に Source/Destination Check (ネットワーク &gt; 送信元/送信先の変更チェック) を Disabled (無効) に設定する必要があります。 インスタンス SSHログイン後まずやること最低限必要なモジュールインストール※ git は ElasticIP付け替え時のシェルをインストールする際に必要になります。 12345[Instance A &amp; B ]# yum install -y git[Instance A &amp; B ]# git --versiongit version 1.8.3.1 Fail Over 検証用に httpd, php インストールあくまで Fail Over 時の動きを見る為の確認用にインストールし起動しています。※必須工程ではありません。 123456[Instance A &amp; B ]# yum --disableexcludes=main install -y gcc[Instance A &amp; B ]# yum install -y gmp gmp-devel[Instance A &amp; B ]# yum install -y php php-mysql httpd libxml2-devel net-snmp net-snmp-devel curl-devel gettext[Instance A &amp; B ]# echo '&lt;?php print_r($_SERVER[&quot;SERVER_ADDR&quot;]); ?&gt;' &gt; /var/www/html/index.php[Instance A &amp; B ]# systemctl start httpd[Instance A &amp; B ]# systemctl enable httpd system clock 調整 JST設定OS内の時間が現実の時間とずれているとaws-cliが正常に動作しない可能性があるので念の為、調整しておきます。 12345678910111213# バックアップ確保[Instance A &amp; B ]# cp /etc/sysconfig/clock /etc/sysconfig/clock.org# 再起動しても設定維持する様にする。[Instance A &amp; B ]# echo -e 'ZONE=&quot;Asia/Tokyo&quot;\\nUTC=false' &gt; /etc/sysconfig/clock# バックアップ確保[Instance A &amp; B ]# cp /etc/localtime /etc/localtime.org# Asia/Tokyo を localtime に設定[Instance A &amp; B ]# ln -sf /usr/share/zoneinfo/Asia/Tokyo /etc/localtime[Instance A &amp; B ]# date Elastic IP作成Elastic IPを作成し Server A に関連付けます。 新しいアドレスを割り当てる 確認ポップアップで「関連付ける」をクリック 成功確認 インスタンスに関連付け 関連するインスタンス選択 確認 以上で Instance A に ElasticIP が関連付けされました。 Instance A &amp; B に SSHログイン Instance A に SSHログイン 1[Local PC]# ssh -i aws.pem centos@&lt;Instance AのPublic IP&gt; Instance B に SSHログイン 1[Local PC]# ssh -i aws.pem centos@&lt;Instance BのPublic IP&gt; /etc/hosts設定12[Instance A ]# uname -nip-10-0-0-10.ap-northeast-1.compute.internal 12[Instance B ]# uname -nip-10-0-1-10.ap-northeast-1.compute.internal 1234567[Instance A &amp; B ]# vi /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6# 以下追加10.0.0.20 ip-10-0-0-20.ap-northeast-1.compute.internal10.0.1.20 ip-10-0-1-20.ap-northeast-1.compute.internal Pacemaker &amp; Corosync インストール pcsは旧来のcrmshに代わるPacemakerクラスタ管理ツールであり、RHEL/CentOS7においてはpcsの使用が推奨されている。 1[Instance A &amp; B ]# yum -y install pcs fence-agents-all バージョン確認 12345678910[Instance A &amp; B ]# pcs --version0.9.143[Instance A &amp; B ]# pacemakerd --versionPacemaker 1.1.13-10.el7Written by Andrew Beekhof[Instance A &amp; B ]# corosync -vCorosync Cluster Engine, version '2.3.4'Copyright (c) 2006-2009 Red Hat, Inc. hacluster パスワード設定corosyncパッケージインストール時に自動で hacluster ユーザが追加される。その hacluster のパスワードを設定する。 12345[Instance A &amp; B ]# passwd haclusterユーザー hacluster のパスワードを変更。新しいパスワード:新しいパスワードを再入力してください:passwd: すべての認証トークンが正しく更新できました。 pcsd 起動cluster監視を実施する為 123[Instance A &amp; B ]# systemctl start pcsd[Instance A &amp; B ]# systemctl enable pcsd[Instance A &amp; B ]# systemctl status pcsd cluster認証クラスタを組む各ホストへのアクセス認証を検証します。 どちらか一方のInstanceから実行します。以下はInstance Aから実行しています。 12345[Instance A ]# pcs cluster auth ip-10-0-0-20.ap-northeast-1.compute.internal ip-10-0-1-20.ap-northeast-1.compute.internalUsername: haclusterPassword:ip-10-0-1-20.ap-northeast-1.compute.internal: Authorizedip-10-0-0-20.ap-northeast-1.compute.internal: Authorized 上記のように Authorized (認証済み) と出力されていれば問題ありませんが以下のような Unable to Communicate というエラーが出力されている場合は各Instance の設定を見直してください。 認証エラーの例123[Instance A ]# pcs cluster auth ip-10-0-0-20.ap-northeast-1.compute.internal ip-10-0-1-20.ap-northeast-1.compute.internal -u hacluster -p ruby2015Error: Unable to communicate with ip-10-0-0-20.ap-northeast-1.compute.internalError: Unable to communicate with ip-10-0-1-20.ap-northeast-1.compute.internal cluster設定クラスタ設定をします。 12345678910111213141516[Instance A ]# pcs cluster setup --name aws-cluster ip-10-0-0-20.ap-northeast-1.compute.internal ip-10-0-1-20.ap-northeast-1.compute.internal --forceShutting down pacemaker/corosync services...Redirecting to /bin/systemctl stop pacemaker.serviceRedirecting to /bin/systemctl stop corosync.serviceKilling any remaining services...Removing all cluster configuration files...ip-10-0-0-20.ap-northeast-1.compute.internal: Succeededip-10-0-1-20.ap-northeast-1.compute.internal: SucceededSynchronizing pcsd certificates on nodes ip-10-0-0-20.ap-northeast-1.compute.internal, ip-10-0-1-20.ap-northeast-1.compute.internal...ip-10-0-0-20.ap-northeast-1.compute.internal: Successip-10-0-1-20.ap-northeast-1.compute.internal: SuccessRestaring pcsd on the nodes in order to reload the certificates...ip-10-0-0-20.ap-northeast-1.compute.internal: Successip-10-0-1-20.ap-northeast-1.compute.internal: Success cluster起動全ホストに向け クラスタ起動します。 1234[Instance A ]# pcs cluster start --allip-10-0-1-20.ap-northeast-1.compute.internal: Starting Cluster...ip-10-0-0-20.ap-northeast-1.compute.internal: Starting Cluster... aws-cli インストール手順: ユーザ作成 でダウンロードした credentials.csv に記載されたAccess Key Id Secret Access Key を使用します。 1234567891011[Instance A &amp; B ]# rpm -iUvh http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-5.noarch.rpm[Instance A &amp; B ]# yum -y install python-pip[Instance A &amp; B ]# pip --versionpip 7.1.0 from /usr/lib/python2.7/site-packages (python 2.7)[Instance A &amp; B ]# pip install awscli[Instance A &amp; B ]# aws configureAWS Access Key ID [None]: *********************AWS Secret Access Key [None]: **************************************Default region name [None]: ap-northeast-1Default output format [None]: json EIP 付け替えリソース作成heartbeat で問題検知した際に起動するリソースとして登録します。 OCF_ROOT が定数として指定されているが、存在しない為 1234567[Instance A &amp; B ]# cd /tmp[Instance A &amp; B ]# git clone https://github.com/moomindani/aws-eip-resource-agent.git[Instance A &amp; B ]# cd aws-eip-resource-agent[Instance A &amp; B ]# sed -i 's/\\${OCF_ROOT}/\\/usr\\/lib\\/ocf/' eip[Instance A &amp; B ]# mv eip /usr/lib/ocf/resource.d/heartbeat/[Instance A &amp; B ]# chown root:root /usr/lib/ocf/resource.d/heartbeat/eip[Instance A &amp; B ]# chmod 0755 /usr/lib/ocf/resource.d/heartbeat/eip pacemaker 設定stonish 無効化1[Instance A ]# pcs property set stonith-enabled=false split-brain (スプリット・ブレイン) が発生しても quorum (クォーラム) が特別な動作を行わないように設定1[Instance A ]# pcs property set no-quorum-policy=ignore split-brain とはハートビート通信を行うネットワークに断線などの問題が発生した場合、ホストに障害が起こったと勘違いし、本来立ち上がってほしくないスタンバイ側のホストがアクティブになってしまうというもの Split-Brain wiki Quorum wiki 属性値更新時の待ち時間( crmd-transition-delay )を 0s(秒) 設定1[Instance A ]# pcs property set crmd-transition-delay=&quot;0s&quot; Pacemaker-1.0.11 がリリースされました 自動フェイルバックなし、同一サーバでリソースの再起動を試みる回数を 1 回に設定1[Instance A ]# pcs resource defaults resource-stickiness=&quot;INFINITY&quot; migration-threshold=&quot;1&quot; EIP切り替え設定今回作成し Instance A に関連付けした ElasticIP は 52.192.203.215 です。以下設定に反映させます。 123456[Instance A ]# pcs resource create eip ocf:heartbeat:eip \\ params \\ elastic_ip=&quot;52.192.203.215&quot; \\ op start timeout=&quot;60s&quot; interval=&quot;0s&quot; on-fail=&quot;stop&quot; \\ op monitor timeout=&quot;60s&quot; interval=&quot;10s&quot; on-fail=&quot;restart&quot; \\ op stop timeout=&quot;60s&quot; interval=&quot;0s&quot; on-fail=&quot;block&quot; cluster 設定確認12345678910111213141516171819202122232425262728293031323334353637[Instance A ]# pcs configpcs configCluster Name: aws-clusterCorosync Nodes: ip-10-0-0-20.ap-northeast-1.compute.internal ip-10-0-1-20.ap-northeast-1.compute.internalPacemaker Nodes: ip-10-0-0-20.ap-northeast-1.compute.internal ip-10-0-1-20.ap-northeast-1.compute.internalResources: Resource: eip (class=ocf provider=heartbeat type=eip) Attributes: elastic_ip=52.192.203.215 Operations: start interval=0s timeout=60s on-fail=stop (eip-start-interval-0s) monitor interval=10s timeout=60s on-fail=restart (eip-monitor-interval-10s) stop interval=0s timeout=60s on-fail=block (eip-stop-interval-0s)Stonith Devices:Fencing Levels:Location Constraints:Ordering Constraints:Colocation Constraints:Resources Defaults: resource-stickiness: INFINITY migration-threshold: 1Operations Defaults: No defaults setCluster Properties: cluster-infrastructure: corosync cluster-name: aws-cluster crmd-transition-delay: 0s dc-version: 1.1.13-10.el7-44eb2dd have-watchdog: false no-quorum-policy: ignore stonith-enabled: false Fail Over 実行確認手順 Fail Over 検証用に httpd, php インストール にてDocumentRoot (/var/www/html/) にPrivate IP ($_SERVER[&quot;SERVER_ADDR&quot;]) を表示させるindex.phpファイルを配置しました。 ブラウザから Private IP を元に Instance A or B どちらのInstance にアクセスしているかがわかります。 ブラウザから ElasticIP にアクセスElasticIP 52.192.203.215 にアクセスするとPrivate IP 10.0.0.20 が表示されていることがわかります。 現状、ElasticIP は Instance A に関連付いていることがわかります。 Instance A の corosync 停止1[Instance A]# systemctl stop corosync 再度ブラウザから ElasticIP にアクセス先ほど表示させていたブラウザを幾度かリロードするとPrivate IP 10.0.1.20 が表示されていることがわかります。 ElasticIP は Instance B に関連付けられたことがわかります。 ElasticIP が Instance A から関連付けが解放され、 Instance B に関連付けされるようになりました。 コンソールページ上でも確認することができます。 以上で簡易的ではありますがCloud Design Pattern の floating IP (ElasticIP) が実現できました。 以上です。 参考 CentOS7.1でPacemaker+corosyncによるクラスタを構成する PacemakerでEIPを付替えるResource Agentを書いてみた yum updateしたらcrmコマンドが無くなった！(pcsコマンド対照表) pacemaker + corosync で HA クラスタ構築 動かして理解するPacemaker ～CRM設定編～","link":"/2015/12/25/2015-12-26-aws-multi-az-pacemaker-corosync-switching-eip/"},{"title":"MySQLトラブルシューティング - ERROR 1045 (28000): Access denied for user &#39;root&#39;@&#39;localhost&#39; (using password: NO)","text":"概要ローカルでMacOSXでMAMPを使っていてある日誤った操作により以下のようなエラーが発生した為対応まとめました。 12$ mysql -u rootERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO) ローカル環境でrootユーザのアクセス権を誤って削除してしまったときなどに発生します。 対策1234567891011121314151617181920212223242526272829303132333435363738394041// mysql停止$ service mysqld stop// rootユーザのアクセス権限が失われている// → テーブル権限をスキップ(無視)して作業するオプション付きでセーフモードでmysql起動$ mysqld_safe --skip-grant-tables &amp;// rootユーザでアクセス$ mysql -u root// 現状の権限設定テーブルを空にするmysql&gt; TRUNCATE TABLE mysql.user;Query OK, 0 rows affected (0.00 sec)// 権限反映mysql&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.01 sec)// rootユーザにlocalhostの全DBアクセス権限付与mysql&gt; GRANT ALL PRIVILEGES on *.* to root@localhost IDENTIFIED BY '(root's password)' WITH GRANT OPTION;Query OK, 0 rows affected (0.01 sec)// 権限反映mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)// 権限設定確認mysql&gt; SELECT host, user FROM mysql.user;// mysql CLIモード停止 (Ctl+c)でもOKmysql&gt; quit;// safe modeで起動させたmysql 関連psをkillする$ ps aux | grep mysql | grep -v grep | awk '{print &quot;kill -9&quot;, $2}'|sh// mysql 起動$ service mysqld start// rootユーザでmysqlアクセス$ mysql -u root -p(root's password)mysql&gt; rootユーザに権限が失われお手上げ状態になったとき、困ったときに safe_mode 使えます。 以上","link":"/2016/01/14/2016-01-15-mysql-error-1045-28000-access-denied-for-user/"},{"title":"Hubot + Slack on Amazon Linux","text":"Prerequisite Amazon Linux AMI 2015.09.1 (HVM), SSD Volume Type(t2.nano) npm 1.3.6 hubot 2.17.0 coffeescript 1.10.0 I use t2.nano released at December 2015. first to do. summarized in the following site. AWS EC2 Amazon Linuxインスタンス起動後、最初にやることまとめ Install npm1234$ sudo yum install -y npm --enablerepo=epel$ npm -v1.3.6 Install hubot, coffee-script, yo, generator-hubot1234567$ sudo npm install -g hubot coffee-script yo generator-hubot$ hubot -v2.17.0$ coffee -vCoffeeScript version 1.10.0 create hubot1234567891011121314151617181920212223242526272829303132333435$ mkdir mybot$ cd mybot$ yo hubot? ==========================================================================We're constantly looking for ways to make yo better!May we anonymously report usage statistics to improve the tool over time?More info: https://github.com/yeoman/insight &amp; http://yeoman.io========================================================================== (Y/n) n _____________________________ / \\ //\\ | Extracting input for | ////\\ _____ | self-replication process | //////\\ /_____\\ \\ / ======= |[^_/\\_]| /---------------------------- | | _|___@@__|__ +===+/ /// \\_\\ | |_\\ /// HUBOT/\\\\ |___/\\// / \\\\ \\ / +---+ \\____/ | | | //| +===+ \\// |xx|? Owner (User &lt;user@example.com&gt;) tech@xxxxxxxx.jp? Bot name (mybot) hubot? Description xxxxxxx's hubot? Bot adapter slack......hubot-maps@0.0.2 node_modules/hubot-maps$ lsProcfile bin hubot-scripts.json package.jsonREADME.md external-scripts.json node_modules scripts こんなエラーが出たときは/root/.config へのアクセス権限がないと言われています。 1234567891011Error: EACCES, permission denied '/root/.config' at Object.fs.mkdirSync (fs.js:654:18) at sync (/usr/lib/node_modules/yo/node_modules/configstore/node_modules/mkdirp/index.js:71:13) at Function.sync (/usr/lib/node_modules/yo/node_modules/configstore/node_modules/mkdirp/index.js:77:24) at Object.create.all.get (/usr/lib/node_modules/yo/node_modules/configstore/index.js:38:13) at Object.Configstore (/usr/lib/node_modules/yo/node_modules/configstore/index.js:27:44) at new Insight (/usr/lib/node_modules/yo/node_modules/insight/lib/index.js:37:34) at Object.&lt;anonymous&gt; (/usr/lib/node_modules/yo/lib/cli.js:156:11) at Module._compile (module.js:456:26) at Object.Module._extensions..js (module.js:474:10) at Module.load (module.js:356:32) 全権限付与 12# mkdir /root/.config# chmod -R 0777 /root/.config 上記で問題なくyo コマンドが通りました。ほっ(-o-) Initial setting hubot. Item Value Owner E-mail address Bot name same the hubot integrated slack Descriiption - Bot adapter slack Install hubot-slack1$ sudo npm install hubot-slack --save modify external-scripts.json12345$ echo '[]' &gt; external-scripts.json// confirm setting.$ cat external-scripts.json[] Add to bin/hubot1$ vim bin/hubot run hubot via port 80 12345678910111213#!/bin/shset -enpm installexport PATH=&quot;node_modules/.bin:node_modules/hubot/node_modules/.bin:$PATH&quot;# add startexport HUBOT_SLACK_TOKEN=xoxb-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxexport PORT=80# add endexec node_modules/.bin/hubot --name &quot;hubot&quot; &quot;$@&quot; creat hubot script1$ vim scripts/hello.coffee save the belowing scripts as scripts/hello.coffee 123456789101112131415161718192021# Description:# hubot basic command.## Commands:# hubot who are you - I'm hubot!# hubot hello# hubot who am I - You are &lt;user_name&gt;# hubot what is this &lt;*&gt; - This is &lt;$1&gt;module.exports = (robot) -&gt; robot.respond /who are you/i, (msg) -&gt; msg.send &quot;I'm hubot!&quot; robot.hear /HELLO$/i, (msg) -&gt; msg.send &quot;hello!&quot; robot.respond /who am I/i, (msg) -&gt; msg.send &quot;You are #{msg.message.user.name}&quot; robot.respond /what is this (.*)/i, (msg) -&gt; msg.send &quot;This is #{msg.match[1]}&quot; run hubotsudo is required and in order to access http port.You will require hubot integrated with outside site - ex) JIRA etc… 12345$ sudo bin/hubot -a slack[Wed Jan 13 2016 13:43:08 GMT+0900 (JST)] INFO Connecting...[Wed Jan 13 2016 13:43:10 GMT+0900 (JST)] INFO Logged in as hubot of RUBY GROUPE, but not yet connected[Wed Jan 13 2016 13:43:11 GMT+0900 (JST)] INFO Slack client now connected no sudo execution result …(&gt;_&lt;)Error occured!! 12345678910111213$ bin/hubot -a slack[Wed Jan 13 2016 16:40:59 GMT+0900 (JST)] INFO Connecting...[Wed Jan 13 2016 16:40:59 GMT+0900 (JST)] ERROR Error: listen EACCES at errnoException (net.js:905:11) at Server._listen2 (net.js:1024:19) at listen (net.js:1065:10) at net.js:1147:9 at dns.js:72:18 at process._tickCallback (node.js:442:13)[Wed Jan 13 2016 16:41:00 GMT+0900 (JST)] INFO Logged in as hubot of RUBY GROUPE, but not yet connected[Wed Jan 13 2016 16:41:02 GMT+0900 (JST)] INFO Slack client now connected Invite hubot at Slackex) botname=hubot 1/invite @hubot Input text at Slack12[me] hubot hello[hubot] hello! daemonize hubotInstall forever1234$ sudo npm install --save forever$ forever --versionv0.15.1 set path as not root user.12$ echo 'export PATH=$PATH:/home/&lt;user&gt;/mybot/node_modules/forever/bin' &gt;&gt; ~/.bashrc$ source ~/.bashrc root user too. 123456// change to super user.$ sudo su# echo 'export PATH=$PATH:/home/&lt;user&gt;/mybot/node_modules/forever/bin' &gt;&gt; ~/.bashrc# source ~/.bashrc# echo $PATH.......:/home/&lt;user&gt;/mybot/node_modules/forever/bin:...... Modify bin/hubot daemonize start hubot - sudo bin/hubot -a slack1$ sudo bin/hubot start stop daemonized hubot1$ sudo bin/hubot stop restart hubot1$ sudo bin/hubot restart check forever list123456$ sudo su# forever listinfo: Forever processes runningdata: uid command script forever pid id logfile uptimedata: [0] sICk coffee node_modules/.bin/hubot -a slack 30494 30496 /root/.forever/sICk.log 0:0:5:35.399 another way stop forever process check process. kill -9 process 1234567$ ps aux | grep hubot | grep -v greproot 31144 0.3 3.1 723820 32392 ? Ssl 17:58 0:00 /usr/bin/node /home/hu/mybot/node_modules/forever/bin/monitor node_modules/.bin/hubotroot 31146 1.1 6.7 979588 69196 ? Sl 17:58 0:00 node node_modules/hubot/node_modules/.bin/coffee /home/hu/mybot/node_modules/.bin/hubot -a slack// kill forcibly process$ sudo kill -9 31146 another one. 1# ps aux | grep hubot | grep -v grep | awk '{print &quot;kill -9&quot;, $2}' | sh Thanks.","link":"/2016/01/12/2016-01-13-hubot-slack-on-amazonlinux/"},{"title":"pod install で ASSERSION対策","text":"pod install で ASSERSION対策 概要pod install 実行時に正常にインストール処理ができなかったのでメモ。 冬休み中にSwift2.0をほんきで学ぶ為、以下書籍購入してチャレンジ。 ほんきで学ぶSwift+iOSアプリ開発入門 Swift2, Xcode7, iOS9対応 | 加藤 勝也 |本 | 通販 | AmazonAmazonで加藤 勝也のほんきで学ぶSwift+iOSアプリ開発入門 Swift2, Xcode7, iOS9対応。アマゾンならポイント還元本が多数。加藤 勝也作品ほか、お急ぎ便対象商品は当日お届けも可能。またほんきで学ぶSwift+iOSアプリ開発入門 Swift2, Xco… これの LESSON 31 でCocoaPodsインストール手順がありますがその通りに進めた際にエラー発生したので対策をまとめました。 この手のエラーは環境依存が主ですし、書籍の執筆時も技術は日進月歩進んでおりますので書籍を責めずCocoaPodsオフィシャルサイトやStackOverFlowなどで情報探ってみるのが精神衛生上も良いかもしれないです。 環境情報 Xcode 7.2 (7C68) ruby 2.0.0p645 gem 2.5.0 書籍に記載されているコマンドCocoaPodsインストール1$ sudo gem install -n /usr/local/bin cocoa pods エラー内容123456789101112131415161718192021222324252627282930$ pod install[!] Unable to load a specification for the plugin `/opt/homebrew-cask/Caskroom/cocoapods/0.37.0/CocoaPods.app/Contents/Resources/bundle/lib/ruby/gems/2.2.0/gems/cocoapods-plugins-install-0.0.1`Analyzing dependenciesCocoaPods 1.0.0.beta.1 is available.To update use: `sudo gem install cocoapods --pre`[!] This is a test version we'd love you to try.For more information see http://blog.cocoapods.organd the CHANGELOG for this version http://git.io/BaH8pQ.Downloading dependenciesInstalling Realm (0.97.0)Installing RealmSwift (0.97.0)Generating Pods project2015-12-31 10:51:38.680 ruby[6275:47591] [MT] DVTAssertions: ASSERTION FAILURE in /Library/Caches/com.apple.xbs/Sources/IDEFrameworks/IDEFrameworks-9548/IDEFoundation/Initialization/IDEInitialization.m:590Details: Assertion failed: _initializationCompletedSuccessfullyFunction: BOOL IDEIsInitializedForUserInteraction()Thread: &lt;NSThread: 0x7fc66d067980&gt;{number = 1, name = main}Hints: NoneBacktrace: 0 0x000000010e001f7f -[DVTAssertionHandler handleFailureInFunction:fileName:lineNumber:assertionSignature:messageFormat:arguments:] (in DVTFoundation) 1 0x000000010e00170c _DVTAssertionHandler (in DVTFoundation) 2 0x000000010e001978 _DVTAssertionFailureHandler (in DVTFoundation) 3 0x000000010e0018da _DVTAssertionFailureHandler (in DVTFoundation) 4 0x0000000110e5154d IDEIsInitializedForUserInteraction (in IDEFoundation) 5 0x000000011393a631 +[PBXProject projectWithFile:errorHandler:readOnly:] (in DevToolsCore) 6 0x000000011393c1b6 +[PBXProject projectWithFile:errorHandler:] (in DevToolsCore) 7 0x00007fff93b2af14 ffi_call_unix64 (in libffi.dylib)zsh: abort pod install 対策12345# 既にインストール済みのCocoaPodsをアンインストールする$ sudo gem uninstall cocoapods# 再度 `cocoapods` をインストールする。$ sudo gem install -n /usr/local/bin cocoapods 実行確認1234567891011121314151617181920212223$ pod installUpdating local specs repositoriesCocoaPods 1.0.0.beta.1 is available.To update use: `gem install cocoapods --pre`[!] This is a test version we'd love you to try.For more information see http://blog.cocoapods.organd the CHANGELOG for this version http://git.io/BaH8pQ.Analyzing dependenciesDownloading dependenciesInstalling Realm (0.97.0)Installing RealmSwift (0.97.0)Generating Pods projectIntegrating client project[!] Please close any current Xcode sessions and use `Chapter6.xcworkspace` for this project from now on.Sending statsPod installation complete! There is 1 dependency from the Podfile and 2 total podsinstalled.~/honki_swift/Chapter6/LESSON31/before/Chapter6 無事、 .xcworkspace, Podfile.lock, Pods が作成されました。 12345678[~/honki_swift/Chapter6/LESSON31/before/Chapter6]$ tree -L 1.├── Chapter6├── Chapter6.xcodeproj├── Chapter6.xcworkspace├── Podfile├── Podfile.lock└── Pods 以上です。","link":"/2015/12/30/2015-12-31-pod-install-assersion/"},{"title":"MySQLトラブルシューティング - mysqldump: Couldn&#39;t execute &#39;FLUSH TABLES&#39;: Access denied; you need (at least one of) the RELOAD privilege(s) for this operation (1227)","text":"概要以下のように mysqldump コマンド実行時に掲題のエラーが発生しました。 123$ mysqldump --lock-all-tables --events -h &lt;host_name&gt; -u &lt;user&gt; -p&lt;password&gt; --no-create-info &lt;db_name&gt; &lt;table, ...&gt; &gt; output.sqlmysqldump: Couldn't execute 'FLUSH TABLES': Access denied; you need (at least one of) the RELOAD privilege(s) for this operation (1227) 対策エラー文の通り、RELOAD権限を付与する。 12mysql&gt; GRANT RELOAD ON *.* TO '&lt;user&gt;'@'&lt;host_name&gt;';mysql&gt; FLUSH PRIVILEGES; 以上","link":"/2016/01/18/2016-01-19-mysqldump-couldnt-execute-flush-tables-access-denied/"},{"title":"CentOS7用 Revel(Golang)フレームワークの起動スクリプト 書いてみた。","text":"起動スクリプト作成まず成果物から 1# vim /usr/lib/systemd/system/revel.service 12345678910[Unit]Description=RevelBuildScriptAfter=nginx.service mysqld.service[Service]Type=simpleExecStart=/bin/bash /var/golang/run.sh[Install]WantedBy=multi-user.target ※After … 上記では nginx と mysqld 起動後に revelを起動させるという設定です。 ※ExecStart … /bin/bash /var/golang/run.sh については以前の記事でローカルビルドによるデプロイ方法を採用しているというお話をしましたがその際に作成される run.sh のパスを指しています。 Golang Revelフレームワーク ホットデプロイ方法 - 長生村本郷Engineers'Blog概要 Revel Officialサイトにあるデプロイ方法を検証しました。 Revel Deployment ローカルでアプリをビルドしサーバにコピーする サーバーで更新したコードをpullし、ビルド・起動する Heroku を利用しデプロイ管理する 1. ローカルビルド # ア… 起動設定1# systemctl enable revel.service 起動1# systemctl start revel.service 停止1# systemctl start revel.service 以上です。 導入経緯AWSでの運用をしていると検証環境は検証時のみに利用し余計なコストは掛けたくないものです。 なので、しょっ中、起動・停止を繰り返します。 Revelフレームワークは起動スクリプトが標準装備されていない為インスタンス起動時に手動で起動する手間が発生していました。 その為、デザインの修正でもシステムさんお願いします〜というような依頼があり相互に手間が発生していたのでその解決として作成しました。 現在は Slack経由で hubot から Jenkins ジョブを実行させRevelの乗ってるAWSインスタンスの起動・停止できるようにしています。 ipもElasticIPは使用せず、No-IPを利用してPublic IPが変更されても同ドメインでアクセスできるようにしています。 こちらもインスタンス起動時にドメイン管理しているNo-ipへPublic IPを通知し動的にドメインとIPを紐付けるようにしています。 no-ipでAWSインスタンスの動的ip更新対応 ~いつも同じドメイン名でアクセスしたい~ - 長生村本郷Engineers'Blog概要 AWSの起動停止をするとElasticIPを設定していない限り Public IPが変更されてしまいます。 ElasticIPは設定するとAWSを停止していても費用が発生します。 検証用環境など一時的に利用するインスタンスについて 起動時にIPが変更したことを関係者に周知す… http://www.noip.com/ 極力費用を抑えた、AWSにおける検証環境構築の参考にもしていただければと思います。 以上です。 ※ 検証環境 はローカル開発環境と異なり、あくまで本番デプロイ前の検証用という認識です。","link":"/2016/02/02/2016-02-03-buildscript-for-revel-go/"},{"title":"no-ipでAWSインスタンスの動的ip更新対応 ~いつも同じドメイン名でアクセスしたい~","text":"概要AWSの起動停止をするとElasticIPを設定していない限りPublic IPが変更されてしまいます。 ElasticIPは設定するとAWSを停止していても費用が発生します。 検証用環境など一時的に利用するインスタンスについて起動時にIPが変更したことを関係者に周知するなどの手間が掛かります。 その為No-IPを利用しドメインを固定しIP変更に対応するようにしました。 No-IPは無料のドメインサービスで動的IP変更を検知するLinux用モジュールも配布しています。 環境 Amazon Linux AMI release 2015.09 noip 2.1.9 手順まずnoipサイトで会員登録し利用したいドメインを登録します。ipはとりあえず適当で良いです。 http://www.noip.com/ 1234567891011121314151617181920// rootユーザで実行$ sudo su# cd /usr/local/src// noipモジュール# wget http://www.no-ip.com/client/linux/noip-duc-linux.tar.gz# tar xzf noip-duc-linux.tar.gz# cd noip-2.1.9# make# make install// 起動スクリプト作成# cp redhat.noip.sh /etc/init.d/noip# chmod 755 /etc/init.d/noip// 起動設定# /sbin/chkconfig noip on// 起動# /etc/init.d/noip start 起動後、no-ipのコンソール上で指定ドメインのIPが1分もしない程度で切り替わっていることが確認できます。 今後No-IPはMicroSoftによりマルウェアの温床となっておりユーザを保護すべく22のNO-IPドメイン差し止めを連邦裁判所に申し立て、受理されましたが No-IP側としては相談していただければ対応もできた、とし申し立て後対応し随時ドメインの復活を果たしています。 ある程度セキュリティを加味して利用する必要がありますね。今の所、AWSのセキュリティグループで特に外部アクセスはなく問題なく動作しています。 また、以下のようなGREEさんの記事がありました。 AWS EC2 での最強の Public IP 取得方法 内部関係者に聞いてみたいと思います。 ===追記=== GREEさんの記事の件、内部関係者に聞いた所ubuntuのみで利用しているそうです。","link":"/2016/02/03/2016-02-04-no-ip-aws-instance-dynamic-ip/"},{"title":"MacOSX python 3.4.3 インストール","text":"概要データサイエンティスト養成読本 機械学習入門編 (Software Design plus) | 比戸 将平, 馬場 雪乃, 里 洋平, 戸嶋 龍哉, 得居 誠也, 福島 真太朗, 加藤 公一, 関 喜史, 阿部 厳, 熊崎 宏樹 |本 | 通販 | AmazonAmazonで比戸 将平, 馬場 雪乃, 里 洋平, 戸嶋 龍哉, 得居 誠也, 福島 真太朗, 加藤 公一, 関 喜史, 阿部 厳, 熊崎 宏樹のデータサイエンティスト養成読本 機械学習入門編 (Software Design plus)。アマゾンならポイント還元本が多数。比戸… 機械学習養成読本という素晴らしい本を頂き早速学習を深めています。 115ページ 第2部 第1章「Pythonのインストール」がすんなりいかなかったのでメモです。 pyenv install 3.4.3 を実行すると以下のようなエラー出ませんでしたか？ 123456789Downloading Python-3.4.3.tgz...-&gt; https://www.python.org/ftp/python/3.4.3/Python-3.4.3.tgzInstalling Python-3.4.3...ERROR: The Python ssl extension was not compiled. Missing the OpenSSL lib?Please consult to the Wiki page to fix the problem.https://github.com/yyuu/pyenv/wiki/Common-build-problemsBUILD FAILED (OS X 10.11.2 using python-build 20150519) 環境 MacOSX ElCapitan 10.11.2（15C50） Homebrew 0.9.5 12345678$ brew install sqlite3$ brew install readline$ brew install openssl$ brew install pyenv$ export CFLAGS=&quot;-I$(brew --prefix openssl)/include&quot;$ export LDFLAGS=&quot;-L$(brew --prefix openssl)/lib -L$(brew --prefix sqlite3)/lib&quot;$ export CPPFLAGS=&quot;-I$(brew --prefix sqlite3)/include&quot;$ pyenv install 3.4.3 以下コンパイラに渡す変数の設定が肝でした。 CFLAGS LDFLAGS CPPFLAGS 10.3 Variables Used by Implicit Rules 気をつければいけないのはMacにデフォルトでインストールされているPythonPython のPATH(/usr/local/bin)から外す 1234$ which python/usr/local/bin/python$ mv /usr/local/bin/python /usr/local/bin/python2.7.10 pip もインストール済みであるならば同様にパスから外す。 1234$ which pip/usr/local/bin/pip$ mv /usr/local/bin/pip /usr/local/bin/pip2.7 他にPythonのPATHをexportしていなければpyenv でインストールしたPythonにパスが通るはずです。 12$ which python/Users/kenzo/.pyenv/shims/python ←このように表示されればOK♪","link":"/2016/02/18/2016-02-19-install-python343-on-macos/"},{"title":"Pythonエラー対応: UnicodeEncodeError: &#39;ascii&#39; codec can&#39;t encode characters","text":"結論Pythonの文字コードを utf-8 に設定する。 概要python で以下のような画像URLから画像をダウンロードする処理を実装した所掲題のエラーが発生しました。 1234567891011121314# -*- coding: utf-8 -*-import urllibimport urllib2import os.pathimport sysfrom HTMLParser import HTMLParserdef download(url): img = urllib.urlopen(url) localfile = open(os.path.basename(url),'wb') localfile.write(img.read()) img.close() localfile.close() 具体的にはここでこけてました。 1localfile.write(img.read()) 環境 CentOS Linux release 7.0.1406 (Core) Python 2.7.5 文字コード確認以下対話式で確認してみると ascii と表示されました。これを utf-8 に変更します。 1234$ python&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.getdefaultencoding()'ascii' pip パス確認バージョン確認時にパスが出力されます。 12$ pip --versionpip 7.1.0 from /usr/lib/python2.7/site-packages (python 2.7) site-packages配下に sitecustomize.py を作成1vi /usr/lib/python2.7/site-packages/sitecustomize.py 12import syssys.setdefaultencoding('utf-8') 上記内容で保存。 再度文字コード確認utf-8 になりました。 1234$ python&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.getdefaultencoding()'utf-8' これで掲題のエラーが解決されました。","link":"/2016/02/15/2016-02-16-python-error-unicodeencodeerror-ascii-codec-cant-encode-characters/"},{"title":"Clam AntiVirus 導入","text":"Clam Antivirus略称ClamAV はUnix系OSで動作するウィルススキャンOSSです。 http://www.clamav.net wiki - Clam_AntiVirus 個人的にさくらVPSを借りてミドルウェア入れて動作確認していたら気づくとDos攻撃を受けて「サーバ停止します」と連絡が来て焦ったことがあり、導入した経緯があります。 導入手順以下2つの方法どちらでも良いです。yum 経由の方が起動スクリプト付きでパスに配置してくれるので導入が安易です。 yum 経由 1# yum install clamav clamav-update ソースからbuild 1234567# cd /usr/local/src# wget http://www.clamav.net/downloads/production/clamav-0.99.tar.gz# tar zxf clamav-0.99.tar.gz# cd clamav-0.99# ./configure --enable-milter# make# make install 設定ファイル更新/etc/clamd.conf 12345// 定義ファイル更新 User clalmのコメントアウトを外す# sed -i 's/^User\\s\\+clam$/#\\0/' /etc/clamd.conf// 設定更新# freshclam 起動設定12345// 起動# service clamd start// 自動起動設定# chkconfig clamd on 実行確認1234567891011# clamscan --infected --remove --recursive----------- SCAN SUMMARY -----------Known viruses: 4269611Engine version: 0.99Scanned directories: 2Scanned files: 8Infected files: 0Data scanned: 0.20 MBData read: 0.10 MB (ratio 1.92:1)Time: 10.934 sec (0 m 10 s) Option Explain –infected ウィルス感染されたファイルのみ表示 –remove ウィルス感染されたファイルを削除 –recursive サブディレクトリを再帰的に検査 各option は clamscan -h で確認できます。 但しあくまでウィルススキャンツールを導入しただけであって日進月歩の技術で侵入されないとは限らない為セキュリティは常に動向に注意しましょう。","link":"/2016/02/22/2016-02-22-clam-antivirus/"},{"title":"PHP+OpenSSLバージョンアップ","text":"概要ベリトランスモジュールのバージョンアップに際して2016年以内にSSL v3.0/TLS 1.0無効化処理が必須となりました。 世界的なセキュリティ対策の一環として必須事項なのでベリトランス以外の決済も、また決済以外でもシステムの対策必須です。 Google、SSL 3.0の脆弱性「POODLE」を公表、SSL 3.0は今後サポート廃止の意向 EC-CUBEでベリトランスの決済モジュールではPHPからOpenSSLライブラリを利用して決済へ通信を実施しています。 そのPHPのOpenSSLライブラリを1.0.1i以上(最新が推奨)にバージョンアップする必要があります。 対応する ToDoとしては以下になります。 TLS1.1以上を利用するには openssl 1.0.1i 以上を利用する必要アリ→ openssl バージョンアップ (1.0.1i以上)→ PHP の再コンパイルし OpenSSLライブラリ(1.0.1i以上)をバージョンアップ 上記対応をまとめました。 ※ ApacheのSSL v3.0/TLS1.0利用不可設定は別途ググればすぐ出てきます♪ 環境 CentOS release 6.6 (Final) PHP 5.3.9 openssl 1.0.1g 手順PHP で利用している OpenSSL のライブラリバージョン確認これからPHPをコンパイルし直すのでOpenSSL support が disableでも問題ないです。 ※今回では既にインストール済みであるというケースを想定しています。 123456# php -i | grep OpenSSLOpenSSL support =&gt; enabledOpenSSL Library Version =&gt; OpenSSL 1.0.1g 28 Jan 2016OpenSSL Header Version =&gt; OpenSSL 1.0.1g 28 Jan 2016OpenSSL support =&gt; enabled PHPでで利用されるOpenSSL Library, Header Version が 1.0.1g であることが確認できました。 既存openssl バックアップ既にインストール済みかと思いますので現行バージョンを一旦退避します。 12345678# openssl version1.0.1g# which openssl/usr/local/bin/openssl// 名前変更でバックアップとして残す# mv /usr/local/bin/openssl /usr/local/bin/openssl1.0.1g 何か不具合が発生した場合にまきもどせるように、念のためバックアップをとりました。 openssl バージョンアップソースからビルドします。 1234567# cd /usr/local/src# wget http://www.openssl.org/source/openssl-1.0.2f.tar.gz# tar xzvf openssl-1.0.2f.tar.gz# cd openssl-1.0.2f# ./config --prefix=/usr/local shared -fPIC# make# make install -fPIC は 動的共有オブジェクト(DSO)としてmod_sslをビルドしPHPのバイナリ実行ファイルからフックして利用する為に必要、と言ったところでしょうか。 openssl バージョンアップ確認正しくバージョンアップされていることを確認しました。 123# openssl versionOpenSSL 1.0.2f 28 Jan 2016 PHP再コンパイル 既存PHPがどのようにconfigureされているか確認 123# php -i | grep configConfigure Command =&gt; './configure' '--enable-mbstring' '--enable-zend-multibyte' '--with-mysql' '--with-mysqli' '--enable-mbregex' '--with-gd' '--with-jpeg-dir=/usr/lib' '--with-png-dir=/usr/lib' '--with-freetype-dir=/usr/lib' '--with-zlib-dir' '--with-libdir=lib64' '--enable-soap' '--with-apxs2=/etc/httpd/bin/apxs' '--with-openssl=/usr/local' --with-openssl がない場合は上記のように追加します。今回は既に指定済みです。 上記のconigure情報を利用して–with-opensslがあることを確認した上で再コンパイルします。 再コンパイル 1234# cd /usr/local/src/php-5.3.9# ./configure --enable-mbstring --enable-zend-multibyte --with-mysql --with-mysqli --enable-mbregex --with-gd --with-jpeg-dir=/usr/lib --with-png-dir=/usr/lib --with-freetype-dir=/usr/lib --with-zlib-dir --with-libdir=lib64 --enable-soap --with-apxs2=/etc/httpd/bin/apxs --with-openssl=/usr/local# make# make install 以下のようなエラーが出る場合は、手順)「openssl バージョンアップ」をご確認下さい。新たにopensslをソースからコンパイルしてビルドしたときなどに出るエラーです。 1configure: error: Cannot find OpenSSL's &lt;evp.h&gt; PHPで利用するOpenSSL ライブラリのバージョン確認123456# php -i | grep OpenSSLOpenSSL support =&gt; enabledOpenSSL Library Version =&gt; OpenSSL 1.0.2f 28 Jan 2016OpenSSL Header Version =&gt; OpenSSL 1.0.2f 28 Jan 2016OpenSSL support =&gt; enabled OpenSSL Library, Header Version が共に指定通りとなりました。 以上です。","link":"/2016/02/22/2016-02-22-versionup-php-openssl/"},{"title":"Install latest Git on CentOS.","text":"Environment Information CentOS release 6.6 (Final) Install required modules1# yum -y install curl-devel expat-devel gettext-devel openssl-devel zlib-devel perl-ExtUtils-MakeMaker Compile &amp; InstallPlease access the below url, get latest archive.Official Repository 123456# cd /usr/local/src# wget https://www.kernel.org/pub/software/scm/git/git-2.7.2.tar.gz# tar xvf git-2.7.2.tar.gz# cd git-2.7.2# make prefix=/usr/local all# make prefix=/usr/local install Show git version.123# git --versiongit version 2.7.2 Thank.","link":"/2016/02/22/2016-02-23-install-latest-git-on-centos/"},{"title":"無料SSL証明書発行しセキュリティでA+を取る！ 〜Apache編〜","text":"概要ベリトランスモジュールバージョンアップに際してSHA256で発行したSSL証明書が必須となりました。 テスト環境でベリトランスモジュールバージョンアップ試験を行う際にSSL証明書導入する必要が生じました。 本番環境と同じ有料SSLを導入するまでとはいかずとも近しい状況を構築する必要があったので無料SSL証明書を発行し導入しました。 上記の手順をまとめましたので以下に記載します。 環境AWS Marketplace: CentOS 6 (x86_64) - with Updates HVM を利用しています。 CentOS release 6.7 (Final) Apache 2.4.12 手順CSR生成事前準備として、SSLインストール対象サーバでCSRを生成しておきます。 SHA256 対応のCSR生成方法について以下ご参考ください。 Apache + OpenSSL でSHA256対応CSR生成 - 長生村本郷Engineers'Blogドメイン登録者情報確認 ※特にドメイン登録者を明確に指定する必要がなければ、「手順」へ進んでください。企業認証周りが必要な場合は 事前にWHOIS(フーイズ)でドメイン登録者情報を確認しておくとスムーズです。JPRS WHOIS /JPRS検索ワードにドメイン入力して検索すると登… StartComに登録ヘッダーメニューの Sign-up クリックStartSSL 必要事項登録し「send verification code」ボタンクリック登録E-mail宛に verification codeが送付されます。 こんな感じのメールが来ます。 verfication codeを入力し登録完了 SSL発行手続き無料版選択 SSLのWeb Server 用を選択 Domain Validation SSLインストール対象サーバのドメイン入力 メール送信による認証 startSSL側で指定するメールアドレスで受信可能な状態にする必要があります。 ec2インスタンスのメール受信設定は以下記事が参考になりました。ありがとうございます。 AWS上のpostfixでメールを受信してみる 上記記事中の注意点としてインスタンス作成直後に yum update 分にはいいですが時にAWSで yum update をするとkernel panicを起こすことがあります。pythonをyumでなくソースから独自に入れ直したり、色々してたせいか… 原因は追い切れていませんが、経年運用したインスタンスについては yum update は控えたいと思います。 うまく受信設定できない場合 /var/log/maillog を 常に tailしてログ確認。 受信可能なメールアドレスが既にあれば aliases で向け先変える。postmaster@(domain) → root@(domain) メールボックスで Permission denied で受信メールを保存できない場合は以下強引にメールディレクトリを変更する。 1234/etc/postfix/main.cf- home_mailbox = Maildir/+ home_mailbox = ../home/ec2-user/Maildir/ SSL証明書 注文へ進むメール受信による認証がクリア後、SSL証明書注文へ進みます。 認証情報作成 情報入力後、 (domain).zip がダウンロードされる。※ 今回は WebServer が Apache なので ApacheServer を参照します。 解凍した zipファイルの ApacheServer 内の以下2ファイルを任意のディレクトリにアップしましょう。 1_root_bundle.crt 2_(domain).crt 今回アップ先ディレクトリは server.key 等のあるパス (/etc/httpd/conf/ssl.csr/) にします。 ssl.conf設定SSL証明書のインストールとはApacheの設定ファイルとして指定ディレクティブで読み込ませることです。 主な設定は以下です。 | Item | Value | Explain || ———————– | —————————————– || SSLCertificateChainFile | /etc/httpd/conf/ssl.csr/1_root_bundle.crt | 中間証明書 || SSLCertificateFile | /etc/httpd/conf/ssl.csr/2_(domain).crt | SSLサーバ証明書 || SSLCertificateKeyFile | /etc/httpd/conf/ssl.csr/server.key | SSLサーバ証明書とペアになる秘密鍵 | 以下URLで各WebServerとopensslバージョンにより最適な設定方法を示唆いただけます。 https://ssl-config.mozilla.org/ /etc/httpd/conf.d/ssl.conf 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172LoadModule ssl_module modules/mod_ssl.soListen 443AddType application/x-x509-ca-cert .crtAddType application/x-pkcs7-crl .crlSSLPassPhraseDialog builtinSSLSessionCache shmcb:/var/cache/mod_ssl/scache(512000)SSLSessionCacheTimeout 300#SSLMutex defaultMutex default ssl-cacheSSLRandomSeed startup file:/dev/urandom 256SSLRandomSeed connect builtin#SSLRandomSeed startup file:/dev/random 512#SSLRandomSeed connect file:/dev/random 512#SSLRandomSeed connect file:/dev/urandom 512SSLCryptoDevice builtin&lt;VirtualHost _default_:443&gt; DocumentRoot &quot;/var/www/html&quot; ServerName (domain):443 ErrorLog /var/log/ssl_error_log TransferLog /var/log/ssl_access_log LogLevel warn SSLEngine on SSLCertificateFile /etc/httpd/conf/ssl.csr/2_(domain).crt SSLCertificateKeyFile /etc/httpd/conf/ssl.csr/server.key SSLCertificateChainFile /etc/httpd/conf/ssl.csr/1_root_bundle.crt &lt;Files ~ &quot;\\.(cgi|shtml|phtml|php3?)$&quot;&gt; SSLOptions +StdEnvVars &lt;/Files&gt; &lt;Directory &quot;/var/www/cgi-bin&quot;&gt; SSLOptions +StdEnvVars &lt;/Directory&gt; SetEnvIf User-Agent &quot;.*MSIE.*&quot; \\ nokeepalive ssl-unclean-shutdown \\ downgrade-1.0 force-response-1.0 CustomLog logs/ssl_request_log \\ &quot;%t %h %{SSL_PROTOCOL}x %{SSL_CIPHER}x \\&quot;%r\\&quot; %b&quot; &lt;Directory &quot;/var/www/html&quot;&gt; AllowOverride All Options -Indexes +FollowSymLinks +Includes +ExecCGI Order allow,deny Allow from all &lt;/Directory&gt;&lt;/VirtualHost&gt;SSLProtocol all -SSLv2 -SSLv3SSLCipherSuite ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA:ECDHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES256-SHA:ECDHE-ECDSA-DES-CBC3-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:DES-CBC3-SHA:!DSSSSLHonorCipherOrder onSSLCompression offSSLSessionTickets off パフォーマンスチューニングを一切していません。あくまで SSL証明書インストールを対象としていますのでご注意ください。 設定ファイルの構文確認12345// 構文チェック# httpd -t// 構文エラーがない場合は以下のように表示されます。Syntax OK 構文エラーが発生している場合は対象箇所が表示されますのでチェックし直してください。但し、構文エラーがないからと言ってApache再読み込み時にエラーが発生しないとは限らないので万が一の為、即元に戻せるようなコマンドを作っておくと良いでしょう。 例えばssl.conf を ssl.conf.bk にリネームして Apacheの設定ファイルとして見ないようにさせるなど。 Apache 設定ファイル再読み込み12# service httpd reloadReloading httpd: [ OK ] ブラウザからアクセスChromeでアクセスしました。 認証の詳細な情報 セキュリティチェック以下サイトで診断できます。QUALYS SSL LABS 「A」が取れました！ ちなみに常に https通信で問題ないサイトであれば以下のように設定すると 12345&lt;VirtualHost *:443&gt; ... Header always set Strict-Transport-Security &quot;max-age=15768000&quot; ...&lt;/VirtualHost&gt; 「A+」取得できました！ 常時httpsはセキュアではありますがサイトの仕様次第なところもあるので状況によりけりです。 以上","link":"/2016/02/24/2016-02-25-get-a-plus-ssl-quality/"},{"title":"Android Studio AVD (エミュレータ) からデスクトップのローカルホストに接続させる設定","text":"概要MacOSX 上で Android Studio で Androidアプリ開発中です。 AVD(Android Virtual Device) から VolleyでAPIの繋ぎこみ先をMAMPやVagrant、Docker等ローカル環境で構築したサーバに接続するようにしています。 単純に emulator上で 地球儀マークのWebブラウザ開いてローカルホスト を指定しても接続できません。 その際に一手間必要でしたので以下手順です。 設定 AndroidStudioでAVD起動中に以下Shellを実行してください。 以上です。","link":"/2016/03/08/2016-03-09-android-studio-avd-from-desktop-to-localhost/"},{"title":"inputボックスにURL打ち込むと自動URL短縮化されるjs","text":"[f:id:kenzo0107:20160310115924p:plain] 概要最近ではURLに日本語を利用するケースが増えてきました。 SEOとして価値がある作業かどうかは眉唾ではありますがネット利用率の低い方にとってはUIとしては分かりやすいのかもしれません。 その点を論じているブログがありました。 URLを日本語にすべきか？ =&gt; UXの観点から日本語のほうがいいかもしれないがSEOとは関係ない | 海外SEO情報ブログ“コンテンツの言語と同じ言語をURLに含めるべきかどうか？” ヒンディー語版のウェブマスター向けオフィスアワーで、GoogleのSyed Malik Mairaj氏は、その国の言語でURLを付けることを推奨した。ただしそれはユーザー体験の観点から。Googleの評価が高まりランキ… 日本語を含むURLをTwitterへ投稿するようなソーシャル連携ツール等の開発の際、文字数に厳密に注意する必要があり、短縮の自動化をbitly APIで行いましたのでそのまとめです。 手順 bitly.com に登録する [https://bitly.com/a/oauth_apps]にて Access Token を取得 以下jsをhtml等でロードしてください。 以上","link":"/2016/03/09/2016-03-10-auto-shorten-url-js/"},{"title":"StatsBotのSlack通知 通知方法と通知時の見え方 一覧","text":"概要Statsbot は、GoogleAnalytics、NewRelic、Mixpanelと連携し各データをSlackへ通知できるサービスです。 連携自体は画面中央の Add to Slack ボタンを押下し各種手続きを手順通り行えばすぐに連携できます。 ※導入まで10秒ほどでした。 データホテルさんが導入法まとめていたので参考までに。 Slack、Brobotなどで快適GAレポーティング！ | NHN テコラス Tech Blog | AWS、機械学習、IoTなどの技術ブログこんにちは、近頃MHXにハマってるUIT室 フロントエンドエンジニアのtrkwです。 NHNテコラスでの主な業務内容はTEMPOSTARやコレカゴplusなどEC事業のフロントエンド開発全般を行っています。 今回はGoogleアナリティクス(以下GA)やGoogleタグマネージャ… 各種設定方法がありますが、どんな見え方になるかは試してみないとわからなかったので、まとめました。 設定は大きく3つあります。 Item Detail Metrics セッション数,ユーザ数,コンバージョン,直帰率,新規ユーザ, Alert ユーザ,コンバージョン,イベントが閾値を超えるor下がると通知 Reports まとめ(ユーザ数, 新規ユーザ数, PV, コンバージョン数, コンバージョン率, イベント数, 平均セッション時間(秒))を通知 以下設定を通知の見え方です。 ※データは知り合いのECサイトです。恥ずかしいデータもありますが、StatsBot の導入を条件に快く許諾いただきました(^-^) MetricsSessions セッション数 1@Statsbot sessions [today, yesterday, this week, last week, this month, last month, this year] Users ユーザ数 1@Statsbot users [today, yesterday, this week, last week, this month, last month, this year] Conversions コンバージョン 1@Statsbot conversions [today, yesterday, this week, last week, this month, last month, this year] Conversion Rate コンバージョン率 1@Statsbot conversion rate [today, yesterday, this week, last week, this month, last month, this year] Bounce Rate 直帰率 1@Statsbot bounce rate [today, yesterday, this week, last week, this month, last month, this year] New Users 新規ユーザ 1@Statsbot new users [today, yesterday, this week, last week, this month, last month, this year] 例) 今週の新規ユーザ (先週との比較) 1@Statsbot new users this week 例) 今日の新規ユーザ (昨日との比較) 1@Statsbot new users today Alert– Stay on top of your website traffic – Set Alertアラート設定 1@Statsbot alert [users, conversions, events] [above, below, &gt;, &lt;] NUM 例) ユーザ数が1000人を超えたらアラート設定 1@Statsbot alert users &gt; 1000 アラート一覧 表示 1@Statsbot list alerts アラート一覧 削除 1@Statsbot alert remove ID ReportsSummary まとめ (ユーザ数, 新規ユーザ数, PV, コンバージョン数, コンバージョン率, イベント数, 平均セッション時間(秒) ) 1@Statsbot summary [today, yesterday, this week, last week, this month, last month, this year] Source 流入 1@Statsbot sources [today, yesterday, this week, last week, this month, last month, this year] Schedule スケジュール 日時週・頻度設定 1@Statsbot schedule [summary, sources, status] スケジュール一覧表示 1@Statsbot list schedule スケジュール設定解除 1@Statsbot unschedule ID","link":"/2016/03/10/2016-03-11-statsbot-slack/"},{"title":"MySQL SQL結果をINTO OUTFILEを使用せずCSV取得","text":"概要MySQL SELECT文の結果をcsvで取得する際以下のようにCLI上で実行することで取得できます。 12$ mysql -u &lt;user&gt; -p&lt;password&gt; &lt;db_name&gt;mysql&gt; SELECT * FROM [table] WHERE hoge=hoge INTO OUTFILE &quot;/tmp/output.csv&quot; FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '&quot;'; ですが、mysql ログインユーザの権限にFILEがない場合以下のようなエラーで出力できません。 1RROR 1 (HY000): Can't create/write to file '/tmp/output.csv' (Errcode: 13) FILE のREAD/WRITE権限を付与すれば問題ないですが権限周りをいじりたくないときなどあるかと思います。 本番環境のDBで権限周りがブラックボックス化していてFLUSH PRIVILEGES すると何か障害が出るんじゃないかとか汗 その際に実施したことを以下まとめました。 考え方TXT として取得して sed で csv ファイルに加工する、という方法で実行しています。 例) 以下のようなSQL実行結果を取得したとすると 1234$ less output.txt商品1 2 1,000 2,000商品2 3 1,500 3,000 ↓ Excel で表示されるように加工すると 1234$ less output.txt&quot;商品1&quot;, &quot;2&quot;,&quot;1,000&quot;,&quot;2,000&quot;&quot;商品1&quot;, &quot;3&quot;,&quot;1,500&quot;,&quot;3,000&quot; 手順query 結果を ouput.txt に出力1$ mysql -u &lt;user&gt; -p&lt;password&gt; &lt;db_name&gt; -e&quot;&lt;query&gt;&quot; &gt; output.txt Excel用に加工 力技 123456789101112131415161718// 各行の一番前(「^」)に「&quot;」を付加$ cat output.txt | sed -e 's/^/&quot;/g' &gt; output2.txt&quot;商品1 2 1,000 2,000&quot;商品2 3 1,500 3,000// 各行の一番後(「$」)に「&quot;」を付加$ cat output2.txt | sed -e 's/$/&quot;/g' &gt; output3.txt&quot;商品1 2 1,000 2,000&quot;&quot;商品2 3 1,500 3,000&quot;// タブ「/t」を「&quot;,&quot;」に変更$ cat output3.txt | sed -e 's/\\t/&quot;,&quot;/g' &gt; ouptut4.txt&quot;商品1&quot;,&quot;2&quot;,&quot;1,000&quot;,&quot;2,000&quot;&quot;商品2&quot;,&quot;3&quot;,&quot;1,500&quot;,&quot;3,000&quot; 文字コード変更このCSVを利用する方(提出先)の方の文字コードに合わせます。提出先では Shift JIS がデフォルトとのことで そこに変更します。 12$ nkf -g output4.txtUTF-8 1234$ nkf -sLw output4.txt &gt; output.csv$ nkf -g output.csvShift_JIS これで INTO OUTFILE を利用せず CSVファイルを取得できました。ouput.csv おまけファイル名に日付をつける。 1$ mv output.csv output_`date '+%Y%m%d'`.csv 1234$ lsoutput.csvoutput_20160314.csv","link":"/2016/03/13/2016-03-14-mysql-output-csv-without-outfile/"},{"title":"ファイルの中身でなくファイル名の文字コードを変更する","text":"概要ECサイトの売上のレポートなどを送信したい、というときファイル名に日本語を指定せざるを得ないとき(クライアントさんが「絶対に日本語！」)がときたまあります。 クライアント様の環境はWindows。 Windows デスクトップのデフォルト文字コードは Shift JIS。 Linux で作成したファイルのデフォルト文字コードがUTF-8だった場合メールに添付し送信し、受信したWindows PCでダウンロードするとファイル名が文字化けしてる、なんてことがあります。 そんなとき、ファイルの中身でなくファイル名自体の文字コードを変更したのでその対応まとめです。 環境 CentOS release 5.11 (Final) convmv 1.10 convmv を利用します。 CentOSならyumでインストール 1# yum install -y convmv 手順 デフォルト文字コードUTF-8とします。 12345678910// UTF-8 でファイル名「ほげほげ」作成$ touch ほげほげ// ファイル名の文字コードを UTF-8 から ShiftJIS に変換$ convmv -r -f utf8 -t sjis ほげほげ --notestmv &quot;./ほげほげ&quot; &quot;./ق°ق°&quot;Ready!$ ls -al-rw-rw-r-- 1 user group 0 3月 16 16:45 ?ق??ق? 全く読み取れないような文字になります。 ごく稀に「絶対日本語で！」という方の一助になれば何よりです。","link":"/2016/03/15/2016-03-16-encode-filename/"},{"title":"IPアドレスからホスト情報取得","text":"概要ipinfo.io の API によりホスト情報を取得するシェルを書きました。 不審なアクセスが増えてきた、というときに接続元ホストを探る際に利用しています。","link":"/2016/03/22/2016-03-23-get-host-data-from-ip/"},{"title":"awk で CSV の特定列を整形 〜数値を文字列扱いする〜","text":"概要ある顧客データを抽出してレポートしたい、というときにCSVファイルで “090” などと携帯電話があると ExcelでCSVファイルを開いたときに“90” になってしまうということがあるかと思います。 そんなときの為に特定列のみ数値を文字列扱いにしたときの内容まとめです。 Excelで数値を文字列扱いするには1&quot;090&quot; とあったとすると 1=&quot;090&quot; のように整形することで文字列扱いになります。 例) 1&quot;デミスハサビス&quot;,=&quot;09099999999&quot;,&quot;DeepMind&quot; 整形してみる 例）以下のような tmp.csv があるとします。 12&quot;デミスハサビス&quot;,&quot;09099999999&quot;,&quot;DeepMind&quot;&quot;いとうせいこう&quot;,&quot;08088888888&quot;,&quot;エムパイヤ・スネーク・ビルディング&quot; 2つ目のカラムのみ =を左端に追加します※”(ダブルクォテーション)で囲まれた値の左側に「=」を追加します。※あえて分かりやすく $1, $2, $3 の項目を明示的に指定してみました。 123$ awk -F ',' '{print $1 &quot;,=&quot;$2&quot;,&quot;$3}' tmp.csv&quot;デミスハサビス&quot;,=&quot;09099999999&quot;,&quot;DeepMind&quot;&quot;いとうせいこう&quot;,=&quot;08088888888&quot;,&quot;エムパイヤ・スネーク・ビルディング&quot; 上記のように 「=」 が追加されました。 出力先を指定 1234$ awk -F ',' '{print $1 &quot;,=&quot;$2&quot;,&quot;$3}' tmp.csv &gt; output.csv$ cat output.csv&quot;デミスハサビス&quot;,=&quot;09099999999&quot;,&quot;DeepMind&quot;&quot;いとうせいこう&quot;,=&quot;08088888888&quot;,&quot;エムパイヤ・スネーク・ビルディング&quot; output.csv を Excelで開いてみます。 できました♪ おまけExcelを開く際、Windowsではデフォルト SJISなので文字コードがUTF8の場合、文字化けします。以下CSVファイルをSJISに文字コード変換します。 1$ nkf -sLw output.csv &gt; output_sjis.csv 以上です。","link":"/2016/03/27/2016-03-28-awk-shapes-csv/"},{"title":".htaccessが効かない、動かないときの対処","text":"概要.htaccess に設定した通りに動作しない、そもそも読み込んでいないように見える場合の対策です。 httpだと mod_rewriteでリダイレクトするけど、 httpsだとしない、とかありがちな設定ミスパターンは以下基本的なことを確認して解決できます。 検証環境 CentOS 6.6 (Final) Apache/2.2.15(Unix) そもそも .htaccess を利用するには/etc/httpd/conf/httpd.conf 等設定ファイル内で以下の記述が必要です。 1AllowOverride All mod_rewrite を利用するには利用頻度の多い mod_rewrite を利用するには以下が必要です。 1. mod_rewrite.so インストール2. mod_rewrite.so をApache設定ファイルからロードまずは上記の確認です。 1. mod_rewrite.so インストール確認Apacheの場合、大抵modlesディレクトリ以下に格納されてます。 12$ ls -al /etc/httpd/modules/mod_rewrite.so-rwxr-xr-x 1 root root 60464 10月 16 23:49 2014 /etc/httpd/modules/mod_rewrite.so 2. mod_rewrite.so をApache設定ファイルからロードされているか確認Apache設定ファイル/etc/httpd/conf/httpd.conf や /etc/httpd/conf.d/*.conf で以下を設定している。 ※環境によっては /etc/httpd/conf/httpd.conf に設定ファイルを置いてない場合もあるので あくまで一般的な例とします。 1LoadModule rewrite_module modules/mod_rewrite.so 補足モジュール読み込み設定は大抵、以下のように ディレクティブ で AllowOverride All が設定されているかと思います。 以下例では「/var/www/html」ディレクトリ以下では 配置した .htaccess を優先して設定されることになります。 123456&lt;Directory \"/var/www/html\"&gt; AllowOverride All Options -Indexes FollowSymLinks Includes ExecCGI Order allow,deny Allow from all&lt;/Directory&gt; 万が一 mod_rewrite.soモジュールが存在しない場合は Apacheの再コンパイルが必要になります。 Apacheに mod_rewriteをインストールしリコンパイル リコンパイルしてApache再起動します。 12345$ sudo su -# cd &lt;path to httpd source&gt;# ./configure –enable-ssl=shared –enable-rewrite –enable-deflate –enable-headers –enable-so# make &amp; make install# service httpd restart 以上です。","link":"/2016/03/27/2016-03-28-htaccess_not_move/"},{"title":"パスワードなし！公開鍵認証でSSHログイン","text":"概要rsyncをJenkinsやcrontabで利用する場合SSH経由で実行する際、パスワードを求められ処理が中断してしまうということがあるかと思います。 その為、SSHで公開鍵認証という方法でパスワードを求めることなくSSHアクセスできる様な設定を明示的にしました。 以下まとめます。 環境 接続元サーバ A の global IP を 192.168.11.200 とします。 接続先サーバ B の global IP を 192.168.11.201 とします。 サーバA → サーバB へ 公開鍵認証でSSHログインを目的とします。 以下手順です。 接続元サーバ A公開鍵作成12345678910111213141516171819[host A]# mkdir ~/.ssh[host A]# ssh-keygen -t rsaGenerating public/private rsa key pair.Enter file in which to save the key (/var/lib/pgsql/.ssh/id_rsa): ←何も入力せず[Enter]を押すEnter passphrase (empty for no passphrase): ←何も入力せず[Enter]キーを押すEnter same passphrase again: ←何も入力せず[Enter]キーを押すYour identification has been saved in &lt;home&gt;/.ssh/id_dsa.Your public key has been saved in &lt;home&gt;/.ssh/id_rsa.pub.The key fingerprint is:7e:38:5c:9f:f3:e2:67:eb:ce:c6:07:83:48:c8:85:ec[host A]# ls -l合計 12-rw------- 1 hogehoge hogehoge 668 5月 25 15:11 id_rsa ←作成された秘密鍵-rw-r--r-- 1 hogehoge hogehoge 610 5月 25 15:11 id_rsa.pub ←作成された公開鍵[host A]# cat id_rsa.pub&lt;中身をコピーする&gt; 接続先サーバ B接続元サーバの公開鍵を authorized_keys に保存1234567891011121314[host B]# cd ~[host B]# chmod 755 .[host B]# mkdir .ssh[host B]# chmod 700 .ssh[host B]# cd .ssh[host B]# vi id_rsa.pub&lt;接続元サーバでコピーした公開鍵の内容をペースト&gt;[host B]# ls id_rsa.pubid_rsa.pub[host B]# cat id_rsa.pub &gt;&gt; authorized_keys[host B]# chmod 600 authorized_keys│-rw------- 1 hogehoge hogehoge 796 4月 5 15:50 authorized_keys 接続元サーバのアクセス許可設定 /etc/hosts.allow を編集し 接続元IP許可設定する。 1[host B]# vi /etc/hosts.allow 12345678## hosts.allow This file describes the names of the hosts which are# allowed to use the local INET services, as decided# by the '/usr/sbin/tcpd' server.#sshd: xxx.x.xx.xx xxx.x.xxx. xx.xx.x. xx.xx.xxx.xxsshd: xxx.xx.xxx.xxsshd: 192.168.11.200 ← 追加 公開鍵認証許可設定 バックアップ保存 1[host B]# cp /etc/ssh/sshd_config /etc/ssh/sshd_config.bk 修正 1[host B]# vi /etc/ssh/sshd_config 12345#PubkeyAuthentication yes ← コメントアウトを外す#AuthorizedKeysFile .ssh/authorized_keys ← コメントアウトを外す↓PubkeyAuthentication yesAuthorizedKeysFile .ssh/authorized_keys 差分確認 1234567[host B]# diff /etc/ssh/sshd_config.bk /etc/ssh/sshd_config&lt; #PubkeyAuthentication yes&lt; #AuthorizedKeysFile .ssh/authorized_keys---&gt; PubkeyAuthentication yes&gt; AuthorizedKeysFile .ssh/authorized_keys sshd configure チェック 123[host B]# sshd -t// 何も出力されなければ構文上問題なし。// 但し存在しないパスを指定するなどまではチェックしないので注意。 sshd リスタート 12[host B (CentOS7)]# systemctl restart sshd[host B (CentOS6)]# service sshd restart 以上で接続先サーバでの準備完了しました。 接続元サーバ A から パスワードなしでSSH接続する12[host A]# ssh 192.168.11.201Last login: Tue Apr 5 16:02:08 2016 from xxx.xx.xxx.xxx ログイン成功！ ログイン失敗する場合 ログを調査しましょう。 権限や所有権がよろしくない、ということで認証失敗理由がわかります。 123# tail -f /var/log/secureAuthentication refused: bad ownership or modes for directory &lt;homeディレクトリ&gt; あとがき以下デフォルトの sshd_config の設定の場合パスワード認証と鍵認証、どちらも認証パス可能です。 123#PubkeyAuthentication yes#AuthorizedKeysFile .ssh/authorized_keysPasswordAuthentication yes インフラ専門の会社さんや街の噂ではデフォルトでどちらも認証OKにしている企業さん多いという話でした。 以上です。","link":"/2016/04/04/2016-04-05-ssh-login-without-password/"},{"title":"とにかくシンプルにPHPでLineBotApiを書きました","text":"概要話題のLine bot Api用スクリプトをPHPで書きました。 とにかくシンプルに = カスタマイズしやすさ という所で修正する場所を限って利用できるようにしたつもりです。 作ったもの適当に文字を打つと →「hello」perfect と打つと → 「human」 と返すようにした本当にシンプルなものです。 そのロジック部分をカスタマイズすればマイbotができますね。 環境 さくらレンタルサーバVPS CentOS release 6.7 (Final) PHP 5.6.16 SSLは無料のStartSSLを利用しました。取得・設定は以下参照してください。 無料SSL証明書発行しセキュリティでA+を取る！ 〜Apache編〜 - 長生村本郷Engineers'Blog概要 ベリトランスモジュールバージョンアップに際して SHA256で発行したSSL証明書が必須となりました。 テスト環境でベリトランスモジュールバージョンアップ試験を行う際に SSL証明書導入する必要が生じました。 本番環境と同じ有料SSLを導入するまでとはいかずとも 近しい状況… スクリプト ### function getMessage でbotに返信させたいメッセージを決定しています。 そこで他APIを呼んだり、サイトからスクレイプしたり情報をとってきて返してあげると簡易的なメッセージ返信Line botできあがりです。 こんな感じです！","link":"/2016/04/27/2016-04-28-simple-php-linebotapi/"},{"title":"Zabbix3.x系からSlack通知","text":"概要Zabbix3.x系でのSlackへの障害内容通知方法をまとめました。 簡単なメッセージの通知でなくある程度見やすく、状況判断しやすい様にしました。 ↓まずざっくりとこんな感じです。 手順Gitに上げてます。ご参照ください。 GitHub - kenzo0107/Zabbix3-Slack: slack notification from zabbix3slack notification from zabbix3. Contribute to kenzo0107/Zabbix3-Slack development by creating an account on GitHub.","link":"/2016/05/06/2016-05-07-zabbix2slack/"},{"title":"sftpをシェル化してファイルアップロード","text":"概要データ解析サービスを提供するサードパーティで指定の Server, Path に指定形式で格納するように、と指示がありました。 また、「Protocol は sftp のみ許可でお願いします」とも。 定期実行する必要もあったためシェルで書けないかということで、以下まとめました。 shell Point expect で通常、対話形式となるsftpコマンドの先読みでパスワードを通過 -oオプションで bachmode no を指定し以下のエラーを回避 12Permission denied (publickey,password).Connection closed -bオプション バッチファイルにsftpログイン後の実行処理を記述 当方では、Jenkins や cron でシェルを定期実行する必要があった為明示的にバッチファイルの内容を指定できるようにしています。 以上です。","link":"/2016/05/09/2016-05-10-sftp-via-shell/"},{"title":"CentOS7にSonarQubeをインストールしアクセス確認まで","text":"概要sonarqube はJava, Python, Ruby, PHP等、複数言語のコードメトリクス集計ができる優れものです。 以前 MacOSXローカル環境でUnityプロジェクト C#コードのコードメトリクス抽出方法をまとめました。 Mac OS X に SonarQube 導入 - UnityのC#コーディング規約チェック[f:id:kenzo0107:20140620233131p:plain] 環境 Mac OS X 10.9.3 (Marvericks) SonarQube 4.3.1 SonarRunner 2.4 概要 C#のコーディング規約チェック管理ツールを導入します。 導入手順… 今回はCentOS7上に構築する方法を以下にまとめました。 環境 CentOS7 64bit Java 1.8 ec2 t.micro JDKインストール12# cd /usr/local# wget --no-check-certificate --no-cookies --header \"Cookie: oraclelicense=accept-securebackup-cookie\" \"http://download.oracle.com/otn-pub/java/jdk/8u45-b14/jdk-8u45-linux-x64.tar.gz\" 1# tar xvf jdk-8u45-linux-x64.tar.gz 1# ln -s jdk1.8.0_45 latest 環境変数 JAVA_HOME 設定.bash_profileでの設定ではユーザにより環境変数が異なるので全ユーザ共通で設定する場合は /etc/profile.d/ 以下にshellを用意する。 12345# echo \"export JAVA_HOME=/usr/local/java/latestexport PATH=$PATH:$JAVA_HOME/bin\" &gt; /etc/profile.d/javaenv.sh# echo \"export JAVA_HOME=/usr/local/java/latestexport PATH=$PATH:$JAVA_HOME/bin\" &gt; /etc/profile.d/javaenv.csh MySQLインストール今回は同一サーバにMySQLインストールしています。別途MySQLサーバを立てる場合は不要です。 12345678910111213# yum -y install http://dev.mysql.com/get/mysql-community-release-el6-5.noarch.rpm# yum -y install mysql-community-server# chown -R mysql:mysql /var/lib/mysql/*# systemctl start mysqld# chkconfig mysqld on// 以下 DB名: sonar, DB User: sonar, DB Pass: sonar で設定# mysql -u rootmysql&gt; CREATE DATABASE sonar;mysql&gt; CREATE USER 'sonar'@'localhost' IDENTIFIED by 'sonar';mysql&gt; GRANT ALL PRIVILEGES ON sonar.* TO 'sonar'@'localhost';mysql&gt; FLUSH PRIVILEGES; SonarQube インストール以下サイトにてダウンロードzipを取得します。SonarQube Donwloads ※ 2016-05-19 時点 最新は ver 5.5 12345# mkdir /usr/local/sonarqube# cd /usr/local/sonarqube# wget https://sonarsource.bintray.com/Distribution/sonarqube/sonarqube-5.5.zip# unzip sonarqube-5.5.zip# ln -s sonarqube-5.5 sonar 環境変数 SONAR_HOME 設定12345# echo \"setenv SONAR_HOME=/usr/local/sonarqube/sonarsetenv PATH=$PATH:$SONAR_HOME/bin/linux-x86-64\" &gt; /etc/profile.d/sonarenv.sh# echo \"setenv SONAR_HOME=/usr/local/sonarqube/sonarsetenv PATH=$PATH:$SONAR_HOME/bin/linux-x86-64\" &gt; /etc/profile.d/sonarenv.csh SonarQubeからMySQLを利用できる様に設定以下ファイルを編集し作成した DBにアクセスできるように設定します。 /usr/local/sonarqube/sonar/conf/sonar.properties 123sonar.jdbc.username=sonar # DB Usersonar.jdbc.password=sonar # DB Passwordsonar.jdbc.url=jdbc:h2:tcp://localhost:9092/sonar # DB url SonarQubeが利用するjavaコマンド設定以下ファイルを編集しSonarQubeが利用するjavaコマンドをインストールしたJDK内のjavaを指定するように変更 /usr/local/sonarqube/sonar/conf/wrapper.conf 12#wrapper.java.command=javawrapper.java.command=/usr/local/java/latest/bin/java SonarQube起動スクリプト設定123# ln -s /usr/local/sonarqube/sonar/bin/linux-x86-64/sonar.sh /etc/init.d/sonar# chkconfig --add sonar# chkconfig sonar on サーバ再起動/etc/profile.d に設定した 環境変数を反映させるべくサーバ再起動します。 1# reboot アクセスして確認http://&lt;IPアドレス&gt;:9000 SonarQube管理ページが表示されればOKです！ 再起動後アクセスできない場合SonarQubeの設定を見直すか、もしくは、 今回利用している ec2 t2.micro のような小メモリの場合メモリ不足でMySQLが落ちる可能性があります。 以下参照して対応してください。 MySQが落ちる トラブルシューティング Cannot allocate memory for the buffer pool概要AWS E2インスタンス上に MySQL, SonarQube インストールし起動するもののMySQLが落ちるという事象が発生。 ログを見ると以下のエラーが。。 /var/log/mysqld.log 123456InnoDB: mmap(137363456 bytes)… あとがきどんなプロジェクトでもここ修正したい！と思うことは多々あるかと思います。 その際、なんとなくここ使いづらいから直そう！という曖昧な判断ではなく 先立ってまず全体としてどういう状態にあるか、を数値でみて判断する、 というプロセスが踏めるようになることを目的に導入しました。 次回はJenkins からの実行方法をまとめます。","link":"/2016/05/18/2016-05-19-install-sonarqube-on-centos7/"},{"title":"MySQが落ちる トラブルシューティング Cannot allocate memory for the buffer pool","text":"概要AWS E2インスタンス上に MySQL, SonarQube インストールし起動するもののMySQLが落ちるという事象が発生。 ログを見ると以下のエラーが。。 /var/log/mysqld.log 123456InnoDB: mmap(137363456 bytes) failed; errno 12[ERROR] InnoDB: Cannot allocate memory for the buffer pool[ERROR] Plugin 'InnoDB' init function returned error.[ERROR] Plugin 'InnoDB' registration as a STORAGE ENGINE[ERROR] Unknown/unsupported storage engine: InnoDB[ERROR] Aborting 123Cannot allocate memory for the buffer poolバッファープールへのメモリ割当ができない 割りあてるメモリがないという話。なので、メモリを作ります。 対策1. swap領域を作成するswap領域作成12345678// 空ファイル作成# dd if=/dev/zero of=/swapfile bs=1M count=1024// 作成した空ファイルをswap領域に設定# mkswap /swapfile// スワップ領域を有効化# swapon /swapfile swap領域確認12345# free total used free shared buff/cache availableMem: 1015472 833592 66456 1456 115424 54708Swap: 1048572 491136 557436 ← Swapの設定が追加されていることを確認 mysql 再起動1# systemctl restart mysqld 対策2. innodb_buffer_pool_size の割当を増やす現在設定されている innodb_buffer_pool_size 確認1234567$ mysql -u &lt;user&gt; -p&lt;pass&gt; &lt;db&gt; -e \"SHOW VARIABLES LIKE 'innodb_buffer_pool_size'\"+-------------------------+-----------+| Variable_name | Value |+-------------------------+-----------+| innodb_buffer_pool_size | 118835956 |+-------------------------+-----------+ エラーログにあった 137363456 を下回ってるのがわかります。この割当メモリを増やします。 my.cnfの場所探し左から順に検索し該当するファイルがあればそのmy.cnfを参照します。/etc/my.cnf → /etc/mysql/my.cnf → /usr/etc/my.cnf → ~/.my.cnf 123# mysql --help | grep my.cnf order of preference, my.cnf, $MYSQL_TCP_PORT,/etc/my.cnf /etc/mysql/my.cnf /usr/etc/my.cnf ~/.my.cnf メモリ追加自分の方ではmy.cnf上に innodb_buffer_pool_size の設定項目がなかったので追加しました。 my.cnf 12345678910[mysqld]......# 以下追加innodb_buffer_pool_size = 256M[mysqld_safe]...... mysql 再起動1# systemctl restart mysqld 増えているか確認1234567$ mysql -u &lt;user&gt; -p&lt;pass&gt; &lt;db&gt; -e \"SHOW VARIABLES LIKE 'innodb_buffer_pool_size'\"+-------------------------+-----------+| Variable_name | Value |+-------------------------+-----------+| innodb_buffer_pool_size | 268435456 |+-------------------------+-----------+ 今回発生していたエラーログが消えました。","link":"/2016/05/19/2016-05-20-mysql-cannot-allocate-memory-for-the-buffer-pool/"},{"title":"Jenkins + SonarQube で PHPコードメトリクス計測！","text":"前回Jenkinsとは別のサーバ上にSonarQube をインストールしアクセスできるまでをまとめました。 CentOS7にSonarQubeをインストールしアクセス確認まで - 長生村本郷Engineers'Blog概要 SonarQube™は Java, Python, Ruby, PHP等、複数言語のコードメトリクス集計ができる 優れものです。 以前 MacOSXローカル環境で Unityプロジェクト C#コードのコードメトリクス抽出方法をまとめました。 kenzo0107.hatena… 今回は Jenkins からソースを解析しSonarQubeでのメトリクス情報を表示までの実行方法をまとめます。 言語は どれでも良いですが、 今回は PHP とします。 Overview以下概要になります。 SonarQube側事前準備プロジェクトを作成しプロジェクトキーを発行します。 1. ログインページへアクセスhttp://:9000/sessions/new デフォルトでは以下 admin:admin アカウントでログイン Item Value ID admin PW admin 2. プロジェクト作成 ヘッダーメニュー Administration クリックし Administration ページへ遷移 Projects &gt; Management クリック Create ボタンクリック Name, Key 入力し プロジェクトが追加されたことがわかります。 3. PHP Plugin インストール Administration ページ System &gt; Update Center クリック Available 選択 → 検索窓で「PHP」と入力 → 表示された PHP Plugin でInstallクリック Restart でSonarQubeに PHP Plugin インストール Installed タブで PHP Plugin がインストールされていることを確認 4. authentication token 発行 Security &gt; User クリック TOKENS クリックしポップアップ表示 任意の文字列を入力しcreate tokenコピーJenkins側の設定時に利用します。 以上でSonarQube側の事前準備は終了です。 Jenkins 側準備1. SonarQube Plugin インストールJenkinsの管理 &gt; Pluginの管理にて SonarQube Plugin インストール 2. SonarQube Scanner インストール以下オフィシャルダウンロードページからリンク取得 Analyzing+with+SonarQube+Scanner 1234$ cd /var/lib/jenkins$ wget https://sonarsource.bintray.com/Distribution/sonar-scanner-cli/sonar-scanner-2.6.1.zip$ unzip sonar-scanner-2.6.1.zip$ ln -s sonar-scanner-2.6.1 sonar-scanner 3. Jenkinsシステム設定 Jenkinsの管理 &gt; システムの設定 へアクセス JenkinsQube servers に必要項目入力 SonarQube Scanner に先ほどインストールした sonar-scannerパスを設定 上記入力後保存 4. ジョブ新規作成「sonarqubeTest」という名前のジョブを新規作成します。 git リポジトリよりPHPプロジェクト取得設定 SonarScanner 実行設定 以上でJenkins側の設定完了です。 SonarQube 反映確認 ちなみにこちら EC-CUBE 1.1のプロジェクトでした。 EC-CUBEのコード重複率が多く無駄が如何に多いかがわかります。 以上です。","link":"/2016/05/20/2016-05-21-jenkins-sonarqube-php-code-metrics/"},{"title":"日本国内からアクセスされるIP取得スクリプト Ruby 30秒クッキング","text":"まずスクリプト 1234567891011121314$ git clone https://gist.github.com/kenzo0107/714ece62cf6450386ff0fb16fd5b777a$ cd 714ece62cf6450386ff0fb16fd5b777a$ ruby getJapanIP.rb1.0.16.01.0.64.01.1.64.01.5.0.0...中略...223.223.164.0223.223.208.0223.223.224.0 概要作った経緯は日本版・海外版URLがあり、海外版に日本からアクセスした場合は日本版サイトにリダイレクトさせたい、という依頼があった為です。 このスクリプトを利用して海外版サイト .htaccess等でリダイレクト設定をしました。 以上です。","link":"/2016/05/25/2016-05-26-ruby-30-sec-cooking-get-access-ip-from-japan/"},{"title":"Jenkins - Short cycles in the day-of-month field will behave oddly near the end of a month","text":"完全なる備忘録です。 Jenkinsで 毎月第1月曜日のみ設定しようとしたら 1Short cycles in the day-of-month field will behave oddly near the end of a month 次の様に分けるとエラーが消えます。 概要アメリカでは月間の期間を4,5週間の単位で売り上げの結果を考える慣習があります。 その際に月初にレポートが欲しいなんてときに今回このような処理をする必要がありました。 以上です。","link":"/2016/05/31/2016-06-01-jenkins-short-cycles-in-the-day-of-month-field-will-behave-oddly-near-the-end-of-a-month/"},{"title":"Ansible+Vagrant でシンプルなPrometheusモニタリング環境構築","text":"概要Prometheus入門 にあるチュートリアルをAnsibleで簡単に構築できるようにした、というものです。 先日2016年6月14日、LINE株式会社でのPrometheus Casual Talks #1に参加しナレッジのおさらいなどしたく、構築法をまとめました。 Prometheusとは最近話題のPull型のQuery Filtering可能で Grafana等と連携できる モニタリング/Alertツールです。 構成 Prometheus Server × 1 Prometheus Client × 2 環境 CentOS 6.5 Prometheus Server 0.20.0 Supervisor 3.3.0 Go 1.6.2 Ansibl 2.1.0.0 Vagrant 1.8.1 MacOSX 10.11.5 前提条件以下ツールをインストールしておいてください。 VirtualBox Vagrant Ansible 使い方1. git repository を clone[https://github.com/kenzo0107/Vagrant-Prometheus] 1$ git clone https://github.com/kenzo0107/Vagrant-Prometheus 2. Vagrant VM 起動12$ cd Vagrant-Prometheus$ vagrant up 3 node running ! 1 node: Prometheus Server 192.168.11.30 other 2 nodes: Prometheus Client Server 192.168.33.31 192.168.33.32 3. ssh.config 追加1$ vagrant ssh-config &gt; ssh.config 4. ping 疎通試験1234567891011121314$ ansible default -m pingserver | SUCCESS =&gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;}client1 | SUCCESS =&gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;}client2 | SUCCESS =&gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;} ok, success. 5. 2node に PrometheusClient 設定1$ ansible-playbook set_clients_prometheus.yml 6. PrometheusClient の起動確認以下PrometheusClientを起動しているサーバにアクセスし起動されているか確認します。 http://192.168.11.31:8080/metrics http://192.168.11.32:8080/metrics 以下のように表示されれば成功です。 7. PrometheusServer 設定1$ ansible-playbook set_server_prometheus.yml 8. PrometheusServer 確認http://192.168.33.30:9090 にアクセス 以下のように表示されれば成功です。 是非多少なりとも一助となれば何よりです！いじくり倒してみてください！ 以上","link":"/2016/06/20/2016-06-21-ansible-vagrant-prometheus/"},{"title":"MySQL テーブル単位の容量確認","text":"SQL 1234567891011mysql&gt; use zabbixDatabase changedmysql&gt; select -&gt; table_name, engine, table_rows as tbl_rows, avg_row_length as rlen, -&gt; floor((data_length+index_length)/1024/1024) as allMB, #総容量 -&gt; floor((data_length)/1024/1024) as dMB, #データ容量 -&gt; floor((index_length)/1024/1024) as iMB #インデックス容量 -&gt; from information_schema.tables -&gt; where table_schema=database() -&gt; order by (data_length+index_length) desc; 実行結果 1234567891011121314151617181920+----------------------------+--------+----------+-------+-------+------+------+| table_name | engine | tbl_rows | rlen | allMB | dMB | iMB |+----------------------------+--------+----------+-------+-------+------+------+| history | InnoDB | 23815217 | 51 | 1802 | 1179 | 623 || history_uint | InnoDB | 21709131 | 51 | 1646 | 1075 | 571 || trends_uint | InnoDB | 45152 | 650 | 28 | 28 | 0 || trends | InnoDB | 30965 | 914 | 27 | 27 | 0 || history_str | InnoDB | 129224 | 109 | 19 | 13 | 5 || items | InnoDB | 2242 | 708 | 2 | 1 | 0 || images | InnoDB | 144 | 11036 | 1 | 1 | 0 || items_applications | InnoDB | 2468 | 66 | 0 | 0 | 0 || triggers | InnoDB | 805 | 223 | 0 | 0 | 0 || graphs_items | InnoDB | 948 | 103 | 0 | 0 | 0 || functions | InnoDB | 833 | 98 | 0 | 0 | 0 || events | InnoDB | 718 | 114 | 0 | 0 | 0 || alerts | InnoDB | 88 | 930 | 0 | 0 | 0 || graphs | InnoDB | 329 | 248 | 0 | 0 | 0 |...... ちなみに上記は Zabbix DBの結果history (履歴) にデータが肥大化傾向にあるのでパーティション や Zabbix housekeeping での保存期間設定を調整するなど必要なことがわかります。","link":"/2016/06/26/2016-06-27-each-table-mysql/"},{"title":"2016年5月現在、SSL評価Fを取らない為に","text":"それは、ある日 QUALYS SSL LABS でSSLチェックしたとき F になってる… 常に A+ を保っていたのに… どうやら 2016/5/3 時点で新たに脆弱性が発見されたとのこと。 SIOS Tech. LabエンジニアのタメになるOSS・クラウド・認証の技術トピックス 今回指摘されている CVE-2016-2107 : AES-NI CBC MACチェックでMITM攻撃者がパディングオラクル攻撃可能なことがわかりました。この問題は、CVE-2013-0169(Lucky 13パディング)の修正のために導入された箇所の不具合から発生しました。 上記対応策をまとめました。 対応策 OpenSSL version up ※試験OS環境: CentOS7 以下記事では 「OpenSSLのバージョンを1.0.2h/1.0.1tにあげてください。」とありますが、 general-security-20160504 段階的に試した所以下コマンドでopenssl のバージョンアップをした所エラーが消え B となりました。 事前にNginx を 1.11.1 にアップデートした為でしょうか汗 1234# yum upgrade openssl -y# openssl versionOpenSSL 1.0.1e-fips 11 Feb 2013 ssl_ciphers の設定変更 config generator で 生成される ssl_ciphers ディレクティブをそのまま設定すると評価がB止まりでした。 https://ssl-config.mozilla.org/ 諸々含めたくない暗号化法があるようです。 1ssl_ciphers 'ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA:ECDHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES256-SHA:ECDHE-ECDSA-DES-CBC3-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:DES-CBC3-SHA:!DSS'; 以下最終的に設定した ssl_ciphers すると A+ に戻りました。 1ssl_ciphers 'ECDH !aNULL !eNULL !SSLv2 !SSLv3'; OpenSSLの脆弱性はチェックし続けていかねば 補足HTTP Strict Transport Security を 有効化 (常時SSL) にしないとA+ は取れません。 常時SSLはパフォーマンスにも影響するのでせめて A まで取る、という指針を決めてから着手してください。","link":"/2016/06/26/2016-06-27-ssl-qualys-get-a-plus/"},{"title":"Nginx 1.9.6 → 1.11.1 へバージョンアップ 脆弱性対応","text":"脆弱性CVE-2016-4450 に対応した Nginx 1.11.1 が 2016-05-31 リリースされたということで早速バージョンアップを試みました。 SIOS Tech. Lab - エンジニアのためになる技術トピックス ダウンタイムゼロで実行できました。 現状の Nginx configure 確認1234# nginx -Vnginx version: nginx/1.9.6configure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_random_index_module --with-http_secure_link_module --with-http_stub_status_module --with-http_auth_request_module --with-http_xslt_module=dynamic --with-http_image_filter_module=dynamic --with-http_geoip_module=dynamic --with-http_perl_module=dynamic --add-dynamic-module=njs-1c50334fbea6/nginx --with-threads --with-stream --with-stream_ssl_module --with-http_slice_module --with-mail --with-mail_ssl_module --with-file-aio --with-ipv6 --with-http_v2_module --with-cc-opt='-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic' [ より nginx-1.11.1 をダウンロード1234# cd /usr/local/src# wget http://nginx.org/download/nginx-1.11.1.tar.gz# tar xvf nginx-1.11.1.tar.gz# cd nginx-1.11.1 Nginx 1.11.1 で利用モジュールやサードパーティ(GeoIP-devel)をインストール以下実行しないと nginx の configure が通りませんでした。 1# yum install pcre-devel zlib-devel openssl-devel libxml2-devel libxslt-devel gd-devel perl-ExtUtils-Embed GeoIP-devel -y gd-devel がインストールされていない場合の Nginx configure error 12./configure: error: the HTTP image filter module requires the GD library.You can either do not enable the module or install the libraries. libxslt-devel がインストールされていない場合の Nginx configure error 12./configure: error: the HTTP XSLT module requires the libxml2/libxsltlibraries. You can either do not enable the module or install the libraries. configure「現状の Nginx configure 確認」で取得したconfigure parameterから「–add-dynamic-module=njs-1c50334fbea6/nginx」を削除 --add-dynamic-module=njs-1c50334fbea6/nginx を指定した場合の Nginx configure error 12adding module in njs-1c50334fbea6/nginx./configure: error: no njs-1c50334fbea6/nginx/config was found 「–add-dynamic-module=njs-1c50334fbea6/nginx」を削除して configure 実施 123456789101112131415161718192021222324252627282930./configure --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_random_index_module --with-http_secure_link_module --with-http_stub_status_module --with-http_auth_request_module --with-http_xslt_module=dynamic --with-http_image_filter_module=dynamic --with-http_geoip_module=dynamic --with-http_perl_module=dynamic --with-threads --with-stream --with-stream_ssl_module --with-http_slice_module --with-mail --with-mail_ssl_module --with-file-aio --with-ipv6 --with-http_v2_module --with-cc-opt='-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic'......checking for GeoIP library ... foundchecking for GeoIP IPv6 support ... foundcreating objs/MakefileConfiguration summary + using threads + using system PCRE library + using system OpenSSL library + md5: using OpenSSL library + sha1: using OpenSSL library + using system zlib library nginx path prefix: &quot;/etc/nginx&quot; nginx binary file: &quot;/usr/sbin/nginx&quot; nginx modules path: &quot;/usr/lib64/nginx/modules&quot; nginx configuration prefix: &quot;/etc/nginx&quot; nginx configuration file: &quot;/etc/nginx/nginx.conf&quot; nginx pid file: &quot;/var/run/nginx.pid&quot; nginx error log file: &quot;/var/log/nginx/error.log&quot; nginx http access log file: &quot;/var/log/nginx/access.log&quot; nginx http client request body temporary files: &quot;/var/cache/nginx/client_temp&quot; nginx http proxy temporary files: &quot;/var/cache/nginx/proxy_temp&quot; nginx http fastcgi temporary files: &quot;/var/cache/nginx/fastcgi_temp&quot; nginx http uwsgi temporary files: &quot;/var/cache/nginx/uwsgi_temp&quot; nginx http scgi temporary files: &quot;/var/cache/nginx/scgi_temp&quot; ビルド12# make# make install Nginx バージョン確認12345# nginx -Vnginx version: nginx/1.11.1configure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_random_index_module --with-http_secure_link_module --with-http_stub_status_module --with-http_auth_request_module --with-http_xslt_module=dynamic --with-http_image_filter_module=dynamic --with-http_geoip_module=dynamic --with-http_perl_module=dynamic --with-threads --with-stream --with-stream_ssl_module --with-http_slice_module --with-mail --with-mail_ssl_module --with-file-aio --with-ipv6 --with-http_v2_module --with-cc-opt='-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic' 構文チェックこける。。 1234# nginx -tnginx: [emerg] unknown directive &quot;geoip_country&quot; in /etc/nginx/nginx.conf:15nginx: configuration file /etc/nginx/nginx.conf test failed サードパーティに依存するディレクティブについてはload_module で SharedObject(*.so) を呼び出す必要があります。 /etc/nignx/nginx.conf の冒頭に以下追加 1load_module &quot;modules/ngx_http_geoip_module.so&quot;; 再度構文チェック1234# nginx -tnginx: the configuration file /etc/nginx/nginx.conf syntax is oknginx: configuration file /etc/nginx/nginx.conf test is successful Nginx 設定再読み込み1# systemctl reload nginx 以上でNginx バージョンアップが無事完了しました。","link":"/2016/06/26/2016-06-27-versionup-nginx/"},{"title":"CentOS5 系 に pip インストール","text":"2017年3月にサポート終了する CentOS5 ですが今なお利用されているサーバは多々あるかと思います。 今回の目的デフォルトインストールされている Python 2.4.3 を残した状態でPython 2.6 をインストールし、pip の上位バージョンを利用できるようにする、ことを目的とします。 EPEL リポジトリ追加1$ sudo rpm -Uvh http://ftp.iij.ad.jp/pub/linux/fedora/epel/5/x86_64/epel-release-5-4.noarch.rpm 通常アップデートで EPEL からアップグレードするのを避ける様設定12345$ sudo vi /etc/yum.repos.d/epel.repo[epel]...enabled=0 EPEL リポジトリから python26 インストール1$ sudo yum install python26 -y --enablerepo=epel バージョン確認12345$ python26 -VPython 2.6.8$ python -VPython 2.4.3 あくまで python26 は pip を利用するようにする為に共存させています。 pip インストール123$ wget --no-check-certificate https://bootstrap.pypa.io/ez_setup.py$ sudo python26 ez_setup.py --insecure$ sudo easy_install-2.6 pip 12$ pip --versionpip 8.1.2 from /usr/lib/python2.6/site-packages/pip-8.1.2-py2.6.egg (python 2.6)","link":"/2016/07/10/2016-07-11-install-pip-on-centos5/"},{"title":"Raspberry Pi 3 B に Raspberry Jessie セットアップ","text":"概要Amazon Prime 会員の無料体験版でRaspberry Pi 3 B 買いました♪ 開発 Ready な状態を作るべくそのセットアップ方法をまとめました。 環境 MacBook Pro : MacOSX 10.11.5 Wifi 環境 買ったもの以下最低限必要なものです。 計 8,459円 Raspberry Pi 3 Model B (ケース付き): 5,980円 Amazon.co.jp： Raspberry Pi3 Model B ボード＆ケースセット 3ple Decker対応 (Clear)-Physical Computing Lab: パソコン・周辺機器Amazon.co.jp： Raspberry Pi3 Model B ボード＆ケースセット 3ple Decker対応 (Clear)-Physical Computing Lab: パソコン・周辺機器 SD カード (32GB) : 1,080円 Amazon | 旧モデル Transcend microSDHCカード 4GB Class10 TS4GUSDHC10 | トランセンドジャパン | パソコン・周辺機器 通販旧モデル Transcend microSDHCカード 4GB Class10 TS4GUSDHC10がパソコン・周辺機器ストアでいつでもお買い得。当日お急ぎ便対象商品は、当日お届け可能です。アマゾン配送商品は、通常配送無料（一部除く）。 電源 (2.5A対応): 1,399円 Amazon | KuGi Raspberry Pi 3 Model B チャージャー ラズベリー・パイ 3 Model B 超小型パソコン 適用 充電器 5V 2.5A アダプタ Raspberry Pi 2 Model B &amp; Pi Model B+ とも対応 ブラック | KuGi | パソコン・周辺機器 通販KuGi Raspberry Pi 3 Model B チャージャー ラズベリー・パイ 3 Model B 超小型パソコン 適用 充電器 5V 2.5A アダプタ Raspberry Pi 2 Model B &amp; Pi Model B+ とも対応 ブラックがパソコン・周辺機器スト… 電源はPCのUSBポートからでも良いかな、と思ってましたがPi3Bの推奨電流が 2.5 A となった為、2.5 A 対応電源が必要になりました。 その他 大体持ち合わせてるのでは、と思います。会社のゴミ山に捨ててあるのではないでしょうか。 HDMI ケーブル : 691円 Amazon | エレコム HDMI ケーブル スーパースリム 1m ハイスピード 【 Nintendo Switch 対応 】 4K 3DフルHD イーサネット対応 ブラック DH-HD14SS10BK | エレコム | 家電＆カメラエレコム HDMI ケーブル スーパースリム 1m ハイスピード 【 Nintendo Switch 対応 】 4K 3DフルHD イーサネット対応 ブラック DH-HD14SS10BKが家電＆カメラストアでいつでもお買い得。当日お急ぎ便対象商品は、当日お届け可能です。アマゾン配… USBキーボード : 530円 Amazon | サンワサプライ 有線USBキーボード 標準日本語配列 メンブレン ブラック SKB-L1UBK | サンワサプライ | パソコン用キーボード 通販サンワサプライ 有線USBキーボード 標準日本語配列 メンブレン ブラック SKB-L1UBKがパソコン用キーボードストアでいつでもお買い得。当日お急ぎ便対象商品は、当日お届け可能です。アマゾン配送商品は、通常配送無料（一部除く）。 USBマウス : 698円 Amazon | Amazonベーシック マウス USB有線 ブラック | Amazonベーシック(AmazonBasics) | パソコン・周辺機器 通販Amazonベーシック マウス USB有線 ブラックがパソコン・周辺機器ストアでいつでもお買い得。当日お急ぎ便対象商品は、当日お届け可能です。アマゾン配送商品は、通常配送無料（一部除く）。 その他含めても 10,378円。こりゃやるっきゃない。 早速セットアップ手順へ SDカード フォーマットSD カードフォーマットアプリ インストール以下サイトからSDカードフォーマッターアプリをダウンロードしフォーマットします。 SD Memory Card Formatter Macであれば for Mac を選択し同意してダウンロードします。 ファイルシステム確認フォーマットするSDカードがどれか知る必要がある為、SDカードを差し込む前にターミナルから以下実行 1$ df -h SDカードをMacに差し込む ※MacBook AirではSDカード差し込み口がないのでカードリーダで読込みます。 Amazon | サンワサプライ アウトレット USB3.0 カードリーダー ブラック ADR-3ML35BK 箱にキズ、汚れのあるアウトレット 品です。 | サンワサプライ | 外付メモリカードリーダー 通販サンワサプライ アウトレット USB3.0 カードリーダー ブラック ADR-3ML35BK 箱にキズ、汚れのあるアウトレット 品です。が外付メモリカードリーダーストアでいつでもお買い得。当日お急ぎ便対象商品は、当日お届け可能です。アマゾン配送商品は、通常配送無料（一部除く）。 もう一度 df -h すると新たに追加されたのが SDカード のファイルシステムです。 SDFormatter でフォーマット ※ 自分は名前を「RASP3B」としました。 もう一度 df -h ディスクイメージ名が変更されているのがわかります。 以上でSDカードフォーマット完了です。 ディスクイメージに Raspbian Jessie (Latest OS) を書き込みRaspbian Jessie ダウンロードオフィシャルサイトから最新 Raspbian OS をインストールします。数分掛かります。 Raspbian 12345$ cd ~/Downloads$ unzip 2016-05-27-raspbian-jessie.zip$ ls -al 2016-05-27-raspbian-jessie.*-rw-r--r--@ 1 kenzo staff 4019191808 5 27 20:50 2016-05-27-raspbian-jessie.img-rw-r--r--@ 1 kenzo staff 1393896178 7 10 19:27 2016-05-27-raspbian-jessie.zip アンマウント先ほど確認した ファイルシステム名 /dev/disk2s1 から s1 を取り除いた、イメージ対象をアンマウントする 1$ diskutil umountDisk /dev/disk2 ダウンロードしたイメージを書き込み1$ sudo dd if=/Users/&lt;User&gt;/Downloads/2016-05-27-raspbian-jessie.img of=/dev/rdisk2 bs=1m disk2 に r を頭に足すとアンバッファモードで実行し速度アップします。 bs=1m … 1度に書き込むサイズ 257 秒かかった 汗 以上で Raspberry Pi に挿す SDカード が作成できました。 SDカード取り出し1$ diskutil eject /dev/disk2 Raspberry PI 3 に 各種接続全て接続し終わるまでRaspberry Pi に電流を流さないでください。 SDカード挿入 USBキーボード・マウス、SDカード、電源 接続 全てが深く刺さっているのを確認した後いよいよ電源を電源アダプタに接続します。 ついた！ なにやら読み込みが始まった！ ※我が家はテレビがディスプレイ代わりです。 おぉ〜GUIのトップ画面がでてきた！ まず成功♪ Raspberry Pi 各種設定Menu &gt; Preferences &gt; Raspberry Pi Configuration クリック SDカードの全容量を利用可能にする System タブの Expand Filesystem ボタンクリック 以上でSDカードの全容量を利用可能になります。 Locale設定 Localisation タブの Set Locale ボタンをクリックします。 Language : ja (Japanese) 選択 Country : JP (Japan) 選択 Character Set : UTF-8 選択 上記選択し OK ボタンクリック Timezone設定 Localisation タブ &gt; Set Timezone ボタン クリック 再度 Set Timezone ボタンをクリックすると Asia/Tokyo が選択されています。 キーボード設定 Localisation タブの Set Keyboard ボタンをクリック Country : Japan 選択 Variant : Japanese 選択 以上選択しOK ボタンクリック Wifi 設定 Wifi 選択しパスフレーズ入力し Wifi接続 Wifi 接続が確認できます MacOS → Raspberry Pi SSH 接続 Raspberry Pi で Terminal 起動 1$ ifconfig Mac から raspberry pi に SSH接続 12345678910[MacOSX local]$ ssh pi@192.168.xxx.xxxpi@192.168.11.18's password: &lt;デフォルトパスワードは \"raspberry\"&gt;The programs included with the Debian GNU/Linux system are free software;the exact distribution terms for each program are described in theindividual files in /usr/share/doc/*/copyright.Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extentpermitted by applicable law.Last login: Mon Jul 11 23:08:51 2016 SSH ログインが確認できました！ 追記 (2016-07-08)あとあと気づいたのだけどわざわざ画面をiPhoneで撮影せずともRemote Desktop して綺麗なものを取ればよかった。。 ということでSSH ついでに Remote Desktop 機能を要する tightvncserver インストール 12345$ sudo apt-get install tightvncserver$ vncserver// 起動$ vncserver :1 設定したホスト名を指定してRemote Desktop し画面共有 1vnc://raspberrypi.local:5901 ブラウザを開いてみるインターネットに接続されているのがわかります。 ただし日本語がもれなく豆腐。。 Zabbix でも以前日本語フォントがなくて同様の事象がありました。。 日本語フォントをインストールする必要があります。 Raspberry Pi に日本語フォントインストール123456$ sudo apt-get update$ sudo apt-get install fonts-vlgothic$ sudo apt-get install ibus-mozc// 再起動$ sudo shutdown -r now もう一度ブラウザを開いてみる 日本語がちゃんと表示されています。 日本語入力できるよう、日本語 - Mozc 選択 以上でセットアップ完了です。 ご清聴ありがとうございました。 Amazon | Raspberry Pi RASPBERRYPI-DISPLAY | LCD表示器 | 産業・研究開発用品 通販Raspberry Pi RASPBERRYPI-DISPLAYがLCD表示器ストアでいつでもお買い得。当日お急ぎ便対象商品は、当日お届け可能です。アマゾン配送商品は、通常配送無料（一部除く）。","link":"/2016/07/11/2016-07-12-setup-raspberrypi3b-jessie/"},{"title":"Zabbix + Reactio 連携","text":"概要Reactio の無料化によりその機能が解放され、様々な監視・アラートツールとの連携が可能になりました。これを機に Zabbix + Reactio 連携したのでまとめました。 Reactioが無料になります 環境 Zabbix 3.0 CentOS Linux release 7.2.1511 (Core) Zabbix 3.0 がインストールされ起動されていることを前提とします。 Zabbix 管理画面で Host 設定※既に設定されている場合はスキップしてください。 Configuration &gt; Hosts &gt; create Host name: Project1 Reactio プロジェクト作成プロジェクト毎にインシデントを管理します。 https://&lt;Organization ID&gt;.reactio.jp/settings/project Zabbix 管理画面で設定している Host 名を Project 名とします。 Reactio API 発行プロジェクト作成ページと同ページ内にある API 項目の 「+」ボタンクリックし API KEY 発行します。 zabbix-reactio インストール12$ cd /usr/lib/zabbix/alertscripts$ git clone http://github.com/zabbix-reactio Zabbix DB情報 と Reactio で作成した Project と 発行した API KEY を設定ファイルに設定12$ cd /usr/lib/zabbix/alertscripts/zabbix-reactio$ vi config.inc db_info に DB 情報設定 &lt;Organization ID&gt; 設定 Project = API KEY 設定 123456789101112131415[db_info]host = &lt;DB Host&gt;user = &lt;DB user&gt;pass = &lt;DB pass&gt;db = &lt;DB name&gt;[reactio_url]default = https://&lt;Organization ID&gt;.reactio.jp/api/v1/incidents[api_key]Project1 = &lt;Project1's API KEY&gt;Project2 = &lt;Project2's API KEY&gt;Project3 = &lt;Project3's API KEY&gt;...... DB カラム追加 Zabbix alerts テーブルに Reactio Incident ID カラムを追加します。 1234567891011121314151617181920212223$ cd /usr/lib/zabbix/alertscripts/zabbix-reactio$ mysql -h &lt;DB Host&gt; -u &lt;DB user&gt; -p&lt;DB pass&gt; &lt;DB name&gt; -e &quot;`cat add_reactioincidentid.sql`&quot;$ mysql -h &lt;DB Host&gt; -u &lt;DB user&gt; -p&lt;DB pass&gt; &lt;DB name&gt; -e &quot;SHOW COLUMNS FROM alerts&quot;+---------------------+---------------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+---------------------+---------------------+------+-----+---------+-------+| alertid | bigint(20) unsigned | NO | PRI | NULL | || actionid | bigint(20) unsigned | NO | MUL | NULL | || eventid | bigint(20) unsigned | NO | MUL | NULL | || userid | bigint(20) unsigned | YES | MUL | NULL | || clock | int(11) | NO | MUL | 0 | || mediatypeid | bigint(20) unsigned | YES | MUL | NULL | || sendto | varchar(100) | NO | | | || subject | varchar(255) | NO | | | || message | text | NO | | NULL | || status | int(11) | NO | MUL | 0 | || retries | int(11) | NO | | 0 | || error | varchar(128) | NO | | | || esc_step | int(11) | NO | | 0 | || alerttype | int(11) | NO | | 0 | || reactio_incident_id | int(11) | NO | | 0 | | ← 追加されているのが確認できます+---------------------+---------------------+------+-----+---------+-------+ Reacito では全てのインシデントは ID で管理されています。Zabbix で障害アラート通知時に Reactio インシデント作成API をコールし インシデントID を保存します。 このインシデントIDは Zabbix で障害回復アラート通知時に Reatio インシデントステータス更新API をコールする際に利用します。 Zabbix Media types: Reactio 作成Administration &gt; Media types Create media type ボタンクリック 以下値を入力し Add ボタンクリック Item Value Name Reactio Type Script Script name zabbix-reactio/reactio.php Script Parameters 1 {ALERT.SUBJECT} Script Parameters 2 {ALERT.MESSAGE} Zabbix Users: Reactio 作成Administration &gt; Users Create media type ボタンクリック Reactio ユーザ作成 Media タブをクリックし Media 情報入力 Permission タブをクリックし Zabbix Super Admin 選択 Addボタン クリックし一覧に表示されることを確認 Zabbix Actions: Reactio Notification 作成Configuration &gt; Actions Create ボタンクリック Action タブ選択し Action 情報入力 Item Value Name Reactio Notification Default subject PROBLEM alert - {TRIGGER.NAME} is {TRIGGER.STATUS} Default message HOST: {HOST.NAME}TRIGGER_NAME: {TRIGGER.NAME}TRIGGER_STATUS: {TRIGGER.STATUS}TRIGGER_SEVERITY: {TRIGGER.SEVERITY}DATETIME: {DATE} / {TIME}ITEM_ID: {ITEM.ID1}ITEM_NAME: {ITEM.NAME1}ITEM_KEY: {ITEM.KEY1}ITEM_VALUE: {ITEM.VALUE1}EVENT_ID: {EVENT.ID}TRIGGER_URL: {TRIGGER.URL} Recovery message チェック Recovery subject RECOVERY alert - {TRIGGER.NAME} is {TRIGGER.STATUS} Recovery message HOST: {HOST.NAME}TRIGGER_NAME: {TRIGGER.NAME}TRIGGER_STATUS: {TRIGGER.STATUS}TRIGGER_SEVERITY: {TRIGGER.SEVERITY}DATETIME: {DATE} / {TIME}ITEM_ID: {ITEM.ID1}ITEM_NAME: {ITEM.NAME1}ITEM_KEY: {ITEM.KEY1}ITEM_VALUE: {ITEM.VALUE1}EVENT_ID: {EVENT.ID}TRIGGER_URL: {TRIGGER.URL} Enabled チェック 以下項目から判断して Reactio API を叩いてます。 subject の PROBLEM/RECOVERY HOST: {HOST.NAME} EVENT_ID: {EVENT.ID} メッセージを整形する場合でも、 上記項目は残しておくようにしてください。 Operations タブ選択し Operations 情報入力 以上 Zabbix で Reactio 連携設定完了しました。 実行結果 インシデント作成できた！ 作成したインシデントのステータスが更新された！ 今後当方、運用し始めです。障害レベルによってメッセージを変更したりと今後更新検討致します。 是非以下も合わせて Zabbix-Slack 連携もご利用ください。 zabbix3-slack 以上ご清聴ありがとうございました。","link":"/2016/07/13/2016-07-14-zabbix-reactio/"},{"title":"MacOSX に Python2, Python3 仮想環境構築","text":"経緯MacOSX デフォルトでは python 2系。 python 2.7 は 2020年までのサポート なのでpython 3 系 に慣れておこうということで3 系環境を構築しようと思いました。 ですがdlib など Python 2系でないとうまく設定ができなかった経緯があり(※自分の不手際の可能性もありますが)両方残そうということで両仮想環境を構築します。 環境12345$ sw_versProductName: Mac OS XProductVersion: 10.11.5BuildVersion: 15F34 Homebrew インストール以下オフィシャルサイト参照してください。 Homebrew Ja Python2, 3 インストール2016/07/28 現在、 python = 2.7.10, python3 = 3.4.3 1$ brew install python python3 pyenv 設定ファイル.bashrc もしくは .zshrc に追記します。 以下仮に .bashrc とします。 1$ vi ~/.bashrc 123export PYENV_ROOT=\"$HOME/.pyenv\"export PATH=\"$PYENV_ROOT/bin:$PATH\"eval \"$(pyenv init -)\" 設定読み込み 1$ source ~/.bashrc virtualenv インストール1sudo easy_install virtualenv 仮想環境構築 python2 での仮想環境構築 123$ which python/usr/local/bin/python2.7 1$ virtualenv -p /usr/local/bin/python2.7 ~/py2env python3 での仮想環境構築 123$ which python3/usr/local/bin/python3 1$ virtualenv -p /usr/local/bin/python3 ~/py3env 仮想環境切り替え Python 2.7 仮想環境へ切り替え 1$ source ~/py2env/bin/active Python 3.4 仮想環境へ切り替え 1$ source ~/py3env/bin/active 今更ですが備忘録的まとめでした。","link":"/2016/07/27/2016-07-28-setup-python2-python3-on-macos/"},{"title":"cat id_rsa.pub &gt;&gt; authorized_keys したつもりが error key_read: uudecode が出るエラー対応","text":"1$ tail -f /var/log/secure 結論「/n」 が入ってしまっていました。「/n」削除すれば問題なくなりました。 多少編集した際に混入させてしまったのかも。。","link":"/2016/07/28/2016-07-29-error-key-read-uudecode/"},{"title":"複数画像ファイルから顔検知し顔部分をトリミングしたサンプルを集める","text":"前回画像ファイルを指定し顔検知させる機能を実装しました。 顔検出 3分クッキング on MacOSX - 長生村本郷Engineers'Blog概要 ラズパイ使って家族と判断したら 「こんにちはご主人様」 家族以外なら 「通報しまーす」 と話してくれるおもちゃを 作ろうと思ってます。 その前段の前段として 静止画で顔検出してみます。 ちなみに顔検出と顔認識は意味が全く異なります。 顔検出 ... 顔部分を検出すること。 … 今回まずはサンプル画像を集めます。自分はネット上から BeautifulSoup でスクレイピングして落としてみました。(スクリプトまとめたら公開します) 適当に13枚。ゆくゆくは機械学習したいのでもっと欲しいところですが今回はスクリプトの紹介がメインなのでこの程度で。 顔部分トリミングスクリプトです。(for Python 3) 123456// clone$ git clone https://gist.github.com/kenzo0107/517258ab06715f73c4a3457e87fd25a5$ cd 517258ab06715f73c4a3457e87fd25a5// スクリプト実行$ python collect_face_samples.py -p &lt;サンプル画像が格納されているディレクトリ&gt; 実際スクリプト実行した様子です。 _trimming フォルダにトリミングされた画像群が格納されているのがわかります。 以下No順に格納されていきます。 No Item Explain 1 _resize 大小さまざまな画像サイズを一定して高さ500以下の画像にリサイズします。 2 _addbox 顔周りに囲い画像が追加された画像が格納されます。各画像でどこが顔として検知されたかの確認用です。 2 _trimming _addboxに格納されているファイルの顔部分をトリミングした画像を 64×64 サイズにリサイズし且つ、数度回転させた画像が格納されています。 これでサンプル集めが捗れば何よりです。","link":"/2016/08/03/2016-08-04-detect-face-triming-sample/"},{"title":"メンテ時に社内Wifi IPのみサイトアクセス許可する","text":"備忘録です。 サイトメンテナンスする際の手順をまとめています。 手順maintenance.htmlDocumentRoot に maintenance.html を配置 .htaccess にてアクセス制限123456789101112ErrorDocument 503 /maintenance.html&lt;IfModule mod_rewrite.c&gt; RewriteEngine On RewriteCond %{REQUEST_URI} !^.*\\.(js|css|gif|jpg|png|ico)$ RewriteCond %{REQUEST_URI} !^/cron/.*$ RewriteCond %{REQUEST_URI} !=/maintenance.html # 社内 RewriteCond %{REMOTE_ADDR} !=&lt;社内Wifi IP&gt; RewriteCond %{REMOTE_ADDR} !=&lt;社内Wifi IP&gt; RewriteRule ^.*$ - [R=503,L]&lt;/IfModule&gt; 以上です。","link":"/2016/08/08/2016-08-09-allow-ip-under-the-maintenance/"},{"title":"http https 混在サイトでの Cookie Secure 属性の扱い方","text":"問題提起https 通信環境下で Cookie に Secure 属性つけていますか？ Secure属性とは？http と https と各通信で相互の行き来がある場合などにhttps の通信でのみ使うべきCookieの値がhttp の通信に流出するおそれがあります。 それを防ぐ為に Cookie に secure 属性を付けてhttps 通信でのみ扱えるようにするという対策があります。 実例PHPの場合を扱おうと思ったのでお世話になってる メルカリさんを参照します。 Chrome の Developer Tool で Secure 項目 確認するとチェックがついているのがわかります。 PHPSESSID は php.ini の session.name で設定されているsession_id の名前です。 12345$ php -i | grep php.ini&lt;path to php.ini&gt;/php.ini$ grep 'session.name =' &lt;path to php.ini&gt;/php.inisession.name = PHPSESSID 常時 https 通信だったら secure 属性を常に設定するようにします。 session_id 発行例session_set_cookie_params 関数を用いて設定 1234$secure = true;$httponly = true;session_set_cookie_params($lifetime, $path, DOMAIN_NAME, $secure, $httponly);session_start(); HttpOnly を true とすると クライアントサイド Javascript からの Cookie 取得不可となり XSS 対策の一環となります。 cookie 発行例123$secure = true;$httponly = true;setcookie($key, $val, $expire, $path, DOMAIN_NAME, $secure, $httponly); http https 混在していたらこの場合に遭遇しました。よくショッピングサイトなどでは以下 http https が混在するケースが見受けられます。 商品一覧ページ …. http 決済情報入力ページ … https こういった場合の Cookie Secure 属性の扱い方を検討してみました。 対策1 http 用 (PHPSESSID):secure属性なし と https 用 (例: SPHPSESSID): secure属性あり で 2つ cookie を発行する 対策2 PHPSESSID は http https 共通の secure 属性なし cookie を発行する https 通信時に secure 属性付き token を発行し発行した token をチェック。不整合が起きた場合は、session_idを書き換え、session の内容を破棄する。 上記 対策1 では http,https 用の session_id の紐付けの管理がしにくい為対策2 について実装しました。 対策 2 実装例 session_id が発行されていない場合(初回アクセス時)、secure 属性なしの session_id (PHPSESSID) 発行 https 通信時は session に token がない場合、secure 属性付き token cookie を発行。 session にも token を保存 session と cookie にある token 情報を突き合わせて一致しない場合、 session_id を変更し session 内部を破棄 あえてわかりやすく冗長な書き方してます。domain, path, secure 等は 環境ごとに define するなりしてください。 123456789101112131415161718192021222324252627282930313233$domain = 'www.example.jp';$path = \"/\";if (session_id() === \"\") { $secure = false; $httponly = true; session_set_cookie_params(0, $path, $domain, $secure, $httponly); if (!ini_get(\"session.auto_start\")) { // セッション開始 session_start(); }}if (!empty($_SERVER['HTTPS'])) { if (empty($_SESSION['token'])) { $token = getToken(); $secure = true; $httponly = true; setcookie('token', $token, $expire, $path, $domain, $secure, $httponly); $_SESSION['token'] = $token; $_COOKIE['token'] = $token; } $sessid = $objCookie-&gt;getCookie('token'); if ($_SESSION['token'] != $sessid) { session_regenerate_id(); session_destroy(); }}function getToken() { return sha1(uniqid(rand(), true));} これで http で発行される session_id は secure 属性がなくともhttps 通信下での情報流出防止策をとりました。 以上です。","link":"/2016/08/09/2016-08-10-http-https-mixed/"},{"title":"PHP エンジニアであれば必ずやるべき 1 ライナー","text":"みんなが幸せになれるhiraku さんの究極の1ライナーです。 1$ composer config -g repositories.packagist composer http://packagist.jp composer による インストールが劇的に早くなります。 遅い理由は 特に packagist.org が フランスにある からとのこと 問題発生早速上記 1 ライナーを実行!! すると… 12You are running composer with xdebug enabled. This has a major impact on runtime performance. See https://getcomposer.org/xdebugDo not run Composer as root/super user! See https://getcomposer.org/root for details xdeug が enabled になっているぞと怒られている。。 xdebug 設定箇所を探す1234567$ php -i | grep xdebug/etc/php.d/xdebug.ini,xdebugxdebug support =&gt; enabled...... /etc/php.d/xdebug.ini で 設定していた。 ※環境によっては php.ini で設定している等あるので注意 xdebug を disabled に設定変更自分の PHP実行環境では xdebug を利用する必要性がなかった為、/etc/php.d/xdebug.ini 退避 1mv /etc/php.d/xdebug.ini /etc/php.d/xdebug.ini.org 再度実行あれ… また出てきた… 今度は、 1Do not run Composer as root/super user! See https://getcomposer.org/root for details root ユーザで実行するなと怒られている。。 root ユーザ以外の通常ユーザへ変更1# su - &lt;user&gt; 再度実行成功した！ 12$ composer config -g repos.packagist composer https://packagist.jp$ 設定確認packagist url が https://packagist.jp になっていることを確認 1234567891011$ cat .composer/config.json{ &quot;config&quot;: {}, &quot;repositories&quot;: { &quot;packagist&quot;: { &quot;type&quot;: &quot;composer&quot;, &quot;url&quot;: &quot;https://packagist.jp&quot; } }} 良きPHPライフを！ 参照光遅い問題を克服してcomposerを10倍速くした話 - Mercari Engineering Blogはじめまして。サーバーサイドエンジニアの中野(@Hiraku)です。2015年12月からメルカリで働いています。 2016年1月27日(水)の第98回PHP勉強会@東京にて、composerを速くする取り組みについて発表をしてきました。 composerの遅さをまじめに考える #…","link":"/2016/08/17/2016-08-18-oneliner-for-phper/"},{"title":"SlowQuery を検知して Explain で解析し Slack へ通知","text":"fluentdでエラーログをSlackへ通知 の続きです。 エラーログをSlack通知する - 長生村本郷Engineers'Blog環境 CentOS Linux release 7.1.1503 (Core) td-agent: 0.12.12 Nginx: 1.8.0 概要 社内でSlackによる連携が進み ログ管理もfluentdにまとめつつあるので エラーログで何かあったらSlack通知させようと思い… 概要MySQL DB サーバ の SlowQuery状況をリアルタイムにSlackで確認できるようにする為に導入しました。 環境 CentOS 6.5 td-agent 0.12.26 Fluent Plugin インストール今回必要モジュールをインストールします。 123# td-agent-gem install fluent-plugin-nata2# td-agent-gem install fluent-plugin-mysql_explain# td-agent-gem install fluent-plugin-sql_fingerprint fluent-plugin-nata2 SET timestamp をあらかじめ除外しアクセスしているDB情報も保持してくれる [https://github.com/studio3104/fluent-plugin-nata2] fluent-plugin-mysql_explain in_mysqlslowquery_ex で取得したJSONの sql 属性にEXPLAIN 実行結果を取得 [https://github.com/kikumoto/fluent-plugin-mysql_explain][https://github.com/kikumoto/fluent-plugin-sql_fingerprint] fluent-plugin-sql_fingerprint SQL のパラメータにマスクをする [https://github.com/kikumoto/fluent-plugin-sql_fingerprint] Percona Tool Kit インストールfluent-plugin-sql_fingerprint で利用する fingersprint をインストールします。 12# rpm -Uhv http://www.percona.com/downloads/percona-release/percona-release-0.0-1.x86_64.rpm# yum install -y percona-toolkit-2.2.5-2.noarch fluentd 設定ファイル作成以下ファイル設定するとします。 /etc/td-agent/conf.d/mysql.conf 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;source&gt; type mysqlslowquery_ex read_from_head path /var/lib/mysql/mysql-slow.log pos_file /var/log/td-agent/mysql-slow.pos tag mysqld.slow_query.bp last_dbname_file /tmp/slowquery.log.lastdb&lt;/source&gt;&lt;filter mysqld.slow_query.**&gt; type record_transformer &lt;record&gt; hostname ${hostname} &lt;/record&gt;&lt;/filter&gt;&lt;filter mysqld.slow_query.**&gt; type mysql_explain host 127.0.0.1 port 3306 database &lt;DB_NAME&gt; username &lt;DB_USER&gt; password &lt;DB_PASSWORD&gt; sql_key sql added_key explain&lt;/filter&gt;&lt;filter mysqld.slow_query.**&gt; type sql_fingerprint fingerprint_tool_path /usr/bin/pt-fingerprint&lt;/filter&gt;&lt;match mysqld.slow_query.**&gt; type copy &lt;store&gt; type slack webhook_url &lt;Slack Webhook URL&gt; channel &lt;Slack Channel&gt; username xxx DB1 [MySQL] Slow Query icon_emoji :ghost: color danger message &quot;*[User]* %s\\r\\n *[Host]* %s\\r\\n *[Query Time]* %s\\r\\n *[Lock Time]* %s\\r\\n *[Rows sent]* %s\\r\\n *[Rows Examined]* %s\\r\\n *[SQL]* %s \\r\\n *[Explain]* %s \\r\\n&quot; message_keys user,host,query_time,lock_time,rows_sent,rows_examined,fingerprint,explain flush_interval 1m &lt;/store&gt;&lt;/match&gt; ※slowquery のパス、DBのアクセスアカウントなどは各環境により変更してください。 td-agent 再起動1# service td-agent restart 確認SlowQueryを発行し、Slackに通知されるか確認します。 3秒 sleep させ、my.cnf に設定されている long-query-time の閾値の秒数を超えるようにしています。 1mysql &gt; SELECT count(*), sleep(3) FROM &lt;table&gt;; Slack 通知確認 Slack に通知されました！ show more をクリックすると Explain が通知されているのがわかる。 総評リアルタイム通知は特に新規開発時に効果的でした。 またElasticSearch へ蓄積し時間軸で分析するのはサイトのイベントとの相関性が見え面白いです。 その環境と状況により発生するスロークエリが見えてきます。 以上です。","link":"/2016/08/23/2016-08-24-detect-slowquery-to-slack/"},{"title":"SSL証明書有効期限をチェックして結果をSlackに通知","text":"概要1SSL証明書の有効期限切れでサイトにアクセスができなくなってしまった。 なんてことが発生しない様にする為に実装しました。 Shell スクリプト DOMAIN_LIST で設定した複数ドメインについて有効期限を確認します。 ※実際には Jenins で実行しており ビルドパラメータでドメイン追加を簡単にしています。 毎月第一月曜日に棚卸ししています。 Zabbix でも監視Qiita に記事がありました。 ZabbixでSSL証明書有効期限を監視する 1週間を切ったら電話通知も設定できますし対策は何にせよしておくと気持ちが落ち着きます。 以上です。","link":"/2016/08/25/2016-08-26-valid-ssl-to-slack/"},{"title":"CSVエンコード問題解決","text":"概要Linux サーバで DBで集計してCSVファイルをレポートするなんてことがあるかと思います。 CSVファイルを Linux サーバで作成しWindows, Mac にメール添付して送信するとどちらもCSVファイルを開くと文字化けしてしまう問題に遭遇しました。 この問題を解決すべく調査しました。 そもそも何で文字化け？CSVファイルはWindows, Macでは基本Excelが起動し開きますがデフォルト Shift_Jis として開こうとします。 テキストファイルに一旦開いてコピーしてエクセルに貼り付ける対策を紹介しているブログもありましたがクライアント様がお相手となる場合やファイルサイズが非常に大きい場合は一手間かける方法はNGです。 調査1 文字コードを変更してから mutt でメール添付送信 文字エンコードは nkf : Network Kanji Filter Version 2.0.7 (2006-06-13) メール送信は mutt 1.4.2.2i mutt の設定ファイルをいじりましたがうまくいかなかったです。 Shift_JIS123456789$ echo '大崎,yoshi,浜田,moto,松本' &gt; sjis.csv$ nkf -g sjis.csvUTF-8$ nkf -s --overwrite sjis.csv$ nkf -g sjis.csvShift_JIS$ echo \"Shift_JIS だよ\" | mutt -n -s \"Shift_JIS CSV 添付\" \"kenzo.tanaka0107@gmail.com\" -a sjis.csv メール受信し添付ファイルをダウンロードし文字コードチェック 12$ nkf -g sjis.csvUTF-8 あれ？ Shift_JISにエンコードして送ったんだけど UTF-8 になってる JIS (ISO-2022-JP)123456$ echo '大崎,yoshi,浜田,moto,松本' &gt; jis.csv$ nkf -j --overwrite jis.csv$ nkf -g jis.csvISO-2022-JP$ echo \"JIS だよ\" | mutt -n -s \"JIS CSV 添付\" \"kenzo.tanaka0107@gmail.com\" -a jis.csv メール受信し添付ファイルをダウンロードし文字コードチェック 12$ nkf -g jis.csvISO-2022-JP ISO-2022-JP で文字コードが変更されず送信されたけど…やっぱり文字化け… UTF-8123456$ echo '大崎,yoshi,浜田,moto,松本' &gt; utf8.csv$ nkf -w --overwrite utf8.csv$ nkf -g utf8.csvUTF-8$ echo \"UTF-8 だよ\" | mutt -n -s \"UTF-8 CSV 添付\" \"kenzo.tanaka0107@gmail.com\" -a utf8.csv メール受信し添付ファイルをダウンロードし文字コードチェック 12$ nkf -g utf8.csvUTF-8 当然文字化け… UTF-8 BOM付き123456$ echo '大崎,yoshi,浜田,moto,松本' &gt; utf8-bom.csv$ nkf --overwrite -oc=UTF-8-BOM utf8-bom.csv$ nkf -g utf8-bom.csvISO-2022-JP$ echo \"UTF-8-BOM だよ\" | mutt -n -s \"UTF-8-BOM CSV 添付\" \"kenzo.tanaka0107@gmail.com\" -a utf8-bom.csv メール受信し添付ファイルをダウンロードし文字コードチェック 12$ nkf -g utf8-bom.csvISO-2022-JP JISと同様の結果… EUC123456$ echo '大崎,yoshi,浜田,moto,松本' &gt; euc.csv$ nkf -e --overwrite euc.csv$ nkf -g euc.csvEUC-JP$ echo \"EUC だよ\" | mutt -n -s \"EUC CSV 添付\" \"kenzo.tanaka0107@gmail.com\" -a euc.csv メール受信し添付ファイルをダウンロードし文字コードチェック 12$ nkf -g euc.csvEUC-JP ファイルエンコードではうまくいきませんでした。 調査2 BINARYファイルにしてみるもっと具体的にいうと 圧縮ファイルを送ってみる Shift_JIS で CSVが開かれるのでShift_JISにエンコードします。 1234567$ echo '大崎,yoshi,浜田,moto,松本' &gt; sjis.csv$ nkf -s --overwrite sjis.csv$ zip sjis.zip sjis.csv$ nkf -g sjis.zipBINARY$ echo &quot;ZIP だよ&quot; | mutt -n -s &quot;ZIP 添付&quot; &quot;kenzo.tanaka0107@gmail.com&quot; -a sjis.zip メール受信し添付ファイルをダウンロードし文字コードチェック 123456$ nkf -g sjis.zipBINARY$ unzip sjis.zip$ nkf -g sjis.csvShift_JIS Shift_JIS のままダウンロードできてる！これは期待できそう！ うまくいった！ 総評 Windows, Mac で送られてきたCSVファイルで文字化けせず開くことができました。 圧縮した方が容量を下げて通信が行えるのでよくなりました。","link":"/2016/09/08/2016-09-09-fix-problem-csv-encode/"},{"title":"PHP 検証フィルタで Email アドレス検証 を検証する","text":"概要Email アドレスのフォーマットチェックとして PHP には検証フィルタが用意されています。 こんな使い方しますね。 12345if (filter_var($email, FILTER_VALIDATE_EMAIL)) { echo '(^-^) OK Email アドレスフォーマットとして妥当';} else { echo '(&gt;_&lt;) NG';} 以下 php.net ではこのように記述されている。 http://php.net/manual/ja/filter.filters.validate.php 値が妥当な e-mail アドレスであるかどうかを検証します。この検証は、e-mail アドレスが RFC 822 に沿った形式であるかどうかを確かめます。 ただし、コメントおよび空白の折り返し (whitespace folding) には対応していません。 検証 結果123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869[OK (^-^) EMAIL LIST]abc@gmail.coma!bc@gmail.coma#bc@gmail.coma$bc@gmail.coma%bc@gmail.coma&amp;bc@gmail.coma`bc@gmail.coma=bc@gmail.coma~bc@gmail.coma~bc@gmail.coma|bc@gmail.coma^bc@gmail.coma*bc@gmail.coma+bc@gmail.coma?bc@gmail.coma`bc@gmail.coma{bc@gmail.coma}bc@gmail.coma}bc@gmail.com!abc@gmail.com#abc@gmail.com$abc@gmail.com%abc@gmail.com&amp;abc@gmail.com=abc@gmail.com~abc@gmail.com|abc@gmail.com^abc@gmail.com*abc@gmail.com+abc@gmail.com?abc@gmail.com`abc@gmail.com{abc@gmail.com}abc@gmail.coma__bc@gmail.comabc_@gmail.comabc@vwx.yz[NG (&gt;_&lt;) EMAIL LIST]a&quot;bc@gmail.coma@bc@gmail.coma(bc@gmail.coma)bc@gmail.coma\\bc@gmail.coma:bc@gmail.coma;bc@gmail.coma&lt;bc@gmail.coma&gt;bc@gmail.coma&gt;bc@gmail.coma,bc@gmail.coma[bc@gmail.coma]bc@gmail.com¥abc@gmail.com&quot;abc@gmail.com@abc@gmail.com(abc@gmail.com)abc@gmail.com\\abc@gmail.com:abc@gmail.com;abc@gmail.com&lt;abc@gmail.com&gt;abc@gmail.com,abc@gmail.com[abc@gmail.com]abc@gmail.coma..bc@gmail.comabc.@gmail.comabc@@vwx.yz NGとしたいような Emailアドレス を通してしまいます。 &amp;abc@xyz.ab これまでの評価PHP 検証フィルタ FILTER_VALIDATE_EMAIL によるバリデーションは社内システムで利用するアカウントでのEmailアドレス検証程度であれば利用可能か。 商用サービスとして検証フィルタのみでバリデーションするのは危険かなと思いました。 マイ Email バリデーション 検証フィルタ FILTER_VALIDATE_EMAIL はベーシックに利用 利用できる文字を 半角英数字 . _ - に制限 Qiita 記事を参照しDNS 検証チェック入れました。 (ShibuyaKosuke さんありがとうございます！) 123456789101112131415161718192021function checkEmailwithDNS($email, $check_dns = false) { switch (true) { case !filter_var($email, FILTER_VALIDATE_EMAIL): case !preg_match(\"/^([a-zA-Z0-9])+([a-zA-Z0-9\\._-])*@([a-zA-Z0-9_-])+([a-zA-Z0-9\\._-]+)+$/\", $email): case !preg_match('/@([\\w.-]++)\\z/', $email, $m): return false; case !$check_dns: case checkdnsrr($m[1], 'MX'): case checkdnsrr($m[1], 'A'): case checkdnsrr($m[1], 'AAAA'): return true; default: return false; }}if (checkEmailDNS($email, true)) { echo '(^-^) OK Email アドレスフォーマットとして妥当';} else { echo '(&gt;_&lt;) NG';} マイ Email バリデーション検証 結果ほぼ弾いてくれます〜 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869[OK (^-^) EMAIL LIST]abc@gmail.coma__bc@gmail.comabc_@gmail.com[NG (&gt;_&lt;) EMAIL LIST]a!bc@gmail.coma&quot;bc@gmail.coma@bc@gmail.coma#bc@gmail.coma$bc@gmail.coma%bc@gmail.coma&amp;bc@gmail.coma`bc@gmail.coma(bc@gmail.coma)bc@gmail.coma=bc@gmail.coma~bc@gmail.coma~bc@gmail.coma|bc@gmail.coma\\bc@gmail.coma^bc@gmail.coma:bc@gmail.coma;bc@gmail.coma*bc@gmail.coma+bc@gmail.coma?bc@gmail.coma&lt;bc@gmail.coma&gt;bc@gmail.coma&gt;bc@gmail.coma,bc@gmail.coma`bc@gmail.coma[bc@gmail.coma]bc@gmail.coma{bc@gmail.coma}bc@gmail.coma}bc@gmail.com¥abc@gmail.com!abc@gmail.com&quot;abc@gmail.com@abc@gmail.com#abc@gmail.com$abc@gmail.com%abc@gmail.com&amp;abc@gmail.com(abc@gmail.com)abc@gmail.com=abc@gmail.com~abc@gmail.com|abc@gmail.com\\abc@gmail.com^abc@gmail.com:abc@gmail.com;abc@gmail.com*abc@gmail.com+abc@gmail.com?abc@gmail.com&lt;abc@gmail.com&gt;abc@gmail.com,abc@gmail.com`abc@gmail.com[abc@gmail.com]abc@gmail.com{abc@gmail.com}abc@gmail.coma..bc@gmail.comabc.@gmail.comabc@@vwx.yzabc@vwx.yz 参照そろそろメールアドレスを正規表現だけでチェックするのは終わりにしませんか？ 以上です。","link":"/2016/09/08/2016-09-09-valid-email-by-php/"},{"title":"機械学習ド素人のWebエンジニアが始める機械学習で顔認識","text":"前回顔検知と顔認識は本質が異なる。 顔検知は顔と判定すること 顔認識は顔が誰か特定の人の顔だと判定すること 今回は後者の顔認識をする仕組みをまとめました。 kenzo0107/FacialRecognitionSystemDeepLearning and Facial Recognition System by TensorFlow - kenzo0107/FacialRecognitionSystem やろうとしてること以下5つのSTEPを順を追って実施しています。 検索エンジンから画像ダウンロード ダウンロードした画像から顔検知し顔部分のみ抜き取る 顔部分を抜き取った画像を訓練用と試験用に分ける 機械学習によりモデル作成 試験用画像をモデルを利用し誰の顔であるか評価 参考機械学習では TensorFlow を利用してます。 ① 以下 TensorFlow の Hello World 的な例題とコピペすればすぐ動作するコードが記載されてます。 https://www.tensorflow.org/versions/r0.10/tutorials/mnist/beginners/index.html#mnist-for-ml-beginners https://www.tensorflow.org/versions/r0.10/tutorials/mnist/pros/index.html#deep-mnist-for-experts https://www.tensorflow.org/versions/r0.10/tutorials/mnist/tf/index.html#tensorflow-mechanics-101 https://www.tensorflow.org/versions/r0.10/tutorials/tfserve/index.html#tensorflow-serving ② すぎゃーんさんの記事は非常に参考になりました。 TensorFlowによるディープラーニングで、アイドルの顔を識別する - すぎゃーんメモ以前は MNISTの例を使って画像識別を試してみた けど、次はカラー画像についての識別を試してみる。「アイドルなんてみんな同じ顔に見える」って 最近も言われてるのかどうか知らないけど、自分もつい5年前くらいまではそう思っていたわけで。その識別を機械学習でやってみよう という試み。… やっていることもシンプルでわかりやすく、且つ、Webエンジニアの発想でサービス化してる所が興味持って望めました。 今後元々やりたかったことはRaspberry PI で顔認識して家族だと判定したら「おはよう」と挨拶させる、なのでその顔認識部分の基礎を今回は学びました。 今後は実際にRaspberry PI とどう今回の仕組みを連結させるかをやってみようと思います。 とはいえ家族の写真はそう簡単には集まらないのでSMAPで基礎作りをもうちょい頑張ってみよう！","link":"/2016/09/19/2016-09-20-machine-learning/"},{"title":"error RPC failed; curl 56 SSLRead() return on MacOS Sierra","text":"概要みんなのGo言語を購入しましてghq でGit管理してみよう！と心動いた方は多いはず 昔から peco で Git Repository 移動コマンドはしてたけど、ghq を利用したリポジトリ管理は便利ですね。 そんな折、ghq コマンドで git repository をクローンしようとした際に掲題のエラーが発生しましたので備忘録。 1234$ ghq get &lt;git repository&gt;...error: RPC failed; curl 56 SSLRead() return... 利用している git が SSL 対応していないようです。 対応 git を openssl, curl 付きで再インストール 1$ brew reinstall git --with-brewed-curl --with-brewed-openssl 再度実行12345$ ghq get &lt;git repository&gt;remote: Total 74442 (delta 145), reused 0 (delta 0), pack-reused 74160Receiving objects: 100% (74442/74442), 701.45 MiB | 1.42 MiB/s, done.Resolving deltas: 100% (50571/50571), done.Checking out files: 100% (11350/11350), done. 無事できた♪ 参照Curl: (56) SSLRead() return error -9806 - Need Help please slight_smile","link":"/2016/09/27/2016-09-28-error-rpc-failed-curl-56-sslread-return-on-macos/"},{"title":"負荷監視とその原因調査","text":"概要新卒向けの説明として簡単な備忘録です。 -Item- -Explain- %user ユーザー空間での CPU 使用率 %system カーネル空間での CPU 使用率 %iowait I/O 待ち時間の割合 %idle I/O 待ち以外で CPU が何もしていない時間の割合 ある日の Zabbix + Grafana のCPU関連のグラフから原因を調査する。 ① %iowait が飛び抜けて高い %iowait 高 %user 低 %system 低 原因スワップが大量に発生している可能性がある。 調査手順1. SwapIn &amp; SwapOut 確認1$ sar -W 2. システム全体のメモリ使用状況1$ free 3. メモリ使用率順でソート後メモリを消費しているプロセスを特定する1$ top Shift+p: CPU使用率順にソート Shift+m: メモリ使用率順にソート 実際の原因定期的に同時刻に発生した為crontab -l でクーロン設定確認したら誰も知らないバッチが動いていた汗 ② %userが飛び抜けて高い %iowait 低 %user 高 %system 低 原因CPU使用率が高い。 調査手順1. CPU使用率の高い順にソートしてプロセス特定1$ top Shift+p: CPU使用率順にソート ほんの一部分ですが参考になれば何よりです。以上です。","link":"/2016/09/20/2016-09-21-heavy-load-the-reason/"},{"title":"LINE Notify で Zabbix Alert 通知","text":"概要Zabbix アラート を LINE Notify を利用してLINEにメッセージを送るように設定しました。 手順LINE Notify アクセス[https://notify-bot.line.me/ja/] 登録してログイン サービス登録 この情報が審査されるということは特になかったですがある程度精度の高い情報を入力して登録しときました トークルーム選択しトークン発行 発行したトークンコピー Zabbix スクリプト設定kenzo0107/Zabbix3-LineNotifyZabbix3 + LineNotify. Contribute to kenzo0107/Zabbix3-LineNotify development by creating an account on GitHub. Env Zabbix 3.0 CentOS Linux release 7.2.1511 (Core) Install Steps12345[Zabbix-Server]$ cd /usr/lib/zabbix/alertscripts # AlertScriptsPath[Zabbix-Server]$ git clone https://github.com/kenzo0107/zabbix3-linenotify[Zabbix-Server]$ mv zabbix3-slack/line_notify.sh .[Zabbix-Server]$ rm -r zabbix3-linenotify[Zabbix-Server]$ chmod 755 line_notify.sh Media Types 設定 Users &gt; Media 設定 通知テストテスト環境などでNginx の process が1つ以上になったらアラート出すように設定してみた結果 所感トークルームに参加するにもLINE アカウントはプライベートアカウントなのでちょっと知られたくないわ〜という時は何とも言えない気持ちになる方もいることがわかりました。 ご利用は計画的に。 今後の期待個人的にTwilio みたいに LINE Notify で電話通知出来たら嬉しいです。 まずは障害がない世界を♪ 以上です。","link":"/2016/10/10/2016-10-11-line-notify/"},{"title":"MySQL COUNT, SUM, GROUP BY, CASE WHEN THEN で集計する","text":"概要ECサイトに新しい決済機能の利用率出したいなと思ったときのクエリです。 ちょうどいくつかの集計関数がまとまった1クエリとなったのでまとめました。 123456789101112131415161718SELECT DATE_FORMAT(create_date, '%Y-%m-%d') AS 日付 ,COUNT(order_id) AS 全件数 ,FORMAT(SUM(payment_total),0) AS \"全支払い合計(円)\" ,COUNT(payment_id = 12 or NULL) AS \"新決済機能の購入件数\" ,COUNT(customer_id = 0 or NULL) AS \"ゲスト購入数\" ,FORMAT(SUM(CASE WHEN payment_id = 12 THEN payment_total else 0 END),0) AS \"新決済機能の購入支払い合計(円)\" ,count(payment_id = 12 or NULL)/count(order_id) * 100 AS \"新決済機能の購入比率(%)\" ,SUM(CASE WHEN payment_id = 12 THEN payment_total else 0 END)/SUM(payment_total) * 100 AS \"新決済機能の購入支払い合計比率(%)\" ,COUNT(customer_id = 0 or NULL)/COUNT(order_id) * 100 AS \"ゲスト購入比率\"FROM dtb_orderWHERE 1=1 AND site_id = 1 AND create_date &gt; '2016-09-21 10:00:00'GROUP BY DATE_FORMAT(create_date, '%Y%m%d') ; パッケージ = EC-CUBE 2.11.5 新決済ID = 12 結果 CASE文をさらっと書けるようになると少し大人になった気分になります。心残りは比率部分の重複部分がまとまったらかっこいいかなと。 精進します。","link":"/2016/10/19/2016-10-20-totalization-mysql/"},{"title":"Let&#39;s encrypt SSL 証明書自動更新","text":"概要Let’s encrypt SSL は開発環境で本番環境と同等にhttps 通信プロトコルを利用したい為に利用しています。 バーチャルホストで複数ドメインを利用している場合等でもマルチドメイン SSL 証明書が取得でき便利です。 オレオレSSL証明書ではブラウザによっては「このページは保護されていません」と表示されるケースがあり非エンジニアの方によっては不信感が募ることもあります。 β版β版時代の Let’s Encrypt SSL証明書管理スクリプトです。今回作成したSSL自動更新スクリプトはこちらではありません。 digint/letsencrypt.shletsencrypt/acme client implemented as a shell-script - digint/letsencrypt.sh SSL自動更新スクリプト今回作成した Let’s Encrypt 自動更新(Apache)スクリプトです。更新判定しSlack通知します。 有効期限が 30 日を切った場合に SSL証明書を更新し httpd を再起動します。 cron設定 毎月第一土曜日 AM6:00 設定 開発環境なら土曜日に実行気づいて最低でも日曜日には治せる為。現運用ではこれは功を奏してます。 100 6 1-7 * * 6 root /root/letsencrypt.sh/refresh_cert.sh Slack通知 強制更新したい場合2016年5月07日より certbot-auto に名称変更されcertbot-auto による自動更新を以下スクリプトになります。 本番環境以外でクライアント様への確認用等でなければこちらを利用しても良いかと思います。 --force-renewal をオプション指定することで強制的に更新します。 そもそも通知いらないという場合こちらも certbot-auto cron で直接コマンド設定 毎月第一土曜日実行 一応ログには残しておく 100 6 1-7 * * 6 root /root/certbot/certbot-auto renew --force-renewal &amp;&amp; service httpd graceful &gt; /root/certbot/renewal.log 以上です。","link":"/2016/11/13/2016-11-14-autoupdate-letsencrypt/"},{"title":"Golang 簡易パフォーマンス測定","text":"概要簡易的なパフォーマンス測定覚書です。よく使うので備忘録的に保存。 12345678910111213141516171819202122232425262728293031323334package mainimport ( \"fmt\" \"runtime\" \"time\")func main() { // CPU数 cpus := runtime.NumCPU() // 開始時メモリ var startMemory runtime.MemStats runtime.ReadMemStats(&amp;startMemory) // 開始時間 start := time.Now() // do something // 経過時間 elapsed := time.Since(start) // 終了時メモリ var endMemory runtime.MemStats runtime.ReadMemStats(&amp;endMemory) fmt.Printf(\"実行時間: %f 秒 \\n\", elapsed.Seconds()) fmt.Printf(\"CPU: %d \\n\", cpus) fmt.Printf(\"Memory All: %f MB \\n\", float64(endMemory.Alloc-startMemory.Alloc)/float64(1024*1024))}","link":"/2016/11/20/2016-11-21-monitoring-performance-of-go/"},{"title":"Python ローカルとリモートサーバ上のファイル差分抽出ツール","text":"概要これまで数社経験してきましたが必ずといっていいほど存在する、現状のステータスがわからないサーバ。。 Git 上の master とも差分が激しく生じている状態。。 そんなサーバとローカルの workspace との差分を確認すべくツールを作成しました。 kenzo0107/DiffToolローカルとリモートサーバ上のファイル差分確認をします。. Contribute to kenzo0107/DiffTool development by creating an account on GitHub. 今後今の所、リモートからファイルをダウンロードする度に SSH のコネクションを張ってしまいパフォーマンス悪い(&gt;_&lt;) はじめ pysftp で ssh コネクション張ってた方がパフォーマンスよかった気がする。でも、手軽さを考えたら hostname 指定の方が良かったのでhostname 指定の方向でパフォーマンスを上げていくことを考えます。 ところがpysftp で実装してみましたが 多段 ssh アクセスの場合がうまくいかず。。随時解消させていきます。 golang で並行処理を試してパフォーマンスを上げるのも検討します。","link":"/2016/11/29/2016-11-28-detect-diff-between-local-and-remote-in-python/"},{"title":"CentOS 5系 Neobundle 対応 vim をインストール","text":"概要CentOS 5 に vim をソースからビルドしようとした所.configure 実行時にエラー発生 1234no terminal library foundchecking for tgetent()... configure: error: NOT FOUND! You need to install a terminal library; for example ncurses. Or specify the name of the library with --with-tlib. terminal library がないと怒られている。 ちなみにこんな流れで vim をソースからダウンロードしビルドしようとしました。 123456$ wget ftp://ftp.vim.org/pub/vim/unix/vim-7.4.tar.bz2$ tar xvf vim-7.4.tar.bz2$ cd vim74$ ./configure --enable-multibyte --with-features=huge --disable-selinux --prefix='/usr/local/vim-7.4'$ sudo make install$ sudo ln -s /usr/local/vim-7.4/bin/vim /usr/bin/vim ncurses-devel インストールし再度実行 1$ sudo yum install -y ncurses-devel 通りました (;_) yum でインストールする vim だと Neobundle が利用不可バージョンだった為ソースからビルドする選択にしました。 程よい rpm があれば教えてください！","link":"/2016/11/29/2016-11-30-install-neobundle-vim/"},{"title":"zsh vcs_info が使えない問題解決","text":"概要CentOS5系で yum でインストールした zsh で以下エラー発生 1precmd: vcs_info: function definition file not found Version 4.3.6 以上でないと vcs_info は利用できないそう 1vcs_info is available since zsh-beta, version 4.3.6-dev-0+20080929-1 or later バージョン確認 123$ /bin/zsh --versionzsh 4.2.6 (x86_64-redhat-linux-gnu) なので zsh バージョンアップデートする必要があります。 zsh 5.2 ダウンロード ビルド12345$ cd /usr/local/src$ wget https://sourceforge.net/projects/zsh/files/zsh/5.2/zsh-5.2.tar.gz/download$ tar xvjf zsh-5.2.tar.gz$ cd zsh-5.2$ ./configure &amp;&amp; make &amp;&amp; sudo make install インストールされた zsh バージョン確認123$ /usr/local/bin/zsh --versionzsh 5.2 (x86_64-unknown-linux-gnu) 新たにダウンロードした zsh にシェル変更12$ echo &quot;/usr/local/bin/zsh&quot; | sudo tee -a /etc/shells$ chsh -s /usr/local/bin/zsh それでも、まだ出てくるこのエラー。。 1precmd: vcs_info: function definition file not found .zcompdump を削除し zsh を実行し直す12$ rm ~/.zcompdump$ exec zsh .zscompdump はコマンドやその補間関数の定義一覧が記載されているファイルです。 無事エラーが消えました。","link":"/2016/11/29/2016-11-30-zsh-not-use-vcs_info/"},{"title":"「会員登録完了メールが迷惑メールに入っちゃいます」対策","text":"概要サイト作りにありがちな設定忘れ「会員登録完了メールが迷惑メールに入っちゃいます」 SPFレコード設定とDNSの逆引き設定が必要です。 まず DNS 正引きとは ドメイン — 問合せ —&gt; IPアドレス 以下のようなメールアドレスがあるとします。 info@hogehoge.jp この hogehoge.jp から IPアドレス を問い合わせるのが 正引き DNS 逆引きとは IP アドレス — 問合せ —&gt; ドメイン IPアドレス から hogehoge.jp を問い合わせるのが 逆引き SPFレコードSPF = Sender Policy Framework メールを送る側のポリシーを設定したフレームワークです。 何故こんなフレームワークがあるの？送信元偽装なんて簡単にできるから！ 1$ echo &quot;TEST&quot; | sendmail -f aiueo@xxxx.jp -t kenzo.xxxxxx.0107@gmail.com SPFレコードを設定することの意味SPFレコード設定する、ということは送信元の IPアドレス, ドメイン を指定することで受信先が 送信元メールアドレスとSPFレコード情報に設定している IP, ドメイン情報と一致しているかわかるようになります。 逆引き設定することの意味送信元のIPアドレスから割り出したドメインと逆引き設定されたIPに紐づくドメインとを照合し異なる場合は偽装と判断することができる為です。 Gmail などではこのフィルターが設定されていて逆引き設定されていないと迷惑メールBOXに入っちゃいます。 SPF レコード設定確認1$ dig -t TXT &lt;メールドメイン&gt; 例) gmail.com 123$ dig -t TXT gmail.comgmail.com. 300 IN TXT &quot;v=spf1 redirect=_spf.google.com&quot; 例) yahoo.co.jp 123$ dig -t TXT yahoo.co.jpyahoo.co.jp. 6 IN TXT &quot;v=spf1 include:spf.yahoo.co.jp ~all&quot; Gmail でSPFレコード設定確認 SPF: NEUTRAL の場合、SPFレコードが正しく設定されていません。 SPF: PASS の場合、SPFレコードが正しく設定されています。 以上です。","link":"/2016/12/01/2016-12-02-meiwaku-mail/"},{"title":"Jenkins が起動しない - Unable to read &#x2F;var&#x2F;lib&#x2F;jenkins&#x2F;config.xml -","text":"とある午後、Jenkins を再起動したときに出たエラー12345678910hudson.util.HudsonFailedToLoad: org.jvnet.hudson.reactor.ReactorException: java.io.IOException: Unable to read /var/lib/jenkins/config.xml at hudson.WebAppMain$3.run(WebAppMain.java:234)Caused by: org.jvnet.hudson.reactor.ReactorException: java.io.IOException: Unable to read /var/lib/jenkins/config.xml at org.jvnet.hudson.reactor.Reactor.execute(Reactor.java:269) at jenkins.InitReactorRunner.run(InitReactorRunner.java:44) at jenkins.model.Jenkins.executeReactor(Jenkins.java:912) at jenkins.model.Jenkins.&lt;init&gt;(Jenkins.java:811) at hudson.model.Hudson.&lt;init&gt;(Hudson.java:82) at hudson.model.Hudson.&lt;init&gt;(Hudson.java:78) at hudson.WebAppMain$3.run(WebAppMain.java:222) /var/lib/jenkins/config.xml が読み込めない というエラー /var/lib/jenkins/config.xml の所有者は jenkins:jenkins だけど、なぜ？ と権限周りを諸々試験して直していくとplugins をディレクトリごと所有者変更すると直りました汗 1$ sudo chown -R jenkins:jenkins /var/lib/jenkins/plugins plugin の挙動で何か所有者変更され得るものがあったのか解明せず。 同様の事象の記事を見つけたので参照","link":"/2016/12/08/2016-12-09-fix-unable-to-read-jenkins-config/"},{"title":"リモートサーバとローカルサーバとの差分のあるファイル情報を取得するツール作ってみた","text":"概要リモートサーバとローカルサーバとの差分のあるファイル情報を取得するツールを Golang で作成しました。 kenzo0107/diffrelodiffrelo is a tool to find the file differences between the remote server and the local workspace. - kenzo0107/diffrelo どんなツールか3行まとめ ローカルワークスペースを元にリモートサーバからディレクトリと拡張子指定し実行ディレクトリ上にファイルをダウンロード ローカルのワークスペースから実行ディレクトリ上にファイルをコピー 1,2 で取得したファイルから差分をチェック 利用想定ケース リモートファイルサーバとローカルワークスペースの同期状況が不明瞭である場合 この1点のみです。整備されたデプロイ環境では発生しにくいケースです。 ですが意外と多いです。 それはこんなケース 担当者が退職して引き継がれていない (&gt;_&lt;) ちょっとしたツールだしGit管理してなかった (&gt;_&lt;) 別の業者さんがサーバにアクセスでき、勝手に編集することがある (&gt;_&lt;) 上記のケースに当たる案件にたまたま担当してしまってデグレった、バグったとならない為に個人的に作ってました。 補足デフォルトの対象拡張子は php,tpl,js,css,html を対象としています。会社でPHPプロジェクトを扱うことが多いので m(_ _)m あとがき元々 Python で書いてましたが Go にしたところ4~5倍程度パフォーマンスアップしました！ 並行処理についてもPython も multiprocessing がありますが書き易さは Go かなと思いました。 ちなみに実装に当たってこちら拝読させていただきました。基礎的なGo言語の構文や環境構築、Semaphore を意識した設計はとても参考になりました。","link":"/2017/01/09/2017-01-10-detect-diff-between-local-and-remote/"},{"title":"Prometheus でサーバ監視","text":"概要以前 Ansible + Vagrant でPrometheusモニタリング環境構築について書きました。 Ansible+Vagrant でシンプルなPrometheusモニタリング環境構築 - 長生村本郷Engineers'Blog概要 Prometheus入門 にあるチュートリアルを Ansibleで簡単に構築できるようにした、 というものです。 先日2016年6月14日、 LINE株式会社でのPrometheus Casual Talks #1に参加し ナレッジのおさらいなどしたく、 構築法をまとめまし… 今回は具体的によくある設定ユースケースを順追って設定していきます。 Prometheus Server 構築 監視対象で Node Exporter 構築 Alertmanager 構築 今回やること 3行まとめ Prometheus Server モジュールインストール Prometheus Server 起動スクリプト作成 Prometheus Server 起動し自身のサーバモニタリング Prometheus の設定ファイルについては全体像を理解した後が良いと思いますのでNode Exporter の設定の後に実施したいと思います。 環境 CentOS Linux release 7.3.1611 (Core) Prometheus インストール パッケージインストール最新のバージョンをチェックしダウンロードしてください。 12345$ cd /usr/local/src$ sudo wget https://github.com/prometheus/prometheus/releases/download/v1.4.1/prometheus-1.4.1.linux-amd64.tar.gz$ sudo tar -C /usr/local -xvf prometheus-1.4.1.linux-amd64.tar.gz$ cd /usr/local$ sudo mv prometheus-1.4.1.linux-amd64 prometheus-server シンボリックリンク作成 1234567891011121314$ sudo ln -s /usr/local/prometheus-server/prometheus /bin/prometheus$ sudo ln -s /usr/local/prometheus-server/promtool /bin/promtool$ prometheus --versionprometheus, version 1.4.1 (branch: master, revision: 2a89e8733f240d3cd57a6520b52c36ac4744ce12) build user: root@e685d23d8809 build date: 20161128-09:59:22 go version: go1.7.3$ promtool versionpromtool, version 1.4.1 (branch: master, revision: 2a89e8733f240d3cd57a6520b52c36ac4744ce12) build user: root@e685d23d8809 build date: 20161128-09:59:22 go version: go1.7.3 Prometheus 起動とりあえず起動するならこれだけ 1$ sudo prometheus -config.file=/usr/local/prometheus-server/prometheus.yml ただ↑これを毎回実行するのは辛いので起動スクリプトを作成してサーバ再起動時に自動起動したりsystemctl start ... と実行したい。 起動スクリプト作成 Prometheus オプションファイル作成 123$ cat &lt;&lt; 'EOF' &gt; /usr/local/prometheus-server/optionOPTIONS=&quot;-config.file=/usr/local/prometheus-server/prometheus.yml -web.console.libraries=/usr/local/prometheus-server/console_libraries -web.console.templates=/usr/local/prometheus-server/consoles&quot;EOF Prometheus 起動スクリプト 1234567891011121314$ sudo cat &lt;&lt; 'EOF' | sudo tee /usr/lib/systemd/system/prometheus.service[Unit]Description=Prometheus ServiceAfter=syslog.target prometheus.service[Service]Type=simpleEnvironmentFile=-/usr/local/prometheus-server/optionExecStart=/bin/prometheus $OPTIONSPrivateTmp=false[Install]WantedBy=multi-user.targetEOF 起動設定 1234$ sudo systemctl daemon-reload$ sudo systemctl enable prometheus.service$ sudo systemctl start prometheus.service$ sudo systemctl status prometheus.service -l アクセスしてみる&lt;IP Address&gt;:9090 にアクセスします。以下のように表示されていれば Prometheus 起動成功です。 オプション設定でも設定した、 /usr/local/prometheus-server/consoles の各htmlにもアクセスしてみてください。 &lt;IP Address&gt;:9090/consoles/prometheus-overview.html?instance=localhost%3a9090 次回は 監視対象で Node Exporter 構築 します。","link":"/2017/01/19/2017-01-20-prometheus-monitoring/"},{"title":"Node Exporter 構築手順 + Prometheus からAWSオートスケール監視","text":"概要前回 Prometheus Server 構築しました。 Prometheus でサーバ監視 - 長生村本郷Engineers'Blog概要 以前 Ansible + Vagrant でPrometheusモニタリング環境構築について書きました。 kenzo0107.hatenablog.com 今回は具体的によくある設定ユースケースを順追って設定していきます。 Prometheus Server 構築 監視対象… 今回は 監視対象サーバで Node Exporter 構築を実施します。 今回やること 3行まとめ Node Exporter インストール Node Exporter 起動スクリプト作成 Node Exporter 起動し Prometheus Server からモニタリング 環境 CentOS Linux release 7.1.1503 (Core) Node Exporter インストール パッケージインストール 12345$ cd /usr/local/src$ sudo wget https://github.com/prometheus/node_exporter/releases/download/v0.14.0-rc.1/node_exporter-0.14.0-rc.1.linux-amd64.tar.gz$ sudo tar -C /usr/local -xvf node_exporter-0.14.0-rc.1.linux-amd64.tar.gz$ cd /usr/local$ sudo mv node_exporter-0.14.0-rc.1.linux-amd64 node_exporter シンボリックリンク作成 1234567$ sudo ln -s /usr/local/node_exporter/node_exporter /bin/node_exporter$ node_exporter --versionnode_exporter, version 0.14.0-rc.1 (branch: master, revision: 5a07f4173d97fa0dd307db5bd3c2e6da26a4b16e) build user: root@ed143c8f2fcd build date: 20170116-16:00:03 go version: go1.7.4 Node Exporter 起動とりあえず起動するならこれだけ 1$ sudo node_exporter http://node_exporter_server:9100/metrics にアクセスし取得できる node_exporter で取得しているサーバのメトリクス情報が確認できます。 Prometheus 同様、Node Exporter も起動スクリプトを作成しそこで起動管理をします。 起動スクリプト作成 Node Exporter 起動スクリプト 123456789101112$ sudo cat &lt;&lt; 'EOF' | sudo tee /usr/lib/systemd/system/node_exporter.service[Unit]Description=Node Exporter[Service]Type=simpleExecStart=/bin/node_exporterPrivateTmp=false[Install]WantedBy=multi-user.targetEOF 起動設定 1234$ sudo systemctl daemon-reload$ sudo systemctl enable node_exporter.service$ sudo systemctl start node_exporter.service$ sudo systemctl status node_exporter.service -l アクセスしてみるhttp://node_exporter_server:9100/metrics にアクセスします。 以下のように表示されていれば Node Exporter 起動成功です。 Prometheus から監視今回は AWS EC2インスタンスで起動中の node_exporter によるメトリクス取得設定です。 ※ 監視実施サーバに AmazonEC2ReadOnlyAccess をアタッチしたロール設定をする必要があります。※ 監視対象サーバに 監視対象サーバから 9100 ポート へアクセスできるようにセキュリティグループ設定します。 /usr/local/prometheus-server/prometheus.yml 編集 以下設定は region 指定しアクセス権のある Instance のメトリクスを取得します。 123456789101112131415161718192021222324# my global configglobal: scrape_interval: 15s evaluation_interval: 15s external_labels: monitor: 'codelab-monitor'rule_files: # - \"first.rules\" # - \"second.rules\"scrape_configs: - job_name: 'prometheus' static_configs: - targets: ['localhost:9090'] - job_name: 'node' ec2_sd_configs: - region: ap-northeast-1 access_key: ******************** secret_key: **************************************** port: 9100 タグで監視対象を絞る全インスタンスを監視であれば上記で問題ありません。 ですが、監視対象をある程度条件で絞りたいケースがあります。そんな時、Prometheus では relabel_configs でインスタンスの設定タグで絞る方法があります。 インスタンスのタグ設定 prometheus.yml 設定 12345678910111213141516171819202122232425262728293031# my global configglobal: scrape_interval: 15s evaluation_interval: 15s external_labels: monitor: 'codelab-monitor'rule_files: # - \"first.rules\" # - \"second.rules\"scrape_configs: - job_name: 'prometheus' static_configs: - targets: ['localhost:9090'] - job_name: 'node' ec2_sd_configs: - region: ap-northeast-1 access_key: ******************** secret_key: **************************************** port: 9100 relabel_configs: - source_labels: [__meta_ec2_tag_Stage] regex: production action: keep - source_labels: [__meta_ec2_tag_Role] regex: web action: keep prometheus.yml 編集後、再起動 1$ sudo systemctl restart prometheus.service Prometheus から node_exporter 起動したサーバを監視できているか確認http://prometheus_server:9090/consoles/node.html Up : Yes となっている Node のリンクをクリックすると CPU, Disck のグラフが確認できます。 次回は 監視対象で Alertmanager 構築します。 参照Automatically monitoring EC2 Instances – Robust Perception | Prometheus Monitoring Experts","link":"/2017/01/24/2017-01-25-prometheus-aws-autoscaling/"},{"title":"標準的な Golang インストール方法","text":"概要Golang オフィシャルサイトに書かれているそのままです。 Getting Started - The Go Programming LanguageGo is an open source programming language that makes it easy to build simple, reliable, and efficient software. 他 Golang 関連記事説明の為に、また、備忘録として記述します。 環境 CentOS Linux release 7.3.1611 (Core) 手順Golang Official - Downloads から環境に合わせ最新バージョンをインストールすることをお勧めします。 ソースからビルド 123$ cd /usr/local/src$ sudo wget https://storage.googleapis.com/golang/go1.7.5.linux-amd64.tar.gz$ sudo tar -C /usr/local -xzf go1.7.5.linux-amd64.tar.gz PATH設定 12345678$ sudo cat &lt;&lt; 'EOF' | sudo tee /etc/profile.d/golang.shexport GOPATH=$HOME/goexport PATH=$PATH:/usr/local/go/binEOF$ sudo cp /etc/profile.d/golang.sh /etc/profile.d/golang.csh$ source /etc/profile 確認 123$ go versiongo version go1.7.5 linux/amd64 以上で Golang のインストール完了です。","link":"/2017/02/02/2017-02-03-standard-instalation-golang/"},{"title":"node_expoter error occured ! hwmon collector failed","text":"概要Amazon Linux に node_exporter をインストールし起動した所以下のエラーが発生し、起動停止してしまいました。 1ERRO[0007] ERROR: hwmon collector failed after 0.000011s: open /proc/class/hwmon: no such file or directory source=\"node_exporter.go:92\" hwmon とは？ Hard Ware MONitoring. Linux カーネルのセンサーチップから Hard Ware の温度やファン回転数や電圧を取得できる。 環境情報は以下の通りです。 Amazon Linux AMI release 2016.09 node_exporter version 0.14.0-rc.1 (branch: master, revision:5a07f4173d97fa0dd307db5bd3c2e6da26a4b16e) 上記エラーですが issue として上がっていました。そして解決されてました！ Allow graceful failure in hwmon collector by mdlayher · Pull Request #427 · prometheus/node_exporter$ ./node_exporter -web.listen-address 192.168.1.1:9100 -collectors.enabled hwmon -log.level debug INFO[0000] Starting node_exporter (version… タイミングが悪かったのかマージされる前の release を取得していた為このエラーに遭遇していました。 最新のソースは master ブランチしてビルドするのが良さそうです。 以下に Amazon Linux で実施したインストール手順をまとめました。 手順Golang インストール以下Golangオフィシャルサイトにある標準的なインストール方法です。参考にしてください。 標準的な Golang インストール方法 - 長生村本郷Engineers'Blog概要 Golang オフィシャルサイトに書かれているそのままです。 Getting Started - The Go Programming Language 他 Golang 関連記事説明の為に、また、備忘録として記述します。 環境 CentOS Linux release 7… node_exporter をソースからインストールしビルド123456789$ mkdir -p $GOPATH/src/github.com/prometheus$ cd $GOPATH/src/github.com/prometheus$ git clone https://github.com/prometheus/node_exporter$ cd node_exporter$ make build// version 確認$ ./node_exporter --versionnode_exporter, version 0.14.0-rc.1 (branch: master, revision: 428bc92b1c9b38f6de96bceb67bc5d9b3bdcf6e7) ついでに起動スクリプト 事前準備 12345678910111213141516171819// pid ファイル置き場 作成$ sudo mkdir -p /var/run/prometheus// log ファイル置き場 作成$ sudo mkdir -p /var/log/prometheus// daemonize インストール$ cd /usr/local/src$ sudo git clone https://github.com/bmc/daemonize$ cd daemonize$ sudo ./configure$ sudo make$ sudo make install// PATHが通ってなかったらPATHに乗せる$ sudo cp daemonize /bin/$ which daemonize/bin/daemonize 起動スクリプト作成 1234$ cd /etc/init.d$ sudo git clone https://gist.github.com/kenzo0107/eebb6c1c06ba04b7073c171580cec445$ sudo cp eebb6c1c06ba04b7073c171580cec445/node_exporter.init ./node_exporter$ sudo chmod 0755 node_exporter 起動 1$ sudo /etc/init.d/node_exporter start 無事エラーなく起動するようになりました♪","link":"/2017/02/02/2017-02-03-node_exporter-hwmon-collector-failed/"},{"title":"Alertmanager 構築手順","text":"概要前回 Node Exporter 構築しました。 Node Exporter 構築手順 + Prometheus からAWSオートスケール監視 - 長生村本郷Engineers'Blog概要 前回 Prometheus Server 構築しました。 kenzo0107.hatenablog.com 今回は 監視対象サーバで Node Exporter 構築を実施します。 今回やること 3行まとめ Node Exporter インストール Node Exporte… 今回は監視実施サーバで Alertmanager 構築を実施します。 今回やること 3行まとめ Alertmanager インストール &amp; 起動スクリプト作成 Prometheus 通知条件設定 Alertmanager でSlack通知 Alertmanager の役割アラートのレベルによって通知先をどの程度の頻度で送信するかを管理します。あくまで、通知先の管理をします。 実際のアラート条件の設定は Prometheus Server でします。 環境 CentOS Linux release 7.3.1611 (Core) Alertmanager インストール パッケージインストール 12345$ cd /usr/local/src$ sudo wget https://github.com/prometheus/alertmanager/releases/download/v0.5.1/alertmanager-0.5.1.linux-amd64.tar.gz$ sudo tar -C /usr/local -xvf alertmanager-0.5.1.linux-amd64.tar.gz$ cd /usr/local$ sudo mv alertmanager-0.5.1.linux-amd64 alertmanager シンボリックリンク作成 12345678$ sudo ln -s /usr/local/alertmanager/alertmanager /bin/alertmanager$ alertmanager --versionalertmanager, version 0.5.1 (branch: master, revision: 0ea1cac51e6a620ec09d053f0484b97932b5c902) build user: root@fb407787b8bf build date: 20161125-08:14:40 go version: go1.7.3 Alert 通知先設定以下 Slack へ通知設定です。 123$ cd /usr/local/alertmanager$ git clone https://gist.github.com/kenzo0107/71574c2d4d70ba7a9efaa88b4ff1be1b$ mv 71574c2d4d70ba7a9efaa88b4ff1be1b/alertmanager.yml . alertmanager.yml slack 通知箇所を適宜変更して下さい。 Alertmanager 起動とりあえず起動するならこれだけ 1$ sudo alertmanager -config.file alertmanager.yml http://alertmanager_server:9093/#/alerts にアクセスすると以下のような画面が表示されます。 Prometheus 同様、Alertmanager も起動スクリプトを作成しそこで起動管理をします。 起動スクリプト作成 オプションファイル作成 123$ cat &lt;&lt; 'EOF' &gt; /usr/local/alertmanager/optionOPTIONS=&quot;-config.file /usr/local/alertmanager/alertmanager.yml&quot;EOF Alertmanager 起動スクリプト 1234567891011121314$ sudo cat &lt;&lt; 'EOF' | sudo tee /usr/lib/systemd/system/alertmanager.service[Unit]Description=Prometheus alertmanager ServiceAfter=syslog.target prometheus.alertmanager.service[Service]Type=simpleEnvironmentFile=-/usr/local/alertmanager/optionExecStart=/bin/alertmanager $OPTIONSPrivateTmp=false[Install]WantedBy=multi-user.targetEOF 起動設定 1234$ sudo systemctl daemon-reload$ sudo systemctl enable alertmanager.service$ sudo systemctl start alertmanager.service$ sudo systemctl status alertmanager.service -l アラート通知条件設定アラート通知条件は Prometheus Server 側で設定します。 Prometheus Official - ALERTING RULES サンプルとして以下設定します。 12345678910111213141516171819202122$ cd /usr/local/prometheus-server$ cat &lt;&lt; 'EOF' &gt; alerts.rules# インスタンスに 5分以上(FOR) アクセスできない場合(IF up == 0)# severity = &quot;critical&quot; とラベル付けし通知ALERT InstanceDown IF up == 0 FOR 5m LABELS { severity = &quot;critical&quot; } ANNOTATIONS { summary = &quot;Instance {{ $labels.instance }} down&quot;, description = &quot;{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.&quot;, }// Prometheus 設定ファイルチェック$ promtool check-config prometheus.ymlChecking prometheus.yml SUCCESS: 1 rule files foundChecking alerts.rules SUCCESS: 1 rules found Prometheus Server でAlertmanager URL設定Prometheus の起動オプションで Alertmanager URL 設定します。 1-alertmanager.url=http://localhost:9093 1234567$ cd /usr/local/prometheus-server$ cat &lt;&lt; 'EOF' &gt; optionOPTIONS=&quot;-config.file=/usr/local/prometheus-server/prometheus.yml -web.console.libraries=/usr/local/prometheus-server/console_libraries -web.console.templates=/usr/local/prometheus-server/consoles -alertmanager.url=http://localhost:9093&quot;EOF// Prometheus 再起動$ sudo systemctl restart prometheus.service 注意今回 Alertmanager は Prometheus Server と同サーバ上に設定しているので 1http://localhost:9093 となっていますが、ドメインが異なる場合は適宜設定してください。 Prometheus Alerts ページアクセス設定した通知条件が表示されています。 通知試験監視対象サーバの node_exporter を停止してみます。 1$ sudo systemctl stop node_exporter すると… Slack に通知が届きました！ http://alertmanager_server:9093/#/alerts にアクセスすると通知内容一覧が表示されます。 以上で簡単ながらPrometheus からリモートサーバを監視しアラート通知するところまでをまとめました。 Prometheus でサーバ監視 Node Exporter 構築手順 + Prometheus からAWSオートスケール監視 Alertmanager 構築手順 補足フロントエンドGrafana 3.x以降でデフォルトプラグインで Prometheus をサポートしていたりとPrometheus のフロントは Grafana が導入しやすく相性が良かったです。 メトリクスを自作したり、Prometheus 独自のクエリを駆使して様々なメトリクス監視が実現できます。 My alerts.rules Learning改めて自身で構築してみてLine Casual Talks #1, #2 を見直すと非常に理解が深まると思います。 Prometheus Casual Talks #1を開催しました 一助になれば何よりです。 以上です。ご静聴ありがとうございました。","link":"/2017/02/01/2017-02-02-prometheus-alertmanager/"},{"title":"node_exporter シェルでクエリ自作","text":"概要node_expoter のオプション --collector.textfile.directory で指定したディレクトリに *.prom という拡張子を配置することでそこに記述したメトリクス情報を prometheus server が読み取ってくれます。 この *.prom ファイルを一定時間毎に更新すればメトリクスが自作できる、というものです。 手順 node_exporter 自体のインストール・セットアップは以下ご参照ください。 Node Exporter 構築手順 + Prometheus からAWSオートスケール監視 - 長生村本郷Engineers'Blog概要 前回 Prometheus Server 構築しました。 kenzo0107.hatenablog.com 今回は 監視対象サーバで Node Exporter 構築を実施します。 今回やること 3行まとめ Node Exporter インストール Node Exporte… 上記手順では以下にnode_exporterを配置しています。環境によって適宜書き換えてください。 1/usr/local/node_exporter/node_exporter text_collector ディレクトリ作成12$ cd /usr/local/node_exporter$ mkdir text_collector shell 作成今回は httpd の process count のメトリクス追加することとします。 /usr/local/node_exporter/text_collector/httpd.sh 作成 cron設定12# node_exporter httpd 5分毎更新*/5 * * * * /usr/local/node_exporter/text_collector/httpd.sh httpd.prom 作成確認 /usr/local/node_exporter/text_collector/httpd.prom 1node_httpd_count 24 上記の node_httpd_count がメトリクス名になります。 node_expoter 再起動以下のようにディレクトリ指定します。 1node_expoter --collector.textfile.directory /usr/local/node_exporter/text_collector 作成したメトリクスを指定し確認する。無事できました！ これを利用してるとシェル芸で色々事足りることもあります♪ 一助になれば何よりです。","link":"/2017/02/15/2017-02-16-node_exporter/"},{"title":"Terraform で AWS インフラストラクチャ！","text":"Terraform とは インフラ構成や設定をコードにより実行計画を確認しながら自動化できるツール AWS, Google Cloud 等多数のクラウドサービスで利用可能 HashiCorp社製 今回やること インスタンス起動 Elastic IP付きインスタンス起動 インスタンス破棄 非常にミニマムなインフラ構築をしてみます。※個人のアカウントでも無料枠を使えば数十円しか掛からなかったです。 環境 Mac OS Sierra X 10.12.3 16D32 Terraform 0.9.1 terraform インストール1$ brew install terraform バージョン確認123$ terraform versionTerraform v0.9.1 では、早速使ってみます。 EC2 instance (t2.micro) 起動 main.tf 作成 12345678910provider &quot;aws&quot; { access_key = &quot;A******************Q&quot; secret_key = &quot;q**************************************Z&quot; region = &quot;ap-northeast-1&quot;}resource &quot;aws_instance&quot; &quot;example&quot; { ami = &quot;ami-71d79f16&quot; instance_type = &quot;t2.micro&quot;} 実行計画確認 1$ terraform plan 実行 1$ terraform apply Amazon Console からインスタンスが起動されたことが確認できます。 変数を別ファイルで管理上記 main.tf を github 等で管理するとなるとaccess_key, secret_key が露見されてしまいます。 その為、以下の様に別ファイルで管理することが望ましいです。 main.tf 12345678910111213141516variable &quot;access_key&quot; {}variable &quot;secret_key&quot; {}variable &quot;region&quot; { default = &quot;ap-northeast-1&quot;}provider &quot;aws&quot; { access_key = &quot;${var.access_key}&quot; secret_key = &quot;${var.secret_key}&quot; region = &quot;${var.region}&quot;}resource &quot;aws_instance&quot; &quot;example&quot; { ami = &quot;ami-71d79f16&quot; instance_type = &quot;t2.micro&quot;} terraform.tfvars terraform 実行時に自動で読み込まれるファイル 12access_key = &quot;A******************Q&quot;secret_key = &quot;q**************************************Z&quot; 実行計画確認 12345$ terraform plan...Plan: 1 to add, 0 to change, 0 to destroy. 正しく実行できることが確認できました。 terraform.tfvars ファイルは .gitignore に登録しておくなど絶対に公開されない様な設定が望ましいと思います。 EC2 instance (t2.micro) AMI変更 main.tf 12345678910111213141516variable &quot;access_key&quot; {}variable &quot;secret_key&quot; {}variable &quot;region&quot; { default = &quot;ap-northeast-1&quot;}provider &quot;aws&quot; { access_key = &quot;${var.access_key}&quot; secret_key = &quot;${var.secret_key}&quot; region = &quot;${var.region}&quot;}resource &quot;aws_instance&quot; &quot;example&quot; { ami = &quot;ami-047aed04&quot; instance_type = &quot;t2.micro&quot;} 実行計画 変更される内容が表示されます。 12345678910111213141516171819202122232425262728293031$ terraform plan...-/+ aws_instance.example ami: &quot;ami-71d79f16&quot; =&gt; &quot;ami-047aed04&quot; (forces new resource) associate_public_ip_address: &quot;true&quot; =&gt; &quot;&lt;computed&gt;&quot; availability_zone: &quot;ap-northeast-1a&quot; =&gt; &quot;&lt;computed&gt;&quot; ebs_block_device.#: &quot;0&quot; =&gt; &quot;&lt;computed&gt;&quot; ephemeral_block_device.#: &quot;0&quot; =&gt; &quot;&lt;computed&gt;&quot; instance_state: &quot;running&quot; =&gt; &quot;&lt;computed&gt;&quot; instance_type: &quot;t2.micro&quot; =&gt; &quot;t2.micro&quot; ipv6_addresses.#: &quot;0&quot; =&gt; &quot;&lt;computed&gt;&quot; key_name: &quot;&quot; =&gt; &quot;&lt;computed&gt;&quot; network_interface_id: &quot;eni-f4a214bb&quot; =&gt; &quot;&lt;computed&gt;&quot; placement_group: &quot;&quot; =&gt; &quot;&lt;computed&gt;&quot; private_dns: &quot;ip-172-31-31-239.ap-northeast-1.compute.internal&quot; =&gt; &quot;&lt;computed&gt;&quot; private_ip: &quot;172.31.31.239&quot; =&gt; &quot;&lt;computed&gt;&quot; public_dns: &quot;ec2-52-199-88-146.ap-northeast-1.compute.amazonaws.com&quot;=&gt; &quot;&lt;computed&gt;&quot; public_ip: &quot;52.199.88.146&quot; =&gt; &quot;&lt;computed&gt;&quot; root_block_device.#: &quot;1&quot; =&gt; &quot;&lt;computed&gt;&quot; security_groups.#: &quot;0&quot; =&gt; &quot;&lt;computed&gt;&quot; source_dest_check: &quot;true&quot; =&gt; &quot;true&quot; subnet_id: &quot;subnet-7a79cc0d&quot; =&gt; &quot;&lt;computed&gt;&quot; tenancy: &quot;default&quot; =&gt; &quot;&lt;computed&gt;&quot; vpc_security_group_ids.#: &quot;1&quot; =&gt; &quot;&lt;computed&gt;&quot;Plan: 1 to add, 0 to change, 1 to destroy. 最初に作成したインスタンスは破棄され、新たにインスタンスを作成していることがわかります。 terraform で新規作成・変更ができました。 次は破棄してみましょう。 EC2 instance (t2.micro) 破棄 実行計画確認 破棄対象のリソースが表示されます。 12345$ terraform plan -destroy...- aws_instance.example 実行 12345678910$ terraform destroyDo you really want to destroy? Terraform will delete all your managed infrastructure. There is no undo. Only 'yes' will be accepted to confirm. Enter a value: yes (← yes 入力)...Destroy complete! Resources: 1 destroyed. Amazon コンソールで破棄されたことを確認できます。 インスタンス起動し Elastic IP (固定IP) 設定 main.tf 1234567891011121314151617181920variable &quot;access_key&quot; {}variable &quot;secret_key&quot; {}variable &quot;region&quot; { default = &quot;ap-northeast-1&quot;}provider &quot;aws&quot; { access_key = &quot;${var.access_key}&quot; secret_key = &quot;${var.secret_key}&quot; region = &quot;${var.region}&quot;}resource &quot;aws_instance&quot; &quot;example&quot; { ami = &quot;ami-047aed04&quot; instance_type = &quot;t2.micro&quot;}resource &quot;aws_eip&quot; &quot;ip&quot; { instance = &quot;${aws_instance.example.id}&quot;} 実行計画確認 123456789101112131415161718192021222324252627282930313233343536373839$ terraform plan...+ aws_eip.ip allocation_id: &quot;&lt;computed&gt;&quot; association_id: &quot;&lt;computed&gt;&quot; domain: &quot;&lt;computed&gt;&quot; instance: &quot;${aws_instance.example.id}&quot; network_interface: &quot;&lt;computed&gt;&quot; private_ip: &quot;&lt;computed&gt;&quot; public_ip: &quot;&lt;computed&gt;&quot; vpc: &quot;&lt;computed&gt;&quot;+ aws_instance.example ami: &quot;ami-047aed04&quot; associate_public_ip_address: &quot;&lt;computed&gt;&quot; availability_zone: &quot;&lt;computed&gt;&quot; ebs_block_device.#: &quot;&lt;computed&gt;&quot; ephemeral_block_device.#: &quot;&lt;computed&gt;&quot; instance_state: &quot;&lt;computed&gt;&quot; instance_type: &quot;t2.micro&quot; ipv6_addresses.#: &quot;&lt;computed&gt;&quot; key_name: &quot;&lt;computed&gt;&quot; network_interface_id: &quot;&lt;computed&gt;&quot; placement_group: &quot;&lt;computed&gt;&quot; private_dns: &quot;&lt;computed&gt;&quot; private_ip: &quot;&lt;computed&gt;&quot; public_dns: &quot;&lt;computed&gt;&quot; public_ip: &quot;&lt;computed&gt;&quot; root_block_device.#: &quot;&lt;computed&gt;&quot; security_groups.#: &quot;&lt;computed&gt;&quot; source_dest_check: &quot;true&quot; subnet_id: &quot;&lt;computed&gt;&quot; tenancy: &quot;&lt;computed&gt;&quot; vpc_security_group_ids.#: &quot;&lt;computed&gt;&quot;Plan: 2 to add, 0 to change, 0 to destroy. 実行 1$ terraform apply Elastic IP が設定されたインスタンスが起動していることが確認できます。※ただ、起動しただけで接続できないことがわかります(&gt;_&lt;) 次回実施します [f:id:kenzo0107:20170323230208p:plain] 実行計画確認 破棄されるElastic IP, インスタンスが確認できます。 1234567$ terraform plan -destroy...- aws_eip.ip- aws_instance.example 実行 12345$ terraform destroy...Destroy complete! Resources: 2 destroyed. 全インスタンスが破棄されていることが確認できました。 その他便利な設定Map 設定 region 毎に AMI を選択し terraform apply 時に変数指定し選択可能 12345678910111213141516171819...variable &quot;amis&quot; { type = &quot;map&quot; default = { us-east-1 = &quot;ami-13be557e&quot; us-east-2 = &quot;ami-71d79f16&quot; us-west-1 = &quot;ami-00175967&quot; us-west-2 = &quot;ami-06b94666&quot; ap-northeast-1 = &quot;ami-047aed04&quot; }}...resource &quot;aws_instance&quot; &quot;example&quot; { ami = &quot;${lookup(var.amis, var.region)}&quot; instance_type = &quot;t2.micro&quot;} ex) region us-west-2 を選択 1$ terraform apply -var region=us-west-2 出力設定生成されたElastic IPの値が知りたいときなど便利です。 main.tf 1234567resource &quot;aws_eip&quot; &quot;ip&quot; { instance = &quot;${aws_instance.example.id}&quot;}output &quot;ip&quot; { value = &quot;${aws_eip.ip.public_ip}&quot;} 出力値が確認できます。 1234567$ terraform apply...Outputs:ip = 52.197.157.206 terraform output より明示的にパラメータを絞って表示できます。 123$ terraform output ip52.197.157.206 show 1234567$ terraform show...Outputs:ip = 52.197.157.206 構成のグラフ化1$ terraform graph | dot -Tpng &gt; graph.png graph.png dot コマンドがない場合は graphviz インストール 1$ brew install graphviz 総評簡単でしょ？と言われているようなツールでした♪ 引き続きプロビジョニングやAWSの各種設定をしていきたいと思います。 次回 EC2インスタンスを起動し、ローカル環境で作った鍵をキーペア登録しSSHログインを実施します。","link":"/2017/03/22/2017-03-23-terraform-aws/"},{"title":"Vagrant (Ubuntu) に Docker, Docker Compose インストール","text":"概要開発環境構築用に作成した、Vagrant (Ubuntu) に Docker と Docker Compose をインストールする手順をまとめました。 Vagrantfile 作成かなりシンプルにしてます。 Vagrantfile 1234567# -*- mode: ruby -*-# vi: set ft=ruby :Vagrant.configure(\"2\") do |config| config.vm.box = \"ubuntu/trusty64\" config.vm.network \"private_network\", ip: \"192.168.35.101\"end vagrant provision で docker compose をインストールすることも可能ですがvagrant ならではの provision だと他環境で利用できない為、OS上でインストールする方針です。 VM 起動123456789MacOS%$ vagrant up...しばし待つ...MacOS%$ vagrant ssh// ssh ログイン成功vagrant%$ Vagrant Ubuntu 環境情報確認1234567vagrant%$ lsb_release -aNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 14.04.5 LTSRelease: 14.04Codename: trusty カーネルバージョン確認12vagrant%$ uname -r3.13.0-116-generic カーネルバージョンが 3.10 より低いとバグを引き起こす危険性があるので NG。別のカーネルバージョンの高い box を使用しましょう。 古いバージョンをアンインストール1vagrant%$ sudo apt-get remove docker docker-engine extra パッケージインストールDocker に aufs ストレージを使用許可する為です。 12345vagrant%$ sudo apt-get updatevagrant%$ sudo apt-get -y install \\ wget \\ linux-image-extra-$(uname -r) \\ linux-image-extra-virtual Docker インストール123456789// Docker インストールvagrant%$ wget -qO- https://get.docker.com/ | sh// Docker バージョン確認vagrant%$ docker --versionDocker version 17.04.0-ce, build 4845c56// vagrant ユーザを docker グループに追加 (一旦ログアウトしログインし直すと有効になることを確認できます)vagrant%$ sudo usermod -aG docker vagrant Docker Compose インストール1234567891011vagrant%$ curl -L &quot;https://github.com/docker/compose/releases/download/1.12.0/docker-compose-$(uname -s)-$(uname -m)&quot; &gt; ~/docker-compose// 実行権限付与vagrant%$ chmod +x ~/docker-compose// 実行パス移動vagrant%$ sudo mv docker-compose /usr/bin/// Docker Compose バージョン確認vagrant%$ docker-compose --versiondocker-compose version 1.12.0, build b31ff33 一度ログアウトし再度ログイン123vagrant%$ exitMacOS%$ vagrant sshvagrant%$ メモリとスワップ利用量の調整Docker を使用していない時にメモリのオーバーヘッドとパフォーマンス劣化を低減させる様、GRUB (GRand Unified Bootloader: グラブ or ジーラブ) に設定する必要があります。 grub 設定 1234vagrant%$ sudo vi /etc/default/grub# GRUB_CMDLINE_LINUX=&quot;&quot;GRUB_CMDLINE_LINUX=&quot;cgroup_enable=memory swapaccount=1&quot; GRUB (GRand Unified Bootloader: グラブ or ジーラブ) 更新 1234vagrant%$ sudo update-grub// 念の為、再起動vagrant%$ sudo reboot 以上で準備完了です♪ 早速試してみる簡単なチュートリアルとして nginx コンテナを立ててみます。 12345678910vagrant%$ docker run --rm -p 80:80 nginx:mainline-alpineUnable to find image 'nginx:mainline-alpine' locallymainline-alpine: Pulling from library/nginx709515475419: Already exists4b21d71b440a: Pull completec92260fe6357: Pull completeed383a1b82df: Pull completeDigest: sha256:5aadb68304a38a8e2719605e4e180413f390cd6647602bee9bdedd59753c3590Status: Downloaded newer image for nginx:mainline-alpine ブラウザアクセスローカルの Mac からブラウザでアクセス http://192.168.35.101 ※192.168.35.101 … Vagrant で指定した private ip 問題なく Welcome ページが表示されました。 先程のログに以下のようにアクセスログが出力されるのがわかります。 1192.168.35.1 - - [13/Apr/2017:10:45:46 +0000] &quot;GET / HTTP/1.1&quot; 304 0 &quot;-&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36&quot; &quot;-&quot; MacOS → Vagrant → Dockerとアクセスできるようになりました♪ 追記今回作成した Box を Vagrant Cloud に置きました。 https://atlas.hashicorp.com/kenzo0107/boxes/ubuntu14.04.5LTS-docker-dockercompose/ こちら設定を元にこれから様々な環境構築を記載していきたいと思います♪ 参照 AUFS ストレージ・ドライバの使用 Docker run リファレンス","link":"/2017/04/12/2017-04-13-install-docker-and-docker-compose-on-vagrant/"},{"title":"Terraform でキーペア登録し起動した EC2 に SSH接続","text":"今回やること Mac ローカルで公開鍵、秘密鍵を生成 Terraform でEC2起動、セキュリティグループで SSH (ポート22)許可、key-pair 登録 Terraform の Hello World 的なチュートリアルと思っていただけたら幸いです。 環境 Mac OS 10.12.3 (Sierra) Terraform 0.9.1 公開鍵、秘密鍵生成RSAフォーマットで鍵を生成します。 12345678910111213$ ssh-keygen -t rsaEnter file in which to save the key (/Users/kenzo_tanaka/.ssh/id_rsa): /Users/kenzo_tanaka/.ssh/terraform-testEnter passphrase (empty for no passphrase): (空のままEnter)Enter same passphrase again: (空のままEnter)......// 生成されたか確認$ ls ~/.ssh/terraform-test*/Users/kenzo_tanaka/.ssh/terraform-test # 秘密鍵/Users/kenzo_tanaka/.ssh/terraform-test.pub # 公開鍵 公開鍵を起動したEC2インスタンスに登録し秘密鍵でアクセスします。 以下のように利用する予定です。 1$ ssh -i ~/.ssh/terraform-test &lt;ec2 user&gt;@&lt;ec2 public ip&gt; Terraform 設定ファイル Point ! resource &quot;aws_key_pair&quot; で使用する公開鍵設定をしています。 resource &quot;aws_security_group&quot; でSSH（ポート22）を開いてます。 resource &quot;aws_instance&quot; で使用しているセキュリティグループの指定は vpc_security_group_ids を利用 セキュリティグループの条件追加・削除する場合にインスタンスを一度削除し作り直すことをしたくない場合に vpc_security_group_ids を利用すると良いです。 main.tf 123456789101112131415161718192021222324252627282930provider &quot;aws&quot; { access_key = &quot;${var.access_key}&quot; secret_key = &quot;${var.secret_key}&quot; region = &quot;${var.region}&quot;}resource &quot;aws_instance&quot; &quot;example&quot; { ami = &quot;${lookup(var.amis, var.region)}&quot; instance_type = &quot;t2.nano&quot; key_name = &quot;${aws_key_pair.auth.id}&quot; vpc_security_group_ids = [&quot;${aws_security_group.default.id}&quot;]}resource &quot;aws_key_pair&quot; &quot;auth&quot; { key_name = &quot;${var.key_name}&quot; public_key = &quot;${file(var.public_key_path)}&quot;}resource &quot;aws_security_group&quot; &quot;default&quot; { name = &quot;terraform_security_group&quot; description = &quot;Used in the terraform&quot; # SSH access from anywhere ingress { from_port = 22 to_port = 22 protocol = &quot;tcp&quot; cidr_blocks = [&quot;0.0.0.0/0&quot;] }} variables.tf 12345678910111213141516171819202122232425262728variable &quot;access_key&quot; {}variable &quot;secret_key&quot; {}variable &quot;region&quot; { default = &quot;ap-northeast-1&quot;}variable &quot;amis&quot; { type = &quot;map&quot; default = { us-east-1 = &quot;ami-13be557e&quot; us-west-2 = &quot;ami-21f78e11&quot; ap-northeast-1 = &quot;ami-1bfdb67c&quot; }}variable &quot;key_name&quot; { description = &quot;Desired name of AWS key pair&quot;}variable &quot;public_key_path&quot; { description = &lt;&lt;DESCRIPTIONPath to the SSH public key to be used for authentication.Ensure this keypair is added to your local SSH agent so provisioners canconnect.Example: ~/.ssh/terraform.pubDESCRIPTION} terraform.tfvars 12345access_key = &quot;A******************Q&quot;secret_key = &quot;q**************************************Z&quot;key_name = &quot;terraform-test&quot;public_key_path = &quot;~/.ssh/terraform-test.pub&quot; いざ実行 実行計画確認 1$ terraform plan 実行 1$ terraform apply 確認 AWS コンソール上で起動確認 キーペアに terraform-test が指定されています。 vpc, subnet も自動的にアタッチされてます。 キーペア一応キーペアを見てみると登録されているのがわかります。 セキュリティグループ確認 SSH ログイン確認 1$ ssh -i ~/.ssh/terraform-test ubuntu@ec2-54-65-244-25.ap-northeast-1.compute.amazonaws.com 無事SSHログインできました！ 所感terraform を見ながら各パラメータの利用意図を確認しながら設定してみましたがパラメータの説明自体はざっくりで利用方法まではわからないです。 Teffaform のチュートリアルに始まりその他 Stack Overflow で適宜パターンを蓄積していく学習が程よいと思います。 参考Terraformで立てたEC2に後からSGを追加しようとするとEC2が再作成される - tjinjin's blogAbout 初回に立てた時はSGが1つだったが、あとからSGを追加したくなったときどうなるか試した結果です。","link":"/2017/03/26/2017-03-27-create-keypair-by-terraform/"},{"title":"Docker Compose チュートリアル","text":"前回 Vagrant (Ubuntu)で Docker, Docker Compose 環境構築しました。 Vagrant (Ubuntu) に Docker, Docker Compose インストール - 長生村本郷Engineers'Blog概要 開発環境構築用に作成した、 Vagrant (Ubuntu) に Docker と Docker Compose をインストールする手順をまとめました。 Vagrantfile 作成 かなりシンプルにしてます。 Vagrantfile # -*- mode: ruby -*… 上記環境を元に Docker Compose チュートリアルを実行しました。 完全な備忘録です。 プロジェクトディレクトリ作成1vagrant%$ mkdir composetest &amp;&amp; cd composetest app.py 作成1234567891011121314from flask import Flaskfrom redis import Redisapp = Flask(__name__)redis = Redis(host='redis', port=6379)@app.route('/')def hello(): count = redis.incr('hits') #return 'Hello World! I have been seen {} times.\\n'.format(count) return 'Hello from Docker! I have been seen {} times.\\n'.format(count)if __name__ == \"__main__\": app.run(host=\"0.0.0.0\", debug=True) requirements.txt 作成pip でインストールするモジュールを列挙します。 12flaskredis Dockerfile 作成12345FROM python:3.4-alpineADD . /codeWORKDIR /codeRUN pip install -r requirements.txtCMD [\"python\", \"app.py\"] docker-compose.yml 作成123456789101112version: '2'services: web: build: . ports: - \"5000:5000\" volumes: - .:/code redis: image: \"redis:alpine\" Docker Compose でイメージビルド、コンテナ起動12345678910111213141516171819202122232425262728293031323334vagrant%$ docker-compose upCreating composetest_web_1Creating composetest_redis_1Attaching to composetest_redis_1, composetest_web_1redis_1 | 1:C 13 Apr 14:25:38.483 # Warning: no config file specified, using the default config. Inorder to specify a config file use redis-server /path/to/redis.confredis_1 | _._redis_1 | _.-``__ ''-._redis_1 | _.-`` `. `_. ''-._ Redis 3.2.8 (00000000/0) 64 bitredis_1 | .-`` .-```. ```\\/ _.,_ ''-._redis_1 | ( ' , .-` | `, ) Running in standalone moderedis_1 | |`-._`-...-` __...-.``-._|'` _.-'| Port: 6379redis_1 | | `-._ `._ / _.-' | PID: 1redis_1 | `-._ `-._ `-./ _.-' _.-'redis_1 | |`-._`-._ `-.__.-' _.-'_.-'|redis_1 | | `-._`-._ _.-'_.-' | http://redis.ioredis_1 | `-._ `-._`-.__.-'_.-' _.-'redis_1 | |`-._`-._ `-.__.-' _.-'_.-'|redis_1 | | `-._`-._ _.-'_.-' |redis_1 | `-._ `-._`-.__.-'_.-' _.-'redis_1 | `-._ `-.__.-' _.-'redis_1 | `-._ _.-'redis_1 | `-.__.-'redis_1 |redis_1 | 1:M 13 Apr 14:25:38.486 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.redis_1 | 1:M 13 Apr 14:25:38.486 # Server started, Redis version 3.2.8redis_1 | 1:M 13 Apr 14:25:38.486 # WARNING overcommit_memory is set to 0! Background save may failunder low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf andthen reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.redis_1 | 1:M 13 Apr 14:25:38.486 * The server is now ready to accept connections on port 6379web_1 | * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)web_1 | * Restarting with statweb_1 | * Debugger is active!web_1 | * Debugger PIN: 135-466-976web_1 | 192.168.35.1 - - [13/Apr/2017 14:25:53] &quot;GET / HTTP/1.1&quot; 200 -web_1 | 192.168.35.1 - - [13/Apr/2017 14:25:53] &quot;GET /favicon.ico HTTP/1.1&quot; 404 - ブラウザにアクセスしてみる。 表示されました！ リロードする度に以下数字部分がインクリメントされるのが確認できます。 1Hello from Docker! I have been seen 1 times. 便利♪","link":"/2017/04/12/2017-04-13-tutorial-docker-compose/"},{"title":"Docker コマンド早見表","text":"バージョン123docker --versionDocker version 17.04.0-ce, build 4845c56 コンテナ123456789101112131415docker ps # running コンテナ一覧docker ps -a # 全コンテナ一覧表示docker start &lt;CONTAINER ID&gt; # コンテナ起動docker restart &lt;CONTAINER ID&gt; # コンテナ再起動docker stop &lt;CONTAINER ID&gt; # コンテナ終了docker kill &lt;CONTAINER ID&gt; # コンテナ強制終了docker attach &lt;CONTAINER ID&gt; # コンテナへアタッチdocker top &lt;CONTAINER ID&gt; # コンテナプロセスを表示docker logs -f &lt;CONTAINER ID&gt; # コンテナログ表示docker inspect &lt;CONTAINER ID&gt; # コンテナ情報表示docker rm &lt;CONTAINER ID&gt; # コンテナID指定でコンテナ削除dockre rm &lt;CONTAINER NAME...&gt; # コンテナ名(複数)指定でコンテナ削除docker container prune # 停止コンテナを削除dockr run -it -h &lt;host name&gt; &lt;IMAGE&gt;[:TAG] &lt;command&gt; # イメージよりコンテナ起動 command 実施 イメージ1234docker pull &lt;IMAGE NAME&gt;[:tag] # イメージダウンロードdocker images ls # イメージ一覧docker inspect &lt;IMAGE ID&gt; # イメージ情報表示docker rmi &lt;IMAGE ID&gt; # イメージ削除 イメージ作成12docker build -t NAME[:TAG]docker commit -m &quot;&lt;comment here&gt;&quot; &lt;CONTAINER ID&gt; &lt;IMAGE NAME&gt;[:TAG] Docker Compose12345docker-compose up -d # デタッチモードでイメージよりコンテナ起動docker-compose ps # コンテナ一覧表示docker-compose stop # docker compose 管理下全てのコンテナ停止docker-compose start # docker compose 管理下全てのコンテナ起動docker-compose rm # docker compose 管理下全ての停止コンテナ削除","link":"/2017/04/13/2017-04-14-teat-docker/"},{"title":"AWS [Retirement Notification] 対応","text":"概要とある日、AWS よりこんなメール通知が来ました。 要約するとホストしている基盤のハードウェアで回復不可能な障害が検知されたので指定期限までに対応しないとインスタンスが停止する、とのこと。 今回こちらの対応をまとめました。 12345678910111213141516171819Dear Amazon EC2 Customer,We have important news about your account (AWS Account ID: xxxxxxxxxxxx). EC2 has detected degradation of the underlying hardware hosting your Amazon EC2 instance (instance-ID: i-xxxxxxxx) in the ap-northeast-1 region. Due to this degradation, your instance could already be unreachable. After 2017-04-25 04:00 UTC your instance, which has an EBS volume as the root device, will be stopped.You can see more information on your instances that are scheduled for retirement in the AWS Management Console (https://console.aws.amazon.com/ec2/v2/home?region=ap-northeast-1#Events)* How does this affect you?Your instance's root device is an EBS volume and the instance will be stopped after the specified retirement date. You can start it again at any time. Note that if you have EC2 instance store volumes attached to the instance, any data on these volumes will be lost when the instance is stopped or terminated as these volumes are physically attached to the host computer* What do you need to do?You may still be able to access the instance. We recommend that you replace the instance by creating an AMI of your instance and launch a new instance from the AMI. For more information please see Amazon Machine Images (http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html) in the EC2 User Guide. In case of difficulties stopping your EBS-backed instance, please see the Instance FAQ (http://aws.amazon.com/instance-help/#ebs-stuck-stopping).* Why retirement?AWS may schedule instances for retirement in cases where there is an unrecoverable issue with the underlying hardware. For more information about scheduled retirement events please see the EC2 user guide (http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-retirement.html). To avoid single points of failure within critical applications, please refer to our architecture center for more information on implementing fault-tolerant architectures: http://aws.amazon.com/architectureIf you have any questions or concerns, you can contact the AWS Support Team on the community forums and via AWS Premium Support at: http://aws.amazon.com/supportSincerely,Amazon Web Services AWS Console イベントを見ると一覧で表示されている。 AWS Console 詳細を見ると Notice が出ている。 ToDOボリュームタイプによって異なります。 EBSボリューム インスタンスの停止後、起動 (Reboot は ×) インスタンスストアボリューム AMI からインスタンス再作成、データ移行 今回は EBS ボリューム対応について記載してます。 対応対象インスタンスが多かったのでローカルPC (macOS) から awscli でインスタンス停止→起動するシェル作成しました。本番環境で利用されるインスタンスも含まれていた為、1件ずつ実行することとしました。 事前準備 awscli, jq インストール 1$ brew install awscli jq 各アカウント毎のアクセスキー、シークレットキー等設定 12$ aws configure --profile &lt;profile&gt;$ grep 'profile' ~/.aws/config インスタンスの停止・再起動シェル 以下のように実行するとインスタンスが起動(running)していれば停止後、再び起動し、ステータスチェックをするようにしました。 1$ sh stop_and_start_ec2_instance.sh &quot;&lt;profile&gt;&quot; &quot;&lt;instance id&gt;&quot; イベント情報取得シェル.aws/config で設定されている profile を全てチェックし未対応インスタンスのみ表示する様修正しました。 結果確認大体 1インスタンス 5分程度で完了。問題なく停止起動でき、対象イベントが一覧から消えたことを確認しました♪ 所感メンテ対象インスタンスの Region が northeast に集中していたのが気になる点でした。このインスタンス何に使ってるんだっけ？とならない様に、インスタンスやprivate key の命名ルール必須と感じました。 以上です。","link":"/2017/04/17/2017-04-18-aws-retairement-notification/"},{"title":"docker-compose で開発環境構築 〜Nginx アクセスログ(ltsv) を fluentd + elasticsearch + kibana で可視化〜","text":"概要前回構築した Vagrant 環境上で docker-compose による開発環境構築をします。 Vagrant (Ubuntu) に Docker, Docker Compose インストール - 長生村本郷Engineers'Blog概要 開発環境構築用に作成した、 Vagrant (Ubuntu) に Docker と Docker Compose をインストールする手順をまとめました。 Vagrantfile 作成 かなりシンプルにしてます。 Vagrantfile # -*- mode: ruby -*… 今回は前回の続きで Nginx のアクセスログを Elasticsearch + Fluentd + Kibana で可視化してみます。アプリ 簡単構築手順12345678910macOS% $ git clone https://github.com/kenzo0107/vagrant-dockermacOS% $ cd vagrant-dockermacOS% $ vagrant upmacOS% $ vagrant sshvagrant% $ cd /vagrant/nginx-efk// -d デタッチモードでないのは各コンテナの起動状況がログで見える為です。vagrant% $ docker-compose up...... docker-compose 構成Git にまとめています。 kenzo0107/vagrant-dockerDocker on Vagrant(ubuntu). Contribute to kenzo0107/vagrant-docker development by creating an account on GitHub. 12345678910├── docker-compose.yml├── fluentd│ ├── conf│ │ ├── conf.d│ │ │ └── nginx.log.conf│ │ └── fluent.conf│ └── Dockerfile└── nginx └── conf └── nginx.conf ポイント nginx のログ格納場所を volume 指定しホスト側とシンク。 それを fluentd 側でも volume 指定し tail するようにしました。 以下のようなイメージです。 ブラウザから Nginx 起動確認ブラウザから http://192.168.35.101/ にアクセスするとNginx の Welcome ページが確認できます。 先程の docker-compose up 後に以下のようなログが見えfluentd が Nginx アクセスログを捕まえているのがわかります。 Kibana にアクセスブラウザから http://192.168.35.101:5601 にアクセスするとKibana ページが表示されます。 Index name or pattern fluentd-* 指定 Time-field name @timestamp 指定 Create ボタン押下 レフトメニューから Discover クリック macOS からログ確認当然ながら macOS と vagrant とシンクしているのでmacOS 上からもログが tail できます。 1macOS%$ tail -f &lt;path/to/vagrant-docker&gt;/docker/nginx-efk/_log/nginx/access.log 以上です。参考になれば幸いです。","link":"/2017/04/20/2017-04-21-fke-on-docker-compose/"},{"title":"peco 小技シリーズ  〜多段ssh + peco, ghq + peco + atom〜","text":"概要本当に小技です。が、割と使ってみると作業時間の短縮となって便利という声を頂きpeco 関連でよく使うものを記事にしました。 peco インストール1macOS %$ brew install peco ssh ログインする Host を検索して選択前提条件として ~/.ssh/config で接続ホストを管理しています。 上記を ~/.bashrc などに貼り付けて source ~/.bashrc すれば使えます♪ 1234567macOS %$ git clone https://gist.github.com/kenzo0107/06b3b1e202f36b70815cfe0207292a66macOS %$ cd 06b3b1e202f36b70815cfe0207292a66macOS %$ cat peco-sshconfig-ssh.sh &gt;&gt; ~/.bashrcmacOS %$ source ~/.bashrc// sshc 実行！macOS %$ sshc 上記の様に順調に進むと候補がリストされます。モザイクしかなくすいません (&gt;_&lt;) ghq で管理している repository を検索して選択し atom で開く こちらも同様、 1234567macOS %$ git clone https://gist.github.com/kenzo0107/e460e31ae2478341cc7a39859ad7fefdmacOS %$ cd e460e31ae2478341cc7a39859ad7fefdmacOS %$ cat peco-git-atom.sh &gt;&gt; ~/.bashrcmacOS %$ source ~/.bashrc// opg 実行！macOS %$ opg 他にも様々な箇所で peco を利用させて頂いてます。こんな peco の使い所あるよーという方、是非教えてください♪ 参照zsh ですが dotfile をまとめてます。 kenzo0107/dotfilesdotfile setting. Contribute to kenzo0107/dotfiles development by creating an account on GitHub.","link":"/2017/04/26/2017-04-27-peco/"},{"title":"Raspberry PI3 Model B に docker-compose で Nginx で認証かけて Prometheus + Node Exporter + Grafana + cAdvisor構築","text":"概要Raspi3に docker-compose で Prometheus による監視機構を作成しました。 kenzo0107/vagrant-dockerDocker on Vagrant(ubuntu). Contribute to kenzo0107/vagrant-docker development by creating an account on GitHub. 環境 Raspberry Pi 3 Model B (Raspbian GNU/Linux 8) arm7l Docker version 17.04.0-ce, build 4845c56 docker-compose version 1.9.0, build 2585387 Raspi に docker インストール123raspi%$ wget -qO- https://get.docker.com/ | shraspi%$ sudo usermod -aG docker piraspi%$ sudo gpasswd -a $USER docker Raspi に docker-compose インストール123456raspi%$ sudo apt-get updateraspi%$ sudo apt-get install -y apt-transport-httpsraspi%$ echo &quot;deb https://packagecloud.io/Hypriot/Schatzkiste/debian/ jessie main&quot; | raspi%sudo tee /etc/apt/sources.list.d/hypriot.listraspi%$ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 37BBEE3F7AD95B3Fraspi%$ sudo apt-get updateraspi%$ sudo apt-get install docker-compose version 確認 12raspi%$ docker-compose --versiondocker-compose version 1.9.0, build 2585387 docker-compose のプロジェクト設定123raspi%$ cd ~raspi%$ git clone https://github.com/kenzo0107/vagrant-dockerraspi%$ cd vagrant-docker/docker/prometheus-grafana-on-raspi3 Nginx Basic 認証設定1.htpasswd 作成時のユーザ/パス == GF_SECURITY_ADMIN_USER/GF_SECURITY_ADMIN_PASSWORD である必要があります。 Grafana の認証機能により設定した Basic 認証でログインできる仕組みがあり、一致しない場合、ログインできず、失敗します。 grafana/env 12GF_SECURITY_ADMIN_USER=admin-userGF_SECURITY_ADMIN_PASSWORD=admin-pass .htpasswd 1234567raspi%$ htpasswd -c nginx/conf/conf.d/.htpasswd admin-userNew password: (「admin-pass」と入力しEnter)Re-type new password: (「admin-pass」と入力しEnter)Adding password for user admin-userraspi%$ cat nginx/conf/conf.d/.htpasswdadmin-user:$apr1$JLxC83lt$uO7aEn9Z59fZtba4EA7C6/ Cron設定Raspi の温度や電圧を定期取得し Prometheus に読み込ませるファイル(*.prom)作成します。 1*/5 * * * * &lt;home/to/path&gt;/vagrant-docker/docker/prometheus-grafana-on-raspi3/node-exporter/collector/raspi.sh docker-compose により Docker 起動1raspi%$ docker-compose up -d Grafana にアクセスしてみるhttp://&lt;your_server_ip&gt;:13000 にアクセスすると .htpasswd で指定したユーザ/パスを求められるので入力します その後、Grafana のページが表示されれば成功です。 「Add data Source」をクリックします。 Data Source 設定以下の様に設定し「Save &amp; Test」をクリックししSuccessすることを確認します。 Dashboard.json インポート左上のアイコンから Dashboards &gt; Import 選択し DockerDashboard.json をインポートします。 Dashboard 表示 ポイント !セキュリティ上の観点から外から直接 Grafana を参照させない様にしました。nginx/conf/conf.d/default.conf 12345678910server { listen 80; location / { auth_basic &quot;Restricted&quot;; auth_basic_user_file /etc/nginx/conf.d/.htpasswd; proxy_pass http://grafana:3000/; }} image 選びは慎重に。以下の点で非常にハマりました。 Raspberry Pi3 Model B (今回はarm7l)上で動作するか Nginx で Proxy 機能が正しく動作するか nginx のproxy機能で grafana に繋げても 以下の様に表示されてしまうケースにぶつかりまくりました。 1{{alert.title}} 総評イメージ探しについ時間取ってしまいましたが自作した方が早かったかもと反省。 今回は自身を監視するという仕組みにしましたが外部から監視し相互に監視し合う体制が必要です。家庭内稟議が通ればもう一台getしよう！ そして、家庭の為になるものを作ろう！","link":"/2017/04/30/2017-05-01-nginx-prometheus-nodeexporter-grafana-cadvisor-on-raspi3/"},{"title":"無料枠で運用！ GKE + Kubernetes で Hubot 〜CLIから実行編〜","text":"概要 無料枠を使って Slack 連携する Hubot を GKE で構築します。 おまけで JIRA 連携も Google Cloud SDK のインストール方法と初期化Mac OS X 用クイックスタート を参照して SDK をダウンロードします。 Mac OS X (x86_64), (x86) かは以下コマンドで確認 123macOS%$ uname -mx86_64 kubectl のインストール1macOS%$ gcloud components update kubectl gcloud デフォルト設定以下は作成したプロジェクト、リージョン、ゾーンを設定してます。今後 gcloud コマンド実行時に region 指定等しなくて良くなります。 作成したプロジェクトID : hubot-167007 us-west 利用で無料枠を使う為に US リージョンに設定してます。 1234macOS%$ gcloud auth loginmacOS%$ gcloud config set project hubot-167007macOS%$ gcloud config set compute/region us-west1macOS%$ gcloud config set compute/zone us-west1-b Google Cloud Platform の無料階層 参照してください。 クラスタ作成 無料枠を利用するべく f1-micro で 30GB 設定 でも作成時は 3 ノード必須 作成完了後、リサイズで 1 ノードに 12345678910macOS%$ gcloud container clusters create hubot-cluster-free \\ --machine-type f1-micro \\ --disk-size=30 \\ --num-nodes=3Creating cluster hubot-cluster-free...done.Created [https://container.googleapis.com/v1/projects/hubot-167007/zones/us-west1-b/clusters/hubot-cluster-free].kubeconfig entry generated for hubot-cluster-free.NAME ZONE MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUShubot-cluster-free us-west1-b 1.5.7 35.xxx.xxx.xxx f1-micro 1.5.7 3 RUNNING コンソールを見ると作成中であることが確認できます。 以下コマンドで確認可 12345macOS% $ kubectl get nodesNAME STATUS AGE VERSIONgke-hubot-cluster-free-default-pool-a3b110d2-9k6s Ready 59s v1.5.7gke-hubot-cluster-free-default-pool-a3b110d2-lqxg Ready 1m v1.5.7gke-hubot-cluster-free-default-pool-a3b110d2-xqs8 Ready 1m v1.5.7 1 ノードにリサイズ 12345678macOS%$ gcloud container clusters resize hubot-cluster-free --size=1Pool [default-pool] for [hubot-cluster-free] will be resized to 1.Do you want to continue (Y/n)? yResizing hubot-cluster-free...done.Updated [https://container.googleapis.com/v1/projects/hubot-167007/zones/us-west1-b/clusters/hubot-cluster-free]. リサイズできるなら初めから 1 ノードで作らせて欲しい (&gt;_&lt;) コンソール上だとやっぱりダメ (T_T) 認証情報 取得 コンテナクラスタの認証情報を取得し、kubectlを利用してコンテナ クラスタ上にコンテナを作成できるようになります。 1234macOS%$ gcloud container clusters get-credentials hubot-cluster-freeFetching cluster endpoint and auth data.kubeconfig entry generated for hubot-cluster-free. コンテナクラスタ情報表示 1macOS%$ gcloud container clusters describe hubot-cluster-free ローカルの Docker 起動[https://github.com/kenzo0107/hubot-slack-on-docker:embed:cite] 123macOS%$ git clone https://github.com/kenzo0107/hubot-slack-on-dockermacOS%$ cd hubot-slack-on-dockermacOS%$ docker-compose up -d 1234macOS%$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES12f77feb09b4 hubotslackondocker_hubot &quot;/bin/sh -c 'bash ...&quot; 24 minutes ago Up 24 minutes 6379/tcp, 0.0.0.0:8080-&gt;8080/tcp hubotslackondocker_hubot_1 Hubot 動作確認Slack上に Hubot が登場していて hello と呼びかけると Hi と返してくれたら成功です。 CONTAINER ID から イメージをcommit12345macOS%$ docker commit 12f77feb09b4 gcr.io/hubot-167007/hubot:latestmacOS%$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEgcr.io/hubot-167007/hubot latest 2f7336b3a3ce 3 seconds ago 484 MB gke registory に push参考: Container Registry への push 1234567891011121314151617181920212223242526macOS%$ gcloud docker -- push gcr.io/hubot-167007/hubot:latestThe push refers to a repository [gcr.io/hubot-167007/hubot]0569b419082b: Pusheda7637cfcdfba: Pushed9f0bdbb7b1fa: Pushedf1d85eafc75a: Pushedc2c2b58591f2: Pushed51c94eacef50: Pushed69e7fcf7ba41: Pushed293d09ca6a9d: Pushed247e72dfcaf5: Pushed8c2bc9bf1f19: Pushed40907ce0d959: Pushedbfba578a7fbe: Pushed561cbcaac156: Pushed293a1e72e88b: Pushedae09eb3da3dc: Pushedc06c14d7f919: Pushede14577d2cac5: Layer already existse8829d5bbd2c: Layer already exists674ce3c5d814: Layer already exists308b39a73046: Layer already exists638903ee8579: Layer already existslatest: digest: sha256:0c3b29d18b64c1f8ecc1a1bf67462c84d5915a4a708fe87df714d09198eb5fa1 size: 4704 latest が被ると過去のイメージのタグが奪われます。容量の無駄になるので削除しましょう。 Deployments 作成12345678910macOS%$ kubectl run pod-hubot \\ --image=gcr.io/hubot-167007/hubot:latest \\ --env=&quot;HUBOT_SLACK_TOKEN=xoxb-xxxxxxxxxxxx-xxxxxxxxxxxxxxxxxxxxxxxx&quot; \\ --env=&quot;HUBOT_SLACK_TEAM=xxxxxx.slack.com&quot; \\ --env=&quot;HUBOT_SLACK_BOTNAME=hubot&quot; \\ --env=&quot;HUBOT_JIRA_URL=https://&lt;jira_server_domain_or_ip&gt;&quot; \\ --port=8080 \\ --restart='Always'deployment &quot;pod-hubot&quot; created deployments 状態確認 123macOS%$ kubectl get deploymentsNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEpod-hubot 1 1 1 0 10s Pod 状態確認 1234macOS%$ kubectl get podsNAME READY STATUS RESTARTS AGEpod-hubot-1713414922-b2dkq 0/1 ImagePullBackOff 0 23s Pod にログイン 1$ kubectl exec -it pod-hubot-1713414922-b2dkq /bin/bash service の状態確認 12345macOS%$ kubectl get serviceNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes 10.23.240.1 &lt;none&gt; 443/TCP 22m EXTERNAL-IP: &lt;none&gt; … 外部へ開いているIPがない。という状態Private IP は付与されたが Public IP がない、外部のネットワークからアクセスできない状態です。 コンテナ公開 Service にロードバランサ付与し公開 ※ ロードバランサを追加すると課金の桁が跳ね上がります。。（2000円/月くらい。念の為、設定した予算アラートでわかりました。） 12macOS%$ kubectl expose deployment pod-hubot --type=&quot;LoadBalancer&quot;service &quot;pod-hubot&quot; exposed Service 確認 EXTERNAL-IP: &lt;pending&gt; となっており、作成途中であることがわかります。 12345macOS%$ kubectl get serviceNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes 10.23.240.1 &lt;none&gt; 443/TCP 25mpod-hubot 10.23.244.214 &lt;pending&gt; 8080:30453/TCP 8s 再度 Service 確認 無事付与されているのがわかりました。 1234macOS%$ kubectl get serviceNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes 10.23.240.1 &lt;none&gt; 443/TCP 27mpod-hubot 10.23.244.214 104.xxx.x.xxx 8080:30453/TCP 1m テスト1234567891011121314151617181920macOS%$ curl \\-X POST \\-H &quot;Content-Type: application/json&quot; \\-d \\'{ &quot;webhookEvent&quot;:&quot;jira:issue_updated&quot;, &quot;comment&quot;:{ &quot;author&quot;:{ &quot;name&quot;:&quot;himuko&quot; }, &quot;body&quot;:&quot;[~kenzo.tanaka] 東京03 秋山 ケンコバ 劇団ひとり&quot; }, &quot;issue&quot;: { &quot;key&quot;:&quot;key&quot;, &quot;fields&quot;:{ &quot;summary&quot;:&quot;summary&quot; } }}' \\http://104.xxx.x.xxx:8080/hubot/jira-comment-dm 後始末掃除しときたい場合に以下実行してください。 service 削除 12macOS%$ kubectl delete service pod-hubotservice &quot;pod-hubot&quot; deleted pod 削除 12macOS%$ kubectl delete pod pod-hubot-729436916-htw3rservice &quot;pod-hubot&quot; deleted deployments 削除 1macOS%$ kubectl delete deployments pod-hubot container clusters 削除container cluster を削除すれば紐付く deployments, service, pod も削除されます。 1macOS%$ gcloud container clusters delete hubot-cluster-free 以上です。 総評GKEは概念が多く、一概に deployment, pod, service, kubernetes 等覚えることが多いですが動かしつつ学ぶのは楽しいです。 ほぼ手元の Mac で設定できました！手元で済むから macOS%$ は不要だった。。 今回作成した service だと外部に 8080 ポート全開です。 次回はアクセス元を制限したポートアクセスやコンテナのアップデートについてまとめます。 [http://kenzo0107.hatenablog.com/entry/2017/05/16/222815:embed:cite]","link":"/2017/05/09/2017-05-10-gke-kubernetes-hubot-cli/"},{"title":"無料枠で運用！ GKE + Kubernetes で Hubot 〜独自ネットワーク作成、設定ファイルから起動編〜","text":"前回手元のMacからコンテナクラスタ → Deployment → LB 作成する手順をまとめました。 無料枠で運用！ GKE + Kubernetes で Hubot 〜CLIから実行編〜 - 長生村本郷Engineers'Blog概要 無料枠を使って Slack 連携する Hubot を GKE で構築します。 おまけで JIRA 連携も Google Cloud SDK のインストール方法と初期化 Mac OS X 用クイックスタート を参照して SDK をダウンロードします。 Mac OS X (x8… 但し、8080ポートがフルオープンとなってしまい、誰でもアクセスが可能であるという、セキュリティ的に非常によろしくない状態でした。 その為、今回は以下実施します。 独自ネットワーク(ファイアウォール)作成 独自ネットワーク上にクラスタ作成 設定ファイルでコンテナ起動・更新 前回の独自ネットワーク設定していないクラスタは削除して問題ないです。お任せします m(_ _)m 前回同様の Git Repository 用意12$ git clone https://github.com/kenzo0107/hubot-slack-on-docker$ cd hubot-slack-on-docker Network 作成 hubot-network というネットワークを作成します。 1macOS%$ gcloud compute networks create hubot-network ファイアウォール作成 作成したネットワークに特定 IP からのみ 8080 ポートアクセス許可 1macOS%$ gcloud compute firewall-rules create hubot-rule --network hubot-network --allow tcp:8080 --source-ranges xxx.xxx.xxx.xxx,yyy.yyy.yyy.yyy.yyy Container Clusters 作成 作成したネットワーク指定しクラスタ作成 123456macOS%$ gcloud container clusters create hubot-cluster-free \\ --machine-type f1-micro \\ --disk-size=30 \\ --num-nodes=3 \\ --network=hubot-network \\ --cluster-ipv4-cidr=10.0.0.0/14 cluster-ipv4-cidr オプション必須！指定しクラスタ内の Pod のIPアドレスの範囲指定しています。※サブネットマスク(10.0.0.0/14 の “/14” 部分)指定は9〜19で指定する必要があります。 例) –cluster-ipv4-cidr=10.0.0.0/8 指定した場合のエラー 1ERROR: (gcloud.container.clusters.create) ResponseError: code=400, message=cluster.cluster_ipv4_cidr CIDR block size must be no bigger than /9 and no smaller than /19, found /8. ノード数を 1 に変更1macOS%$ gcloud container clusters resize hubot-cluster-free --size=1 Deployment 作成1macOS%$ kubectl create -f gke-deployment.yml Deployment, Replicaset, Pod 一覧表示 ラベル付けした app: hubot を条件指定 1macOS%$ kubectl get deployments,replicasets,pods --selector app=hubot フォーマットを yaml 形式で出力1macOS%$ kubectl get deployment deployment-hubot -o yaml サービス公開する為、LoadBalancer 付加1macOS%$ kubectl create -f gke-lb.yml サービス一覧表示1234macOS%$ kubectl get svcNAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes 10.3.240.1 &lt;none&gt; 443/TCP 20mloadbalancer 10.3.241.129 zz.zzz.zzz.zzz 8080:31628/TCP 4m ※EXTERNAL-IP : zz.zzz.zzz.zzz はグローバルIP いざ、テスト !1234567891011121314151617181920macOS%$ curl \\-X POST \\-H &quot;Content-Type: application/json&quot; \\-d \\'{ &quot;webhookEvent&quot;:&quot;jira:issue_updated&quot;, &quot;comment&quot;:{ &quot;author&quot;:{ &quot;name&quot;:&quot;himuko&quot; }, &quot;body&quot;:&quot;[~kenzo.tanaka] 東京03 秋山 ケンコバ 劇団ひとり&quot; }, &quot;issue&quot;: { &quot;key&quot;:&quot;key&quot;, &quot;fields&quot;:{ &quot;summary&quot;:&quot;summary&quot; } }}' \\http://zz.zzz.zzz.zzz:8080/hubot/jira-comment-dm できた！[f:id:kenzo0107:20170516220548p:plain] 更新（ローリングアップデート）ReplicationController を利用することで無停止で更新します。 実際に以下の様にして更新しているのが確認できます。 既存の Running 中のコンテナの個数分、更新したイメージからビルドしたコンテナを起動 更新版コンテナがRunning状態になったら既存コンテナを削除 123456789101112// ローカルで更新した Docker Container を コミットmacOS%$ docker commit 12f77feb09b4 gcr.io/hubot-167007/hubot:latest// Google Container Registory にプッシュmacOS%$ gcloud docker -- push gcr.io/hubot-167007/hubot:latest// Pod 表示macOS%$ kubectl get podsNAME READY STATUS RESTARTS AGEdeployment-hubot-cfe7528ee0b5059b14a30b942597e5ef-z8nws 1/1 Running 1 1d// push したImageを元にローリングアップデートmacOS%$ kubectl rolling-update deployment-hubot-cfe7528ee0b5059b14a30b942597e5ef-z8nws --image=gcr.io/hubot-167007/hubot:latest 後片付け Deployment 削除 1macOS%$ kubectl delete -f gke-deployment.yml LoadBalancer 削除 1macOS%$ kubectl delete -f gke-lb.yml 総評ネットワークのファイアウォール設定してコンテナ起動したが動かなかった所、かなり詰まりました (; _)Stackoverflow にたまたま同様のイシューをあげている方がおり参考にさせて頂きました。助かった汗 これから Nginx + Rails 等、よくありそうなケースで GKE + Kubernetes を試して運用してみたいと思います。まとまったらまた追記します！ 参考Unable to launch a GKE (Google Container Engine) cluster with a custom network","link":"/2017/05/15/2017-05-16-gke-kubernetes-hubot/"},{"title":"Apache 2.2.15 → 2.4.25   PHP 5.6 → 7 へアップデート on CentOS 6.9","text":"概要PHP5 利用していますか？ PHP5.6 のセキュリティサポート期限は 31 Dec 2018 迄※ Supported Versions 参考 Apache/PHP アップデート、腰が重かったのですが個人契約サーバなら誰にも迷惑かけないしいいか♪ ということで放置気味にされた Apache2.2.15/PHP5.6 の個人のサーバをアップデートすべく実施した内容をまとめました。 三行まとめ SoftwareCollection を利用し現存Apache/PHPを残したまま、アップデート版を共存させ切り替え。のち古い Apache/PHP 削除 必要モジュール (MySQLi, PHPRedis)インストール PHP 7 で廃止された PHP5.6 機能やシンタックスを修正 SoftwareCollection とは？ 公式サイト によると以下の様に説明されています。 英語 Software Collections give you the power to build, install, and use multiple versions of software on the same system, without affecting system-wide installed packages. 日本語 ソフトウェアコレクションは、システム全体でインストールされたパッケージに影響を与えることなく、同じシステム上に複数のバージョンのソフトウェアを構築、インストール、使用する能力を提供します。 同じシステム上に複数バージョンのソフトウェアをインストールできる様になる、ということです。 SoftwareCollection インストール1$ sudo yum install centos-release-scl httpd24 関連のモジュールインストール1234$ sudo yum-config-manager --enable rhel-server-rhscl-6-rpms$ sudo yum install httpd24-httpd httpd24-httpd-devel httpd24-mod_proxy_html httpd24-mod_session httpd24-mod_ssl$ sudo scl enable httpd24 bash$ sudo service httpd graceful 123$ httpd -vServer version: Apache/2.4.25 (Red Hat)Server built: Apr 12 2017 06:35:50 RHSCL リポジトリ利用可設定1$ sudo yum-config-manager --enable rhel-server-rhscl-7-rpms php7 関連モジュールをインストール12345678# yum install -y scl-utils# yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm# yum install -y http://rpms.remirepo.net/enterprise/remi-release-7.rpm# yum install -y php70# yum install -y php70-php-mysqlnd# yum install -y php70-php-curl# yum install -y php70-php-simplexml# yum install -y php70-php-devel php70-php-gd php70-php-json php70-php-mcrypt php70-php-mbstring php70-php-opcache php70-php-pear php70-php-pecl-apcu php70-php-pecl-geoip php70-php-pecl-imagick php70-php-pecl-json-post php70-php-pecl-memcache php70-php-pecl-xmldiff php70-php-pecl-zip php70-php-pspell php70-php-soap php70-php-tidy php70-php-xml php70-php-xmlrpc mysqli インストール1# yum --enablerepo=remi-php70 install php-mysqli PHP7 用 phpredis インストール123456789# cd /usr/local/src# git clone https://github.com/phpredis/phpredis.git# cd phpredis# git checkout php7# phpize# ./configure# make# make install# echo 'extension=redis.so' &gt; /etc/opt/rh/rh-php70/php.d/redis.ini php-fpm 再起動1# /etc/init.d/php70-php-fpm restart httpd 再起動1# /etc/init.d/httpd24-httpd restart ここまでで PHP7 で動作する環境が整っているかと思います。エラーログを見ながら修正に当たってください。 PHP 7 で廃止された構文を修正PHP Parse error: syntax error, unexpected ‘new’ (T_NEW) &amp;= new &lt;クラス名&gt; の指定が不可となり、 = new &lt;クラス名&gt; にする必要があります。 123&amp;= new Class↓= new Class PHP Fatal error: Cannot use ‘String’ as class name as it is reserved PHP7 では class String, Int と型名の Class を作成できなくなりました。 自分は以下の様に修正しました。※ 適宜プロジェクトのコーディングルールに則ってご変更ください。 123class String {↓class Stringer { 123class Int {↓class Intger { プロジェクトによってはもっと色々出てくると思いますので適宜修正ください。 総評放置されがちになるミドルウェアのアップデートは小まめにやっておきたいですね。脆弱性の定期的な棚卸しせねば 業務でアップデートするのであればアップデートする環境を別途用意してアップデートする、そこでミドルウェア、アプリケーションのコードレベルでのアップデート手順をまとめ本番環境で実施。 機能(url)毎に正しく動いたものだけプロキシで PHP7 へ流すというのもアリかなと思います。 以上です。 参考 PHP 5.6.x から PHP 7.0.x への移行 PHP5.6からPHP７にアップデートする上で気をつけるコト","link":"/2017/06/12/2017-06-13-update-apache-php/"},{"title":"Install PHP7, PECL, PEAR on MacOS","text":"備忘録です。忘れない為の自分への一筆。 PHP 7 インストール1234$ brew update$ brew install homebrew/php/php70$ echo 'export PATH=&quot;$(brew --prefix homebrew/php/php70)/bin:$PATH&quot;' &gt;&gt; ~/.bashrc$ source ~/.bashrc 12345$ php -vPHP 7.0.19 (cli) (built: May 21 2017 11:56:11) ( NTS )Copyright (c) 1997-2017 The PHP GroupZend Engine v3.0.0, Copyright (c) 1998-2017 Zend Technologies PHP 7 で Pecl, Pear インストール1$ curl -O http://pear.php.net/go-pear.phar 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110$ sudo php -d detect_unicode=0 go-pear.pharBelow is a suggested file layout for your new PEAR installation. Tochange individual locations, type the number in front of thedirectory. Type 'all' to change all of them or simply press Enter toaccept these locations. 1. Installation base ($prefix) : /usr/local/Cellar/php70/7.0.19_11 2. Temporary directory for processing : /tmp/pear/install 3. Temporary directory for downloads : /tmp/pear/install 4. Binaries directory : /usr/local/Cellar/php70/7.0.19_11/bin 5. PHP code directory ($php_dir) : /usr/local/Cellar/php70/7.0.19_11/share/pear 6. Documentation directory : /usr/local/Cellar/php70/7.0.19_11/docs 7. Data directory : /usr/local/Cellar/php70/7.0.19_11/data 8. User-modifiable configuration files directory : /usr/local/Cellar/php70/7.0.19_11/cfg 9. Public Web Files directory : /usr/local/Cellar/php70/7.0.19_11/www10. System manual pages directory : /usr/local/Cellar/php70/7.0.19_11/man11. Tests directory : /usr/local/Cellar/php70/7.0.19_11/tests12. Name of configuration file : /usr/local/etc/php/7.0/pear.conf// インストール先指定1-12, 'all' or Enter to continue: (「1」と入力しEnter)(Use $prefix as a shortcut for '/usr/local/Cellar/php70/7.0.19_11', etc.)Installation base ($prefix) [/usr/local/Cellar/php70/7.0.19_11] : (「/usr/local/pear」と入力しEnter)Below is a suggested file layout for your new PEAR installation. Tochange individual locations, type the number in front of thedirectory. Type 'all' to change all of them or simply press Enter toaccept these locations. 1. Installation base ($prefix) : /usr/local/pear 2. Temporary directory for processing : /tmp/pear/install 3. Temporary directory for downloads : /tmp/pear/install 4. Binaries directory : /usr/local/pear/bin 5. PHP code directory ($php_dir) : /usr/local/pear/share/pear 6. Documentation directory : /usr/local/pear/docs 7. Data directory : /usr/local/pear/data 8. User-modifiable configuration files directory : /usr/local/pear/cfg 9. Public Web Files directory : /usr/local/pear/www10. System manual pages directory : /usr/local/pear/man11. Tests directory : /usr/local/pear/tests12. Name of configuration file : /usr/local/etc/php/7.0/pear.conf// バイナリディレクトリ指定1-12, 'all' or Enter to continue: (「4」と入力しEnter)(Use $prefix as a shortcut for '/usr/local/pear', etc.)Binaries directory [$prefix/bin] : /usr/local/binBelow is a suggested file layout for your new PEAR installation. Tochange individual locations, type the number in front of thedirectory. Type 'all' to change all of them or simply press Enter toaccept these locations. 1. Installation base ($prefix) : /usr/local/pear 2. Temporary directory for processing : /tmp/pear/install 3. Temporary directory for downloads : /tmp/pear/install 4. Binaries directory : /usr/local/bin 5. PHP code directory ($php_dir) : /usr/local/pear/share/pear 6. Documentation directory : /usr/local/pear/docs 7. Data directory : /usr/local/pear/data 8. User-modifiable configuration files directory : /usr/local/pear/cfg 9. Public Web Files directory : /usr/local/pear/www10. System manual pages directory : /usr/local/pear/man11. Tests directory : /usr/local/pear/tests12. Name of configuration file : /usr/local/etc/php/7.0/pear.conf// 以上で基本設定が済んだのでインストールを開始する1-12, 'all' or Enter to continue: (何も入力せずEnter)// インストールが開始されます。Beginning install...Configuration written to /usr/local/etc/php/7.0/pear.conf...Initialized registry...Preparing to install...installing phar:///Users/kenzo.tanaka/azure/go-pear.phar/PEAR/go-pear-tarballs/Archive_Tar-1.4.2.tar...installing phar:///Users/kenzo.tanaka/azure/go-pear.phar/PEAR/go-pear-tarballs/Console_Getopt-1.4.1.tar...installing phar:///Users/kenzo.tanaka/azure/go-pear.phar/PEAR/go-pear-tarballs/PEAR-1.10.4.tar...installing phar:///Users/kenzo.tanaka/azure/go-pear.phar/PEAR/go-pear-tarballs/Structures_Graph-1.1.1.tar...installing phar:///Users/kenzo.tanaka/azure/go-pear.phar/PEAR/go-pear-tarballs/XML_Util-1.4.2.tar...install ok: channel://pear.php.net/Archive_Tar-1.4.2install ok: channel://pear.php.net/Console_Getopt-1.4.1install ok: channel://pear.php.net/Structures_Graph-1.1.1install ok: channel://pear.php.net/XML_Util-1.4.2install ok: channel://pear.php.net/PEAR-1.10.4PEAR: Optional feature webinstaller available (PEAR's web-based installer)PEAR: Optional feature gtkinstaller available (PEAR's PHP-GTK-based installer)PEAR: Optional feature gtk2installer available (PEAR's PHP-GTK2-based installer)PEAR: To install optional features use &quot;pear install pear/PEAR#featurename&quot;The 'pear' command is now at your service at /usr/local/bin/pear** The 'pear' command is not currently in your PATH, so you need to** use '/usr/local/bin/pear' until you have added** '/usr/local/bin' to your PATH environment variable.Run it without parameters to see the available actions, try 'pear list'to see what packages are installed, or 'pear help' for help.For more information about PEAR, see: http://pear.php.net/faq.php http://pear.php.net/manual/Thanks for using go-pear! PECL インストール確認12345678$ which pecl/usr/local/bin/pecl$ pecl versionPEAR Version: 1.10.4PHP Version: 7.0.19Zend Engine Version: 3.0.0Running on: Darwin pc-12-332.local 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar 3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64 PEAR インストール確認1234567$ which pear/usr/local/bin/pearPEAR Version: 1.10.4PHP Version: 7.0.19Zend Engine Version: 3.0.0Running on: Darwin pc-12-332.local 16.5.0 Darwin Kernel Version 16.5.0: Fri Mar 3 16:52:33 PST 2017; root:xnu-3789.51.2~3/RELEASE_X86_64 x86_64 以上です。","link":"/2017/06/12/2017-06-04-install-php7-pecl-pear-on-macos/"},{"title":"Flask Python3 で 戸田市 tocoちゃんバスあと何分？ Webアプリ作成♪","text":"Flask(フラスク) とはFlask Official Site を参照すると冒頭に以下の文章があります。 1Flask is a microframework for Python based on Werkzeug, Jinja 2 and good intentions Flask は Python の小規模なフレームワークで、 Werkzug や Jinja 2 をベースとしています。 Made by Armin Ronacher (Australian) Bottle（ボトル）に対する言葉遊びで命名。（Flask: フラスコ） 2010年のエイプリルフールのジョークが始まり 2017年大注目のPython！WEBフレームワーク3つを徹底比較 を見ると次第に人気が高まっている（給与も良さそう？） 何がいいの？最小限の構成で簡単な Web アプリケーションが作成できることです。django, Rails でも簡単に出来なくもないですが、さらに手順は短く容易です。 チュートリアル123456from flask import Flaskapp = Flask(__name__)@app.route(\"/\")def hello(): return \"Hello World!\" Flaskを利用する判断基準 大規模なプロジェクトでない。（ファイル構成は自分 or チームで決定する必要がある為） DB を使わない。使うにしても複雑なDB設計でない。 利用するライブラリが Python 製でそのgatewayとして利用したい。 手始めに！初心者向け記事は多数あるので割愛し学習がてら Web アプリケーション作ったのでそちらをどうぞ♪ 実際動くものとそのソースを見た方がイメージ湧くと思います。 tocoちゃんバス あと何分？ 戸田市のローカルバス toco ちゃんバスの停留所を指定しあと何分で来るかをカウントダウンするアプリです。自分が使うので作ったら割と広まってきて嬉しい限り♪ 12※ちなみに戸田市役所に確認し非公式ではありますが公開許可をいただいております。※戸田市役所ご担当者様より「可能であれば最新の迂回情報なども載せていただけたら〜」という要望も頂きました♪ ソースはこちら♪kenzo0107/toda-tocochan-busContribute to kenzo0107/toda-tocochan-bus development by creating an account on GitHub. ソースを参照頂けるとやってみたことがわかりやすいと思います。 やってみたこと docker で flask ローカル開発環境作成 config ファイルからデータ呼び出し (config.py) session 機能 Bootstrap 適用 superagent.js で非同期通信 flickity.js でフリッカブルに 本番動作環境 さくらVPS CentOS 6 httpd2.4 + wsgi + virtualenv + python3 さくらVPSには CakePHP や SpringBoot のプロジェクトが乗っかっていたりとやりたい放題の環境として所持しています。 開発期間ほぼほぼ 1週間。仕事の昼休みと日曜大工で 10時間足らずで公開出来ました。どちらかというと js 側の学習コストが掛かった感じ。 総評今回作成したユーザ情報を管理しない、DBを持たないアプリにはうってつけでした。構成が複雑になりすぎず丁度良かったです。 やはり大きな規模のプロジェクトには django が適しています。 個人的に戸田市役所に電話してWebアプリ公開の許可を頂く、という承認申請が出来、地域貢献できる喜びが非常に大きかったです。 今後、バージョンアップしたいと思います。 Flask 利用に際して参考になれば何よりです。 以上です。","link":"/2017/07/06/2017-07-07-flask/"},{"title":"Install latest Nginx on Ubuntu","text":"Just a memo. Install Nginx123456ubuntu%$ sudo suubuntu%$ curl http://nginx.org/keys/nginx_signing.key | sudo apt-key add -ubuntu%$ sh -c &quot;echo 'deb http://nginx.org/packages/ubuntu/ trusty nginx' &gt;&gt; /etc/apt/sources.list&quot;ubuntu%$ sh -c &quot;echo 'deb-src http://nginx.org/packages/ubuntu/ trusty nginx' &gt;&gt; /etc/apt/sources.list&quot;ubuntu%$ apt-get updateubuntu%$ apt-get install -y nginx Install sysv-rc-confSysV is a runlevel configuration tool. 1ubuntu%$ apt-get install -y sysv-rc-conf Configure runlevel of nginx. The command chkconfig is no longer available in Ubuntu. The equivalent command to chkconfig is update-rc.d. This command nearly supports all the new versions of ubuntu. chkconfig —&gt; sysv-rc-conf 1ubuntu%$ sysv-rc-conf nginx on Show runlevel of nginx12ubuntu%$ sysv-rc-conf --list nginxnginx 0:off 1:off 2:on 3:on 4:on 5:on 6:off","link":"/2017/07/09/2017-07-10-install-latest-nginx-on-ubuntu/"},{"title":"Mackerel で Docker の起動状態確認","text":"概要Docker コンテナがいつの間にか Exit していた！なんてことを防ぐ為の Mackerel Agent の設定です。 mackerel-plugin-docker-state インストール1234567$ sudo mkdir -p /etc/mackerel-agent/conf.d$ sudo curl https://raw.githubusercontent.com/ABCanG/mackerel-plugin-docker-state/master/mackerel-plugin-docker-state -o /etc/mackerel-agent/conf.d/mackerel-plugin-docker-state$ sudo chmod +x /etc/mackerel-agent/conf.d/mackerel-plugin-docker-state$ sudo cat &lt;&lt;'EOF'&gt;/etc/mackerel-agent/conf.d/docker-state.conf[plugin.metrics.docker-state]command = \"/etc/mackerel-agent/conf.d/mackerel-plugin-docker-state\"EOF mackerel-agent.conf に include 設定追加 /etc/mackerel-agent/mackerel-agent.conf 12345678910111213pidfile = \"/var/run/mackerel-agent.pid\"root = \"/var/lib/mackerel-agent\"verbose = falseapikey = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"diagnostic = trueroles = [\"xxxxxxxx:xxx\"]# include conf.d/*.confinclude = \"/etc/mackerel-agent/conf.d/*.conf\"...... Mackrel Agent 再起動1$ sudo service mackerel-agent restart グラフ確認しばらくするとグラフが表示されます。※0 or 1 のみのグラフなので積み重ねグラフの方が見やすかったです ※上記グラフではコンテナ2つが起動しています。 新規監視ルールを作成running で検索すると出てきます。 3分間の平均が1 より低くなったらコンテナが停止(Exit)と見なし通知する様にしました。 総評今回たまたま Mackerel の入ったサービスを触る機会を頂きました。 Mackerel の様なマネージドサービスを利用するメリットは監視サーバを監視しないで良い、という省運用コストだなぁと改めて実感。","link":"/2017/07/13/2017-07-14-monitor-state-of-docker-by-mackerel/"},{"title":"I used Phinx, DB migration Tool on Docker!","text":"OverviewThis is Sandbox for DB Migration Tool Phinx. Preparation12$ git clone https://github.com/kenzo0107/phinx-mysql$ cd phinx-mysql Create and Run Containers of Phinx, DB (MySQL).1$ make build 123456$ docker-compose ps Name Command State Ports-------------------------------------------------------------------------phinxmysql_db-migrate_1 phinx --help Exit 0phinxmysql_db_1 docker-entrypoint.sh mysqld Up 3306/tcp The Container db-migrate is used as for one-off container, so its state is Exit 0. Initialize Phinx ProjectPhinx creates a default file called phinx.yml. 1$ make init In default setting, phinx select development environment. 1. Create TableCreate phinx definition file123456$ make create DB=hogehoge CLASS=CreateTableUsers$ make create DB=mogemoge CLASS=CreateTableMembers......created db/migrations/hogehoge/20170724065658_create_table_users.phpcreated db/migrations/mogemoge/20170724065738_create_table_members.php Edit phinx definition file db/migrations/hogehoge/20170724065658_create_table_users.php Writing Migrations 1234567891011121314151617181920212223242526272829303132333435&lt;?phpuse Phinx\\Migration\\AbstractMigration;use Phinx\\Db\\Adapter\\MysqlAdapter;class CreateTableUsers extends AbstractMigration{ public function up() { // Automatically generated id is excluded, and primary key is set as user_id $t = $this-&gt;table('users', ['id' =&gt; 'user_id']); $t-&gt;addColumn('last_name', 'string', ['limit' =&gt; 10, 'comment' =&gt; '姓']) -&gt;addColumn('first_name', 'string', ['limit' =&gt; 10, 'comment' =&gt; '名']) -&gt;addColumn('last_kana_name', 'string', ['null' =&gt; true, 'limit' =&gt; 10, 'comment' =&gt; '姓（カナ）']) -&gt;addColumn('first_kana_name', 'string', ['null' =&gt; true, 'limit' =&gt; 10, 'comment' =&gt; '名（カナ）']) -&gt;addColumn('username', 'string', ['limit' =&gt; 20, 'comment' =&gt; 'ユーザ名']) -&gt;addColumn('password', 'string', ['limit' =&gt; 40, 'comment' =&gt; 'パスワード']) -&gt;addColumn('email', 'string', ['limit' =&gt; 100, 'comment' =&gt; 'Email']) -&gt;addColumn('postcode', 'string', ['limit' =&gt; 10, 'comment' =&gt; '郵便番号']) -&gt;addColumn('birthday', 'date', ['comment' =&gt; '誕生日']) -&gt;addColumn('gender', 'integer', ['limit' =&gt; MysqlAdapter::INT_TINY, 'comment' =&gt; '性別(1:男 2:女 3:その他)']) -&gt;addColumn('card_number', 'string', ['null' =&gt; true, 'limit' =&gt; 20, 'comment' =&gt;'クレジットカードNo']) -&gt;addColumn('description', 'text', ['null' =&gt; true, 'limit' =&gt; MysqlAdapter::TEXT_LONG, 'comment' =&gt;'説明']) -&gt;addColumn('created', 'timestamp', ['default' =&gt; 'CURRENT_TIMESTAMP']) -&gt;addColumn('updated', 'datetime', ['null' =&gt; true]) -&gt;addIndex(['username', 'email'], ['unique' =&gt; true]) -&gt;create(); } public function down() { $this-&gt;dropTable('users'); }} db/migrations/mogemoge/20170724065738_create_table_members.php 123456789101112131415161718192021&lt;?phpuse Phinx\\Migration\\AbstractMigration;class CreateTableMembers extends AbstractMigration{ public function up() { $t = $this-&gt;table('members'); $t-&gt;addColumn('member_code', 'string', ['limit' =&gt; 20, 'comment' =&gt; '会員コード']) -&gt;addColumn('created', 'timestamp', ['default' =&gt; 'CURRENT_TIMESTAMP']) -&gt;addColumn('updated', 'datetime', ['null' =&gt; true]) -&gt;addIndex(['member_code'], ['unique' =&gt; true]) -&gt;create(); } public function down() { $this-&gt;dropTable('members'); }} 2. Add ColumnCreate phinx definition file1234$ make create CLASS=AddTableUsersColumnsCity......created db/migrations/hogehoge/20170724065838_add_table_users_columns_city.php Edit phinx definition file db/migrations/hogehoge/20170724065838_add_table_users_columns_city.php Add the column city after the column email. 1234567891011121314151617181920&lt;?phpuse Phinx\\Migration\\AbstractMigration;class AddTableUsersColumnsCity extends AbstractMigration{ public function up() { $t = $this-&gt;table('users'); $t-&gt;addColumn('city', 'string', ['limit' =&gt; 10, 'comment' =&gt; '都市', 'after' =&gt; 'postcode']) -&gt;update(); } public function down() { $t = $this-&gt;table('users'); $t-&gt;removeColumn('city') -&gt;save(); }} Migration1$ make migrate Result 12345678910111213141516171819202122232425262728293031323334mysql&gt; use hogehogemysql&gt; show full columns from users;+-----------------+--------------+-----------------+------+-----+-------------------+----------------+---------------------------------+---------------------------------+| Field | Type | Collation | Null | Key | Default | Extra| Privileges | Comment |+-----------------+--------------+-----------------+------+-----+-------------------+----------------+---------------------------------+---------------------------------+| user_id | int(11) | NULL | NO | PRI | NULL | auto_increment| select,insert,update,references | || last_name | varchar(10) | utf8_general_ci | NO | | NULL || select,insert,update,references | 姓 || first_name | varchar(10) | utf8_general_ci | NO | | NULL || select,insert,update,references | 名 || last_kana_name | varchar(10) | utf8_general_ci | YES | | NULL || select,insert,update,references | 姓（カナ） || first_kana_name | varchar(10) | utf8_general_ci | YES | | NULL || select,insert,update,references | 名（カナ） || username | varchar(20) | utf8_general_ci | NO | MUL | NULL || select,insert,update,references | ユーザ名 || password | varchar(40) | utf8_general_ci | NO | | NULL || select,insert,update,references | パスワード || email | varchar(100) | utf8_general_ci | NO | | NULL || select,insert,update,references | Email || city | varchar(255) | utf8_general_ci | NO | | NULL || select,insert,update,references | || postcode | varchar(10) | utf8_general_ci | NO | | NULL || select,insert,update,references | 郵便番号 || birthday | date | NULL | NO | | NULL || select,insert,update,references | 誕生日 || gender | tinyint(4) | NULL | NO | | NULL || select,insert,update,references | 性別(1:男 2:女 3:その他) || card_number | varchar(20) | utf8_general_ci | YES | | NULL || select,insert,update,references | クレジットカードNo || description | longtext | utf8_general_ci | YES | | NULL || select,insert,update,references | 説明 || created | timestamp | NULL | NO | | CURRENT_TIMESTAMP || select,insert,update,references | || updated | datetime | NULL | YES | | NULL || select,insert,update,references | |+-----------------+--------------+-----------------+------+-----+-------------------+----------------+---------------------------------+---------------------------------+mysql&gt; use mogemogemysql&gt; show full columns from members;+-------------+-------------+-----------------+------+-----+-------------------+----------------+---------------------------------+-----------------+| Field | Type | Collation | Null | Key | Default | Extra | Privileges | Comment |+-------------+-------------+-----------------+------+-----+-------------------+----------------+---------------------------------+-----------------+| id | int(11) | NULL | NO | PRI | NULL | auto_increment | select,insert,update,references | || member_code | varchar(20) | utf8_general_ci | NO | UNI | NULL | | select,insert,update,references | 会員コード || created | timestamp | NULL | NO | | CURRENT_TIMESTAMP | | select,insert,update,references | || updated | datetime | NULL | YES | | NULL | | select,insert,update,references | |+-------------+-------------+-----------------+------+-----+-------------------+----------------+---------------------------------+-----------------+ Rollback1$ make rollback 3. Create sample seeds for Multi Databases;Create phinx definition file123456$ make seed_create DB=hogehoge CLASS=UserSeeder$ make seed_create DB=mogemoge CLASS=MembersSeeder......created ./db/seeds/hogehoge/UsersSeeder.phpcreated ./db/seeds/mogemoge/MembersSeeder.php Edit phinx definition file ./db/seeds/hogehoge/UsersSeeder.php 1234567891011121314151617181920212223242526272829303132333435363738&lt;?phpuse Phinx\\Seed\\AbstractSeed;class UsersSeeder extends AbstractSeed{ public function run() { $t = $this-&gt;table('users'); $t-&gt;truncate(); $genders = [1,2,3]; $faker = Faker\\Factory::create('ja_JP'); $d = []; for ($i = 0; $i &lt; 10; $i++) { $d[] = [ 'last_name' =&gt; $faker-&gt;lastName(10), 'first_name' =&gt; $faker-&gt;firstName(10), 'last_kana_name' =&gt; $faker-&gt;lastKanaName(10), 'first_kana_name' =&gt; $faker-&gt;firstKanaName(10), 'username' =&gt; $faker-&gt;userName(20), 'password' =&gt; sha1($faker-&gt;password), 'email' =&gt; $faker-&gt;email, 'postcode' =&gt; $faker-&gt;postcode, 'city' =&gt; $faker-&gt;city, 'birthday' =&gt; $faker-&gt;date($format='Y-m-d',$max='now'), 'gender' =&gt; $faker-&gt;randomElement($genders), 'card_number' =&gt; $faker-&gt;creditCardNumber, 'description' =&gt; $faker-&gt;text(200), 'created' =&gt; date('Y-m-d H:i:s'), 'updated' =&gt; date('Y-m-d H:i:s'), ]; } $this-&gt;insert('users', $d); }} ./db/seeds/hogehoge/MembersSeeder.php 123456789101112131415161718192021222324&lt;?phpuse Phinx\\Seed\\AbstractSeed;class MembersSeeder extends AbstractSeed{ public function run() { $t = $this-&gt;table('members'); $t-&gt;truncate(); $faker = Faker\\Factory::create('ja_JP'); $d = []; for ($i = 0; $i &lt; 10; $i++) { $d[] = [ 'member_code' =&gt; $faker-&gt;regexify('[0-9]{20}'), 'created' =&gt; date('Y-m-d H:i:s'), 'updated' =&gt; date('Y-m-d H:i:s'), ]; } $this-&gt;insert('members', $d); }} Run seed1$ make seed Result 123456789101112131415161718192021222324252627282930313233343536mysql&gt; use hogehoge;mysql&gt; select * from users;+---------+-----------+------------+-----------------+-----------------+-------------+------------------------------------------+------------------------------+----------+--------------+------------+--------+------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+---------------------+| user_id | last_name | first_name | last_kana_name | first_kana_name | username | password | email | postcode | city | birthday | gender | card_number | description | created | updated |+---------+-----------+------------+-----------------+-----------------+-------------+------------------------------------------+------------------------------+----------+--------------+------------+--------+------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+---------------------+| 1 | 佐々木 | 零 | ヤマダ | カナ | akira97 | e270038c94f231da7bca25dead3e386ba3984491 | hirokawa.rika@hotmail.co.jp | 1867251 | 佐々木市 | 1987-09-25 | 1 | 4024007116991463 | Dolor reiciendis fuga fugiat id molestiae eos. Dolores sint rem repudiandae perspiciatis. Ducimus aut mollitia aut asperiores laboriosam. | 2017-07-25 12:22:50 | 2017-07-25 12:22:50 || 2 | 宮沢 | 千代 | ノムラ | ヨウイチ | nagisa.taro | 695a90d1b84cf004357aad3eb37697b51afbf5cc | tanabe.hiroshi@kudo.org | 8639535 | 江古田市 | 1977-06-01 | 3 | 344103919563863 | Doloribus et recusandae quam accusantium pariatur nobis reiciendis quo. Eos quae et commodi quos accusamus ex. Ullam repellendus maiores vero sit sit et. | 2017-07-25 12:22:50 | 2017-07-25 12:22:50 || 3 | 斉藤 | 充 | ミヤケ | オサム | kana.suzuki | f309f34d08b4d0d686863fa38ed3d3af5e0b2104 | kana.kudo@mail.goo.ne.jp | 2763622 | 青田市 | 1997-01-30 | 1 | 4716886227252 | Veritatis voluptatem pariatur libero aut quia. Facere nemo quos enim amet ut ipsum sequi. Nobis natus et aspernatur aut. Natus pariatur deserunt voluptatum deserunt. | 2017-07-25 12:22:50 | 2017-07-25 12:22:50 || 4 | 吉田 | 太郎 | ウノ | ツバサ | naoko.uno | 45d04bda7ac79244c90a33ff68798b979138054a | taro.nagisa@hirokawa.com | 6099661 | 江古田市 | 2006-03-19 | 2 | 5372535333698250 | Nostrum velit nostrum eos magni. Reiciendis quos enim adipisci quisquam sed voluptas. Necessitatibus sint qui dolorem animi impedit consectetur commodi. | 2017-07-25 12:22:50 | 2017-07-25 12:22:50 || 5 | 野村 | 亮介 | サトウ | ミノル | rika.tanabe | dd3d50714c0775bfee453f7d9a9815ce26ba57db | wkudo@hotmail.co.jp | 6966314 | 渡辺市 | 1985-12-21 | 1 | 4929108488987091 | Id atque molestiae expedita omnis libero natus et. Repellendus ut tenetur molestias voluptas. Perspiciatis nisi et illum aut aut vel repudiandae. | 2017-07-25 12:22:50 | 2017-07-25 12:22:50 || 6 | 木村 | 裕美子 | タナカ | ヒロキ | hiroshi53 | 033bfd0493b72efd0ff60bc15c7eeb3b2e054501 | ztanabe@tanabe.biz | 3155238 | 山田市 | 1996-01-02 | 3 | 5476616628100007 | Assumenda consectetur ea sed et omnis alias fugiat quo. Porro nihil similique sint laudantium asperiores blanditiis. Error dolores vitae quia explicabo facilis deleniti distinctio. | 2017-07-25 12:22:50 | 2017-07-25 12:22:50 || 7 | 吉本 | 陽一 | キムラ | ヒデキ | akira27 | 51de6afc65f535ae58f927d698f07e60e04c7746 | rika59@suzuki.com | 6457702 | 田辺市 | 2010-04-12 | 2 | 5388155063289311 | Nesciunt qui beatae ut officia qui error autem. Temporibus alias earum ullam incidunt quo recusandae enim qui. Sed atque veritatis sed ad ullam qui. Repellendus est nostrum et pariatur. | 2017-07-25 12:22:50 | 2017-07-25 12:22:50 || 8 | 渡辺 | 翔太 | ササダ | クミコ | uno.momoko | fa2d16d5f2acffd5aeeaab6791fe64c9f70a9b2f | stanabe@uno.com | 5849600 | 伊藤市 | 2012-06-09 | 1 | 5274550197820022 | Odio quasi sunt tempora. Molestias aut qui sed quos beatae eum accusantium. Non dolores quam veniam et ab quidem nostrum repellendus. Qui ducimus et optio et. | 2017-07-25 12:22:50 | 2017-07-25 12:22:50 || 9 | 坂本 | 翔太 | ナカツガワ | ナオキ | akira.kudo | 4af41e536bf19fa3cb0527304adad0de76260e82 | suzuki.momoko@mail.goo.ne.jp | 8609563 | 宮沢市 | 2005-10-23 | 3 | 5231530310398512 | Qui id neque molestiae facere aut et consequatur. Delectus ea voluptatibus provident atque assumenda maxime eum. At quidem sint accusamus. Eaque sed voluptate quo sint non non. | 2017-07-25 12:22:50 | 2017-07-25 12:22:50 || 10 | 野村 | 翼 | ヒロカワ | ナオコ | taro.kudo | f8a63d0010c99d6403e0c1f458005b934ec03f8c | kana.tanabe@mail.goo.ne.jp | 5804069 | 桐山市 | 1988-12-25 | 2 | 5140671281503530 | Dolorem consequatur nulla alias perspiciatis ut. Tenetur modi cumque incidunt dolor. | 2017-07-25 12:22:50 | 2017-07-25 12:22:50 |+---------+-----------+------------+-----------------+-----------------+-------------+------------------------------------------+------------------------------+----------+--------------+------------+--------+------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+---------------------+mysql&gt; use mogemoge;mysql&gt; select * from members;+----+----------------------+---------------------+---------------------+| id | member_code | created | updated |+----+----------------------+---------------------+---------------------+| 1 | 86190539096622228312 | 2017-07-25 12:22:51 | 2017-07-25 12:22:51 || 2 | 77322186584623078448 | 2017-07-25 12:22:51 | 2017-07-25 12:22:51 || 3 | 17169562241415794809 | 2017-07-25 12:22:51 | 2017-07-25 12:22:51 || 4 | 86738824931379981947 | 2017-07-25 12:22:51 | 2017-07-25 12:22:51 || 5 | 23125815173540252188 | 2017-07-25 12:22:51 | 2017-07-25 12:22:51 || 6 | 81839177491562485300 | 2017-07-25 12:22:51 | 2017-07-25 12:22:51 || 7 | 82938165381845652192 | 2017-07-25 12:22:51 | 2017-07-25 12:22:51 || 8 | 87208503292784158954 | 2017-07-25 12:22:51 | 2017-07-25 12:22:51 || 9 | 80172779107984112104 | 2017-07-25 12:22:51 | 2017-07-25 12:22:51 || 10 | 22825755425594828330 | 2017-07-25 12:22:51 | 2017-07-25 12:22:51 |+----+----------------------+---------------------+---------------------+ Reference fzaninotto/Faker","link":"/2017/07/20/2017-07-21-commit-to-phinx-migration-tool/"},{"title":"GKEチュートリアルでサイト構築・運用","text":"概要以前さくらVPS上で tocoちゃんバスアプリを作成しました。 Flask Python3 で 戸田市 tocoちゃんバスあと何分？ Webアプリ作成♪ - 長生村本郷Engineers'BlogFlask(フラスク) とは Flask Official Site を参照すると冒頭に以下の文章があります。 Flask is a microframework for Python based on Werkzeug, Jinja 2 and good intentions … さくらVPSは個人プロジェクトを幾つか載せていましたが一部終了した為、tocoちゃんバスアプリを GCP にお引越ししました。 その時の話を GKE チュートリアルを兼ねて改めてまとめました。 何故 GCP ？toco ちゃんバスアプリはDBも持たない軽量なサイトです。その為、GCPの無料枠が利用できると思い、移行に至りました。 構成GCPでは Container Cluster を利用しこの様な構成を取っております。 以下、GCP のチュートリアルに倣い構築手順まとめました。 gcloud デフォルト設定以前無料枠を利用して構築した際の記事を参照ください。Pod 単体の寂しい構成ではありますが汗 無料枠で運用！ GKE + Kubernetes で Hubot 〜CLIから実行編〜 - 長生村本郷Engineers'Blog概要 無料枠を使って Slack 連携する Hubot を GKE で構築します。 おまけで JIRA 連携も Google Cloud SDK のインストール方法と初期化 Mac OS X 用クイックスタート を参照して SDK をダウンロードします。 Mac OS X (x8… コンテナクラスタ作成こちらも以前の記事同様、無料枠を利用すべく初めに3ノードで作成し完了後、1ノードにします。 クラスターバージョンは 1.7.2 を指定しました。((2017年8月2日時点で cluster version 最新は 1.7.2)) 1$ gcloud container get-server-config Fetching server config for us-west1-bdefaultClusterVersion: 1.6.7defaultImageType: COSvalidImageTypes: CONTAINER_VM COS UBUNTUvalidMasterVersions: 1.7.2 1.6.7validNodeVersions: 1.7.2 1.7.1 1.7.0 1.6.7 1.6.6 1.6.4 1.5.7 1.4.9 コンテナクラスタ作成 12345$ gcloud container clusters create tocochan-cluster-free \\ --cluster-version=1.7.2 \\ --machine-type f1-micro \\ --disk-size=30 \\ --num-nodes=3 Node 数を 1 に設定 1$ gcloud container clusters resize tocochan-cluster-free --size=1 確認 1$ gcloud container clusters describe tocochan-cluster-free | grep currentNodeCount currentNodeCount: 1 現在のノード数が 1 であることが確認できました。これで無料枠！ クラスタ作成後、コンテナクラスタの認証情報を取得しkubectl でクラスタ接続し操作できる様にします。 1$ gcloud container clusters get-credentials tocochan-cluster-free Container Registory 登録ローカルで起動したコンテナからイメージ作成しGCP 上の Private な Docker リポジトリである Container Registory に登録します。 以下リポジトリを利用します。 https://github.com/kenzo0107/toda-tocochan-bus Docker コンテナ起動 123$ git clone https://github.com/kenzo0107/toda-tocochan-bus$ cd toda-tocochan-bus$ docker-compose up --build -d 起動した Docker コンテナからイメージ作成・GCR へ push ((プロジェクトIDは 「mametsubuservice-175801」 )) 123$ container_id=$(docker ps | grep [f]lask | awk '{print $1}')$ docker commit $container_id gcr.io/mametsubuservice-175801/tocochan:latest$ gcloud docker -- push gcr.io/mametsubuservice-175801/tocochan:latest Pod 単体デプロイ as チュートリアル① 基本単体でデプロイすることは稀です。単純に Pod 内のコンテナが異常停止した場合などを管理できない為です。今回は無料運用の為と内容理解の為のチュートリアルとしての作業です。 pod.yaml 123456789apiVersion: v1kind: Podmetadata: name: tocochanspec: containers: - image: gcr.io/mametsubuservice-175801/tocochan:latest imagePullPolicy: Always name: tocochan Pod 単体デプロイ実行1$ kubectl create -f pod.yaml Pod 状態確認1$ kubectl get pods アクセス設定 flask は 5000 ポートで起動します。 1234$ kubectl port-forward tocochan 5000Forwarding from 127.0.0.1:5000 -&gt; 5000Forwarding from [::1]:5000 -&gt; 5000 ブラウザから http://localhost:5000 にアクセス トップページが取得できることが確認できます。 Pod 名指定し削除Pod 単体デプロイが確認できましたので削除しましょう。 1$ kubectl delete pods tocochan ReplicaSet デプロイ as チュートリアル②Pod 単体作成した場合、 Pod に異常停止したとしても特に何もリカバーされません。ReplicaSet では常に正常に動作するコンテナ数を管理しており異常停止があった場合は新たに Pod を追加します。 こちらもチュートリアルとして記載してます。こちらは終わったら削除します。 replicaset.yaml 123456789101112131415apiVersion: extensions/v1beta1kind: ReplicaSetmetadata: name: tocochanspec: replicas: 1 # 常に動作するコンテナ数 template: metadata: labels: name: tocochan spec: containers: - image: gcr.io/mametsubuservice-175801/tocochan:latest imagePullPolicy: Always name: tocochan ReplicaSet デプロイ実行1$ kubectl create -f replicaset.yaml ReplicaSet 確認1$ kubectl get rs -l name=tocochan NAME DESIRED CURRENT READY AGEtocochan-4006188167 1 1 1 10m 仮に Pods を削除しようとすると？1$ kubectl delete pods -l name=tocochan 起動コンテナが 0 になることなく、新たに作成されていることがわかります。 NAME READY STATUS RESTARTS AGEtocochan-14s3b 1/1 Running 0 4stocochan-tsvfn 1/1 Terminating 0 5m ReplicaSet 削除1$ kubectl delete rs tocochan Deployment デプロイReplicaSet のデプロイは k8s 上に履歴が残りません。Deployment デプロイでは履歴が残り、現行バージョンに異常があった場合はバージョンを簡単に戻せます。 これまでの問題を解決しているのが Deployment デプロイです。 deployment.yaml 123456789101112131415apiVersion: apps/v1beta1kind: Deploymentmetadata: name: tocochanspec: replicas: 1 template: metadata: labels: name: tocochan spec: containers: - image: gcr.io/mametsubuservice-175801/tocochan:latest imagePullPolicy: Always name: tocochan Deployment デプロイ実行--record を付けることで操作履歴を残すことができます。履歴に残すことで問題がある場合に kubectl の操作で過去のバージョンに戻すことができます。 1$ kubectl create -f deployment.yaml --record Deployment 確認1$ kubectl get deployments -l name=tocochan NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEtocochan 1 1 1 1 10m ReplicaSet 確認1$ kubectl get rs -l name=tocochan NAME DESIRED CURRENT READY AGEtocochan-2006588533 1 1 1 10m Pod 確認1$ kubectl get pods -l name=tocochan NAME READY STATUS RESTARTS AGEtocochan-4006188167-3zrn9 1/1 Running 0 10m デプロイ結果確認1$ kubectl rollout status deployment/tocochan deployment “tocochan” successfully rolled out 正しく Rollout 公開されたことがわかりました。 履歴確認1$ kubectl rollout history deployment tocochan deployments “tocochan”REVISION CHANGE-CAUSE 1 kubectl create –filename=deployment.yaml –record=true 編集1$ kubectl edit deployment tocochan vim が起動し deployment の編集が可能です。 12- image: gcr.io/mametsubuservice-175801/tocochan:latest+ image: gcr.io/mametsubuservice-175801/tocochan:v0.0.1 上記の様に編集し保存して終了すると NAME READY STATUS RESTARTS AGEtocochan-1297744065-2qb87 1/1 Terminating 0 15mtocochan-4006188167-3zrn9 1/1 Running 0 10s 既存コンテナが停止中となコンテナが新たに立ち上がったことがわかります。 履歴確認 1$ kubectl rollout history deployment tocochan REVISION CHANGE-CAUSE1 kubectl create –filename=all.yaml –record=true2 kubectl edit deployment tocochan Rollout 履歴を確認すると 編集内容が追加されていることがわかります。 バージョンを戻すREVISION 1 に戻します。 1$ kubectl rollout undo deployment tocochan --to-revision=1 1$ kubectl get pods -l name=tocochan NAME READY STATUS RESTARTS AGEtocochan-1297744065-2qb87 1/1 Terminating 0 6mtocochan-4006188167-zswcj 1/1 Running 0 7s 先ほどと同様に既存コンテナが停止しコンテナが新たに起動している様子がわかります。 外部から接続できる？ここまでの Pod の状態で以下コマンドを実行します。 1$ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes 10.3.240.1 443/TCP 3h kubernetes の cluster-ip が割り振られている以外は特に IP が割り振られておらず外部からアクセスできない状態です。 外部からアクセス出来る様、設定する必要があります。 Service 作成 外部向けの IP を設定し、外部から Pod にアクセス出来る様にルーティングします。 service.yaml 12345678910apiVersion: v1kind: Servicemetadata: name: tocochanspec: type: LoadBalancer selector: name: tocochan ports: - port: 5000 Service 作成1$ kubectl create -f service.yaml Service 確認1$ kubectl get svc 数分経過すると から IPになります。 NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes 10.3.240.1 443/TCP 10mtocochan 10.3.240.70 xx.xxx.xxx.xxx 5000:32429/TCP 10m 以下コマンドで Web ページにアクセス出来ることが確認できます。 1$ curl -v http://$EXTERNAL-IP:5000 ロードバランサー作成 ロードバランサーを立てることが可能です。80 port で受け、5000 port をバックエンドに流します。 ingress.yaml 123456789101112apiVersion: extensions/v1beta1kind: Ingressmetadata: name: hello-worldspec: rules: - http: paths: - path: /* backend: serviceName: hello-world servicePort: 5000 Ingress 作成1$ kubectl create -f ingress.yaml Ingress 確認1$ kubectl get ingress tocochan NAME HOSTS ADDRESS PORTS AGEtocochan * yy.yyy.yy.yy 80 10m 以下アクセスで先ほど実施した curl -v http://$EXTERNAL-IP:5000 と同様の結果が取得できることがわかります。 1$ curl http://$INGRESS_IP/ 設定ファイルをまとめる123456789$ echo '---' &gt; hyphen.txt; \\ cat \\ deployment.yaml \\ hyphen.txt \\ service.yaml \\ hyphen.txt \\ ingress.yaml \\&gt; all.yaml; \\rm hyphen.txt all.yaml 123456789101112131415161718192021222324252627282930313233343536373839apiVersion: apps/v1beta1kind: Deploymentmetadata: name: tocochanspec: replicas: 1 template: metadata: labels: name: tocochan spec: containers: - image: gcr.io/mametsubuservice-175801/tocochan:v0.0.1 imagePullPolicy: Always name: tocochan---apiVersion: v1kind: Servicemetadata: name: tocochanspec: type: LoadBalancer selector: name: tocochan ports: - port: 5000---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: tocochanspec: rules: - http: paths: - path: /* backend: serviceName: tocochan servicePort: 5000 以降、以下コマンドで OK ! 1$ kubectl create -f all.yaml --record ドメイン取得toda-tocochan-bus.tk は freenom で無料ドメイン取得しIngress の IP を設定し公開しています。 総評ローカルで開発して〜デプロイ、という流れが本当に簡単になりました。コンテナの理念遂行に kubernetes は大きく寄与しているなぁと実感しました。 以上です。参考になれば幸いです。","link":"/2017/08/02/2017-08-03-tutorial-of-gke/"},{"title":"Go+Revelフレームワーク 非同期でS3へ画像リサイズ&#x2F;アップロード","text":"備忘録です。 概要AWS向けのgoライブラリが乱立していてどれ使ったらいい？という感じだったので本家の launchpad.net/goamz/aws を利用して実装することにしました。 Controller app/controllers/img.go Component画像アップロード部分をcomponent化しました。 app/utility/aws.go Views Views/Img/Index.html public/js/ajax.js public/js/jquery.uploadThumbs.js","link":"/2017/08/08/2017-08-09-go-revel-upload-to-s3/"},{"title":"今更聞けない！CPU, Memory 使用率の見方","text":"気持ちを抑えられずありがちなタイトルを付けました。 サーバ負荷監視時のボトルネックの特定をする為、実際に手を動かして自分で見て解決するというチュートリアルとして本記事を参照いただければ何よりです。 サーバに接続し辛い ブラウザからURLを打ち込みサイトにアクセスするもページが表示されない API が timeout する 上記の様な事象が発生した場合は監視グラフに異変が起きているはずです。 その監視グラフを元にアクセスしづらくなった徴候のある負荷状況を確認しボトルネックを特定していきます。 以下、出来るだけ原理を知る上で CLI を元に話を進めていきます。 サーバにCPU負荷を掛ける CPU負荷を掛けるツールとしてLinuxのI/OやCPUの負荷とロードアベレージの関係を詳しく見てみるのスクリプトを拝借しました。※ありがとうございます @kunihirotanaka 社長！ loadtest.pl arg1: 並列実行するプロセス数 arg2: システムコールするプログラムを動作させるか判定 12345678910111213141516#!/usr/bin/perlmy $nprocs = $ARGV[0] || 1;for( my $i=0; $i&lt;$nprocs; $i++ ){ my $pid = fork; die $! if( $pid &lt; 0 ); if( $pid == 0 ){ while(1){ if( $ARGV[1] ){ open(IN, \"&gt;/dev/null\"); close(IN); } } }}wait; ロードアベレージを 2 に近づける様にCPU負荷を掛ける 12$ chmod +x ./loadtest.pl$ ./loadtest.pl 2 CPU使用状況確認コマンド リアルタイム監視 → top, vmstat 過去確認 → sar リアルタイム監視top で概要確認1$ top 12345678910111213top - 12:12:39 up 592 days, 18:55, 4 users, load average: 1.97, 1.13, 0.46Tasks: 125 total, 3 running, 122 sleeping, 0 stopped, 0 zombieCpu(s): 99.7%us, 0.2%sy, 0.0%ni, 0.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.2%stMem: 1020052k total, 943132k used, 76920k free, 144136k buffersSwap: 2097148k total, 466724k used, 1630424k free, 410784k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND29955 mameko_d 20 0 25196 520 184 R 99.9 0.1 4:04.91 loadtest.pl29956 mameko_d 20 0 25196 516 180 R 99.5 0.1 4:04.86 loadtest.pl24534 apache 20 0 425m 25m 7480 S 0.3 2.6 1:42.63 httpd 1 root 20 0 19232 412 224 S 0.0 0.0 0:01.77 init...... 上記結果から Load Average が上昇している %CPU, COMMAND から上昇の原因は loadtest.pl 暫定的な対処としてはこのプロセスを kill することで負荷を止めることができます。 12$ kill -9 6528$ kill -9 6529 処理途中でプロセスを kill してしまい不整合が発生する様な処理の場合は別途、CPU の増強等を検討する等、状況によりますが対応を検討する必要があります。 CPU 使用率高いランキング Top10 表示1$ ps ax --sort=-%cpu -o command,pid,pid,%cpu|head -n 11 ※ -n 11 なのは 1行目はカラム名が入る為 1COMMAND PID PID %CPU グラフで見るこれまで CLI で確認した考察の答え合わせとして確認しましょう。 12:07 辺りから負荷上昇 loadavg5 急上昇 CPU user 急上昇。 CPU system はそこまで上がっていない。→ アプリケーションのプロセスがCPUを食っている。 memory は消費していない top コマンドの様にどのプロセスが原因かまではグラフからは不明です。サーバにアクセスして12:07 あたりからのログを調査する等原因を特定していきます。 補足ちなみにApache のモジュールとして PHP を利用している場合は COMMAND に httpd と表示されます。fluentd は ruby で実行されているので ruby です。 PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND12376 apache 20 0 833m 115m 19m S 2.4 3.1 0:03.52 httpd 1455 td-agent 20 0 461m 74m 0 S 1.2 2.0 1098:30 ruby vmstat で CPU 使用率確認1秒ごとに出力 123456789101112131415161718192021222324$ vmstat 1procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu-----r b swpd free buff cache si so bi bo in cs us sy id wa st0 0 515396 80756 37120 220320 0 0 12 2 0 0 0 0 100 0 00 0 515396 80756 37120 220320 0 0 0 0 110 200 0 0 100 0 00 0 515396 80756 37120 220320 0 0 0 0 103 205 0 0 100 0 10 0 515396 80632 37120 220320 0 0 0 0 121 212 0 0 99 0 00 0 515396 80632 37120 220320 0 0 0 0 114 216 0 0 100 0 02 0 515328 80648 36944 220432 0 0 0 0 2092 237 100 0 0 0 02 0 515328 80648 36944 220432 0 0 0 0 2071 224 100 0 0 0 02 0 515328 81020 36952 220432 0 0 0 0 2162 381 100 1 0 0 02 0 515328 80896 36952 220432 0 0 0 0 2164 266 100 0 0 0 02 0 515328 80756 36952 220432 0 0 0 0 2139 308 100 0 0 0 02 0 515328 80772 36952 220432 0 0 0 0 2111 237 100 0 0 0 02 0 515328 80772 36952 220432 0 0 0 0 2087 238 100 0 0 0 02 0 515328 80772 36952 220432 0 0 0 0 2077 237 100 0 0 0 02 0 515328 80772 36952 220432 0 0 0 0 2076 232 99 1 0 0 02 0 515328 80772 36952 220432 0 0 0 0 2078 235 100 0 0 0 02 0 515328 80904 36952 220432 0 0 0 0 2081 231 85 0 0 0 150 0 515328 81448 36952 220432 0 0 0 0 267 254 6 0 94 0 00 0 515328 81448 36952 220432 0 0 0 0 151 250 0 0 99 0 00 0 515328 81448 36952 220432 0 0 0 0 230 307 0 0 99 0 00 0 515328 81456 36952 220432 0 0 0 0 123 230 0 0 100 0 0 上記から以下のことが確認できます。 ./loadtest.pl 2 を実行中は procs r (実行中プロセス) = 2 となっている cpu us が 100%, cpu id が 0% cpu id がなくなり、プログラムが 100% CPU を食いつぶしている system in (割り込み回数)、system cs (コンテキストスイッチ回数) が増加 コンテキストスイッチ自体が CPU を食いシステムの負荷を上げている 過去確認123456789$ sar -u -s 21:00:00 -e 22:10:00 -f /var/log/sa/sa3121:00:01 runq-sz plist-sz ldavg-1 ldavg-5 ldavg-1521:10:01 0 200 0.00 0.04 0.0721:20:01 0 203 0.00 0.00 0.0121:30:01 0 203 0.00 0.00 0.0021:40:01 0 211 0.00 0.03 0.0021:50:01 0 210 0.65 0.82 0.37Average: 0 205 0.13 0.18 0.09 sar コマンドは過去まで遡って確認できるので便利です。 sar -q 実行結果各項目 Item Explain runq-sz CPU を実行する為のメモリー内で待機中のカーネルスレッド数。通常、この値は 2 未満になる。値が常に 2 より大きい場合は、システムが CPU の限界に到達している可能性がある plist-sz プロセスリストのプロセスとスレッド数 ldavg-1 過去1分間のロードアベレージ ldavg-5 過去5分間のロードアベレージ ldavg-15 過去15分間のロードアベレージ システムコールを伴うCPU負荷ロードアベレージを 2 に近づける &amp; システムコールする様にCPU負荷を掛ける 1$ ./loadtest.pl 2 1 vmstat で監視123456789101112131415161718192021222324252627$ vmstat 1procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 597832 886856 3808 34844 0 0 12 2 0 0 0 0 100 0 0 1 0 597828 886856 3808 34908 0 0 0 0 134 233 1 0 99 0 0 0 0 597788 886684 3944 34876 20 0 156 0 238 307 1 1 96 4 0 0 0 597788 886684 3944 34904 0 0 0 0 109 219 0 0 100 0 0 2 0 597756 884044 3956 36296 96 0 1500 0 1075 1274 18 26 41 15 0 ← loadtest.pl 2 1 実行開始 2 0 597756 884044 3956 36296 0 0 0 0 2080 2265 42 58 0 0 0 2 0 597756 884076 3956 36296 0 0 0 0 2083 2458 41 60 0 0 0 2 0 597756 884200 3964 36292 0 0 0 32 2103 2458 42 59 0 0 0 2 0 597756 884200 3964 36296 0 0 0 0 2079 2588 41 60 0 0 0 3 0 597756 883952 3964 36296 0 0 0 0 2080 2209 40 60 0 0 1 2 0 597756 884216 3964 36296 0 0 0 0 2085 2395 42 58 0 0 0 2 0 597756 884216 3964 36296 0 0 0 0 2061 2399 43 57 0 0 0 3 0 597756 884092 3964 36296 0 0 0 0 2061 1991 44 57 0 0 0 2 0 597756 884216 3964 36296 0 0 0 0 2059 2333 42 58 0 0 1 2 0 597756 884216 3964 36296 0 0 0 0 2058 2211 42 58 0 0 1 2 0 597756 884092 3964 36296 0 0 0 0 2058 2461 43 58 0 0 0 2 0 597756 883844 3964 36296 0 0 0 0 2059 2641 42 58 0 0 0 2 0 597756 884216 3964 36296 0 0 0 0 2158 2715 42 59 0 0 0 ← loadtest.pl 2 1 実行終了 0 1 597744 884588 3964 36364 44 0 144 0 1995 2313 37 58 3 2 0 0 0 597724 884388 3964 36524 208 0 380 0 173 239 0 1 95 5 0 0 0 597724 884388 3964 36568 0 0 0 0 102 196 0 0 100 0 0 0 0 597636 884388 3964 36568 0 0 0 0 102 203 0 0 100 0 0 0 0 597636 884512 3964 36568 0 0 0 0 104 195 0 0 100 0 1 loadtest.pl からシステムコールが多数実行される為、cpu sy 上昇している。 グラフで見る 12:25 辺りから負荷急上昇 loadavg5 急上昇 CPU user, system 共に急上昇。 system の割合が多い→ システムコールを伴うアプリケーションのプロセスがCPUを食っている。 memory は消費していない 対応例 アプリケーションのCPU使用箇所の特定 datadog, NewRelic 等の APM(Aplication Performance Management) 導入しアプリケーションのボトルネック抽出し修正 コストこそ掛かりますが非常に有用です CPU増設 対象アプリのプロセスを kill (先ほどの プロセス kill ) 例）管理画面で集計処理し、DBに負荷掛けサービスに影響してしまった時に集計処理のプロセスを kill サーバにメモリ負荷を掛ける memorytest.pl 1秒毎に 20MB 消費する様に設定 1234567#!/usr/bin/perlwhile(1){ sleep 1; push @h , 1 x 1024 x 1024 x 20}wait; メモリ負荷実行 12$ chmod +x ./memorytest.pl$ ./memorytest.pl メモリ使用状況確認コマンド リアルタイム → top -a, free 過去確認 → sar 残メモリ確認top で概要確認1$ top -a もしくは top 実行後、 Shift + m PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 6780 mameko_d 20 0 385m 362m 1212 S 5.3 36.4 0:01.88 memorytest.pl 上記結果から %MEM, COMMAND から上昇の原因は memorytest.pl 暫定的な対処としては loadtest.pl と同様、プロセスを kill することで負荷を止めることができます。 1$ kill -9 6780 free で残メモリ確認123456$ free total used free shared buffers cachedMem: 1020052 908664 111388 572 29764 204492-/+ buffers/cache: 674408 345644Swap: 2097148 517696 1579452 何度か実行すると徐々に Mem の used 上昇 free 減少 free 減少に引っ張られて buffers, cached 減少 free 実行結果各項目 Mem: 実メモリ Swap: 仮想メモリ Item Explain shared プロセス間で共有できる領域用メモリ buffers buffer に利用されるメモリI/Oアクセスする時に、直接I/Oに行くのではなく、キャッシュ経由でアクセスさせる為のメモリ cached ページキャッシュ用メモリアプリで実メモリが必要な際は、 cached のメモリが破棄される 確認観点上記 free コマンド実行結果から解放されたメモリは 1Mem free 111388 kB = 108 MB これだけでは 108 MB が残りと思いがちですが通常、各プロセスにメモリ割り振った残りを buffer と cache に利用して disk I/O を軽減している為、buffer + cache も含まれます。 実質、残メモリはどこ見れば良い？free + buffers + cached= 111388 + 29764 + 204492 kB= 345644 kB= 338 MB= -/+ buffers/cache free 以上から残メモリの目安は -/+ buffers/cache free 確認 or スラッシングを確認します。 Swap 発生は何を見ればわかる？vmstat 実行してスラッシングが発生しているか確認 スラッシング確認方法 si (swap in from disk), so (swap out to disk) が多発している場合、スラッシングが発生しています so が比較的高い場合、メモリ不足の可能性大 12345678910111213141516171819202122$ vmstat 1procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 593244 211856 3704 34744 0 0 0 0 236 215 4 2 94 0 0 1 0 593244 211876 3704 34744 0 0 0 0 144 216 1 0 98 0 0 0 0 593244 160664 3704 34744 0 0 0 0 207 220 4 1 96 0 0 0 0 593244 109328 3704 34744 0 0 0 0 227 226 4 2 94 0 0 0 1 593052 58900 3704 33312 0 0 0 0 515 241 4 2 59 34 0 0 1 618460 55144 1224 15220 0 25416 0 25416 39357 528 5 24 34 37 1 ← ここで memorytest.pl 行開始 0 1 670408 56644 1160 13732 0 51948 0 51948 37384 644 7 22 34 37 1 1 1 671048 59256 456 12480 0 640 0 640 182 254 1 1 49 50 0 0 0 735892 72040 436 10088 0 64844 96 64844 14559 1044 3 15 67 15 0 0 0 786748 71436 436 9596 0 50856 124 50856 13678 745 4 13 69 14 1 0 2 830880 63556 436 9504 32 44144 320 44144 15659 636 5 13 54 28 0 0 3 849932 48976 436 8652 304 19168 1104 19168 6568 661 6 6 32 55 1 0 1 900916 71564 776 8264 88 50992 560 50992 9549 706 1 10 27 62 0 0 3 941292 64152 1308 9880 80 40380 2564 40380 10800 622 5 11 29 56 1 0 1 993108 69908 1700 10656 0 51820 2024 51852 10208 848 5 12 43 40 1 2 0 994168 71536 1700 10428 0 1060 0 1060 216 257 3 1 82 15 0 0 0 1045720 71384 1700 9456 0 51552 0 51552 5356 789 2 9 77 12 1 0 0 1096468 71468 1332 9012 0 50748 0 50748 15385 857 6 13 72 9 1 bo (block out to device) … ブロックデバイスに送られたブロック bi (block in from device) … ブロックデバイスから受け取ったブロック 物理メモリ使用量(Resident Set Size)高いランキング Top10 表示1$ ps ax --sort=-rss -o command,pid,ppid,vsz,rss|head -n 11 グラフで見る 12:35 辺りからメモリ急上昇 cached 減、 used 増 対応例 プロセスレベルで監視 datadog, mackerel, prometheus, zabbix 等監視ツール導入 メモリ増設 対象アプリのプロセスを kill (先ほどの プロセス kill ) Disk I/OI/Oディスク利用状況 確認123456789$ sar -b -s 13:00:00 -e 14:00:00 -f /var/log/sa/sa3113:00:01 tps rtps wtps bread/s bwrtn/s13:10:01 0.28 0.01 0.27 0.11 2.9013:20:01 0.28 0.00 0.28 0.05 2.9913:30:01 0.22 0.00 0.22 0.00 2.3013:40:01 0.24 0.00 0.24 0.00 2.5013:50:01 0.23 0.00 0.23 0.03 2.35Average: 0.25 0.00 0.25 0.04 2.61 sar -b 実行結果項目 Item Explain tps １秒あたりの転送 (デバイスに対するIOリクエスト) 数の合計 rtps １秒あたりの読み込みIOリクエストの回数の合計 wtps １秒あたりの書き込みIOリクエストの回数の合計 bread/s １秒あたりの（ブロック単位）読み込みIOリクエストのデータ量の合計 bwrtn/s １秒あたりの（ブロック単位）書き込みIOリクエストのデータ量の合計 確認観点 I/O 待ち増加する原理 123456789プロセスのメモリ消費量増↓キャッシュにメモリが使えなくなる↓Disk I/O にメモリが使えなくなる↓Disk I/O 増加↓I/O 待ちプロセス増加 対策 メモリの使用状況と Swap 確認 まとめCPU, Memory 使用率が上昇する原理を知った上で監視をすると全然グラフの見え方が違うことを実感しました。 本来はグラフから見ることになるかと思います。その際に top, vmstat, sar を実行した時の数値の変化の仕方をイメージするとより原因追及に大きな一歩となると思います。 以上少しでも参考になれば幸いです。 Reference vmstat コマンドの読み方 単独のサーバーの「負荷」の正体を突き止める 「非エンジニア」でもできるサーバ過負荷対策。ロードアベレージ、CPU使用率、I/Oディスク利用状況、メモリ使用量の調査方法 サーバ負荷の原因を探る方法 Linux Performance Measurements using vmstat LinuxのI/OやCPUの負荷とロードアベレージの関係を詳しく見てみる 補足Swap とは？カーネルがほぼ利用されていないメモリを検出して排除している状態を表示しています。 メモリの空きを入出力のバッファとキャッシュに使用 さらに活動が少ない部分をメモリから排除して swap へ押し出す swap からバッファやキャッシュに転用 その為、 swap が発生している、といって慌てない。 Swap を利用する意義メモリ不足でもメモリの一部をディスクに退避させて計算し続けることができます。メモリを使い切った様に見せつつもまだ使える仕組みを 仮想メモリ と言います。 スラッシングとは？実メモリ と Swap のメモリの移動が大量発生し処理が遅延する現象です。 sar コマンドがない場合はインストール12345// CentOS$ yum install -y sysstat// Ubuntu$ apt-get install -y sysstat","link":"/2016/08/09/2017-08-10-how-to-see-cpu-memory-utili/"},{"title":"Vagrant + docker-compose で Rails 5.1.0 (Puma) + Nginx + MySQL 環境構築","text":"概要簡易的に Rails 環境を構築・開発できる様にすべく構築しました。 こんな時に利用してます。 新規プロジェクト開発 新規 gem, その他ミドルウェアの試験 簡単なモックを作ってディレクターに見せたい時とか 構築手順をまとめました。 環境 macOS Sierra 10.12.5 VirtualBox 5.1.18r114002 Vagrant 1.9.3 VagrantBox Ubuntu 14.04.5 Docker version 17.06.0-ce, build 02c1d87 Git Clone12345macOS%$ git clone https://github.com/kenzo0107/vagrant-dockermacOS%$ cd vagrant-dockermacOS%$ vagrant upmacOS%$ vagrant sshvagrant%$ cd /vagrant/rails-puma-nginx-mysql/ Rails プロジェクト作成12// database = mysqlvagrant%$ docker-compose run --rm web rails new . --force --database=mysql --skip-bundle puma.rb 設定123// backupvagrant%$ cp ./rails/config/puma.rb ./rails/config/puma.rb.bkvagrant%$ cp puma.rb ./rails/config/ ./rails/config/puma.rb 12345678threads_count = ENV.fetch(\"RAILS_MAX_THREADS\") { 5 }.to_ithreads threads_count, threads_countport ENV.fetch(\"PORT\") { 3000 }environment ENV.fetch(\"RAILS_ENV\") { \"development\" }plugin :tmp_restartapp_root = File.expand_path(\"../..\", __FILE__)bind \"unix://#{app_root}/tmp/sockets/puma.sock\" データベース接続情報設定123// backupvagrant%$ cp ./rails/config/database.yml ./rails/config/database.yml.bkvagrant%$ cp database.yml ./rails/config/ ./rails/config/database.yml 1234567default: &amp;default adapter: mysql2 encoding: utf8 pool: &lt;%= ENV.fetch(\"RAILS_MAX_THREADS\") { 5 } %&gt; username: root password: &lt;%= ENV['MYSQL_ROOT_PASSWORD'] %&gt; # &lt;--- MYSQL_ROOT_PASSWORD host: db # &lt;--- service name DB作成12345678910111213141516vagrant%$ docker-compose run --rm web rails db:createCreated database 'app_development'Created database 'app_test'vagrant%$ docker-compose exec db mysql -u root -p -e'show databases;'Enter password: (password)+--------------------+| Database |+--------------------+| information_schema || app_development | &lt;--- add !| app_test | &lt;--- add !| mysql || performance_schema || sys |+--------------------+ Rails 実行1vagrant%$ docker-compose up -d http://192.168.35.101 にアクセスすると Rails のウェルカムページが表示されます。 rails grails g 実行時は基本 one-off container で実行するのが良いです。 例えば以下は articles テーブルを作成、また、関連する controller, view, model を作成します。 1vagrant%$ docker-compose run --rm web rails g scaffold article title:string body:text Gemfile 更新Gemfile 更新した際はビルドし再起動します。 123vagrant%$ docker-compose stop webvagrant%$ docker-compose build webvagrant%$ docker-compose up -d web あとがきRack server との接続は一癖ありましたが、そこさえ乗り越えたらすっと行きました♪ DB は 3306 でオープンしてるのでMac のローカルから Sequel Pro で接続して確認できます。 これをベースに EFK でログ確認できる様にしたり、mailcatcher でメール機能を試験できる様にしたりと何かと便利です。 Docker 有難や♪","link":"/2017/08/21/2017-08-22-docker-compose-rails-nginx-mysql-on-vagrant/"},{"title":"MySQL 一定秒以上 Sleep しているプロセスを一括 kill","text":"メモ 300秒以上 Sleep しているプロセスIDをまとめて表示1234567$ mysql -h &lt;host&gt; -u &lt;user&gt; -p -e'SELECT GROUP_CONCAT(ID) FROM information_schema.PROCESSLIST WHERE TIME &gt; 300;'+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| GROUP_CONCAT(ID) |+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| 2147,2143,2138,2113,2111,2104,2102,2098,2087,2085,2082,2081,2079,2078,2069,2068,2045,2037,2029,2025,2023,2016,2015,2006,2005,2003,2002,2001,1999,1998,1997,1995,1987,1986,1984,1982,1981,1974,1973,1968,1966,1963,1961,1959,1957,1955,1954,1949,1937,1936,1928,1925,1923,1920,1916,1914,1912,1908,1906,1898,1892,1869,1847,1842,1651,1650,1572,1570,1568,1566,1539,1522,1517,1516,1514,1511,1506,1483 |+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ プロセスIDをまとめて一括 kill1$ mysqladmin kill &lt;pid,pid,pid...&gt; -h &lt;host&gt; -u &lt;user&gt; -p","link":"/2017/08/24/2017-08-25-mysql-kill-sleep-process/"},{"title":"docker-comopse で Rails 5 (Puma) + Nginx + Mysql 構築 on Vagrant(Ubuntu)","text":"自身の Rails 開発環境の雛形として利用している docker-compose です。 kenzo0107/vagrant-dockerDocker on Vagrant(ubuntu). Contribute to kenzo0107/vagrant-docker development by creating an account on GitHub. 以下設定手順です。 Vagrant 起動1234macOS%$ git clone https://github.com/kenzo0107/vagrant-dockermacOS%$ cd ./vagrant-docker/macOS%$ vagrant upmacOS%$ vagrant ssh Rails プロジェクト作成123// on vagrantvagrant%$ cd /vagrant/rails-puma-nginx-mysqlvagrant%$ docker-compose run --rm web rails new . --force --database=mysql --skip-bundle puma 設定ファイルセット1vagrant%$ cp puma.rb ./rails/config/ ./rails/config/puma.rb 12345678threads_count = ENV.fetch(\"RAILS_MAX_THREADS\") { 5 }.to_ithreads threads_count, threads_countport ENV.fetch(\"PORT\") { 3000 }environment ENV.fetch(\"RAILS_ENV\") { \"development\" }plugin :tmp_restartapp_root = File.expand_path(\"../..\", __FILE__)bind \"unix://#{app_root}/tmp/sockets/puma.sock\" データベース設定ファイルセット1vagrant%$ cp database.yml ./rails/config/ ./rails/config/database.yml 1234567default: &amp;default adapter: mysql2 encoding: utf8 pool: &lt;%= ENV.fetch(\"RAILS_MAX_THREADS\") { 5 } %&gt; username: root password: &lt;%= ENV['MYSQL_ROOT_PASSWORD'] %&gt; # &lt;--- MYSQL_ROOT_PASSWORD host: db # &lt;--- service name データベース作成12345678910111213141516vagrant%$ docker-compose run --rm web rails db:createCreated database 'app_development'Created database 'app_test'vagrant%$ docker-compose exec db mysql -u root -p -e'show databases;'Enter password: (password)+--------------------+| Database |+--------------------+| information_schema || app_development | &lt;--- add !| app_test | &lt;--- add !| mysql || performance_schema || sys |+--------------------+ 以上で必要最低限の Rails プロジェクトの準備ができました！ Rails, Nginx, MySQL 全コンテナ起動1vagrant%$ docker-compose up -d ブラウザより http://192.168.35.101 へアクセスしRails トップページが表示されることが確認できます。 総評docker でコンテナ化しているので Nginx, MySQL 等、バージョンアップしたい時でもコンテナを置き換えるだけで簡単に使用感を確認できたり機能を確認できたりと便利です。 これに Elasticsearch + Kibana でログを可視化したりMailcatcher でメール送信を確認できるようにしたりと開発するには十分な状況が用意できます。 是非開発の一助になれば幸いです。","link":"/2017/09/12/2017-09-13-docker-compose-rails5-nginx-mysql-on-vagrant/"},{"title":"CasperJS+PhantomJS で Github Organization 移行","text":"概要Github Enterprise の Organization 移行を実施した際に CasperJS と PhantomJS でヘッドレスブラウザより操作し移行しました。Github 上で API がない（はず？）為、この様な対応をしました。 どちらかというと CasperJS+PhantomJS でブラウザ上の試験作りを楽しんでいたこともあり試してみてすんなりできたので採用した経緯になります。 ヘッドレスブラウザとは？GUI なしのブラウザを CLI で利用するというもの。ページ描画や画像ロード、ログインしたりとフロントの試験で期待される機能を持っています。 三行まとめ CasperJS + PhantomJS でログイン認証はじめブラウザ操作で Transfer する工程を実行 移行後、元の URL が移行先にリダイレクトされるか確認 期待する要素が時間内に取得できないときはページをキャプチャ やってみるGet Started ご参照ください。 kenzo0107/transfer-repository-on-gheTransfer repositories to a new owner by using headless browser with CasperJS and PhantomJS. - kenzo0107/transfer-repository-on-ghe 以下にも手順まとめてます。 Clone12macOS%$ git clone https://github.com/kenzo0107/ghe-org-transfermacOS%$ cd ghe-org-transfer scripts/ghe-org-transfer.js の &lt;your ****&gt; 編集1234567891011121314151617181920var config = { // site root. siteRoot: 'https://&lt;your github domain&gt;', // Github Login path loginPath: '/login', // Github Login Account loginParams: { email: '&lt;your email&gt;', password: '&lt;your password&gt;' }, viewportSize: { width: 3000, height: 2500 }, paths: [ \"&lt;your owner&gt;/&lt;your repository&gt;\" ], destOrganization: '&lt;your destination of organization&gt;', reason: 'transfer this repository to new oragnization',}; 移行イメージ ex) hoge/mywonderfulrepo —&gt; moge/mywonderfulrepo 1234paths: [ \"hoge/mywonderfulrepo\"],destOrganization: 'moge', paths は複数指定しても問題ありません。 移行実施1macOS%$ make run 実行結果 ※ 移行後に元の URL でリダイレクトされるか試験しています。 12345[url] https://github.aiueo.com/hoge/mywonderfulrepo/settings[step] fill '#transfer_confirm &gt; form'[step] input checkbox #team-59[step] click the button labeled \"Transfer\"(^-^) redirect ok:https://github.aiueo.co.jp/hoge/mywonderfulrepo to https://github.aiueo.co.jp/moge/mywonderfulrepo 総評以前 Grafana のグラフのスナップショットを撮る Grafana API がうまく動作しなかったのもCasperJS+PhantomJS で取得する様にできました。 いやはや便利♪ 全然別件ですが、サイトの認証に利用される reCAPTCHA の突破など挑戦してましたがうまく行かず…うまくいったぜ！という方、是非教えてください m(_ _)m","link":"/2017/10/24/2017-10-25-casperjs-phantomjs-github-organization/"},{"title":"WAF+CloudFront でリファラチェック (直リンク禁止)","text":"概要AWS WAF (Web Application Firewall) を利用し Cloudfront でのリファラ制御を実装しましたのでそのまとめです。 直リンク禁止対策として導入しました。 以下手順になります。 Go to AWS WAF ボタンクリックサービス &gt; WAF &amp; Shield と辿り Go to AWS WAF クリック Configure Web ACL ボタンクリックACL (Access Control List) を設定していきます。 概要確認特にチェックせず Next ボタンクリック web ACL 設定 以下、設定項目を設定し、Next ボタンクリック Item Value Web ACL name (任意) 例ではCloudfront の CNAME を設定しています。 CloudWatch metric name Web ACL name を入力すると自動で入力される。変更したい場合のみ変更 Region Global(CloudFront) 選択 AWS resource to associate 対象となるCloudfrontを選択する箇所。運用中の Cloudfront を対象とすると場合は後々設定。 条件作成今回は文字列一致を条件とする為、 String match conditions にて Create condition ボタンクリック string match condition 作成 以下設定し Add filter ボタンクリック。複数 filter がある場合、Add filter を繰り返します。 Item Value Name (任意) Part of the request to filter on Header Header Referer Match type Contains Transformation Convert to lowercase Value to match 対象となるドメイン設定 Add filter 後、 Create ボタンクリック。 Next ボタンクリック追加したもののすぐに反映されていない。そのまま Next ボタンクリック ルール作成Create rule ボタンクリック。 ルールに条件を紐付けName, Cloudwatch metric name を設定しAdd conditions で条件を追加します。 その後、Create ボタンクリック。 ルール以外のリクエストのアクセス禁止やはり Rule は反映されていない。ですが、続けてBlock all requests that don't match any rules をチェックし Review and create ボタンクリック。 ※対象のCloudfront に反映させたくない場合は、Cloudfront を選択したリソースを解除する必要があります。※最後に関連付けができるのでここではするべきではないと思います。 確認ページで入力内容確認後作成Confirm and create ボタンクリック。 対象の web ACL を編集WEB ACLs より選択し Edit web ACL ボタンクリック web ACL 編集 作成したルールを選択 Add rule to web ACL ボタンクリック Allow 選択 Update ボタンクリック [f:id:kenzo0107:20171005183720p:plain] Cloudfront 関連付けAdd association ボタンクリック Web ACL に Cloudfront を関連付けResource で 対象の Cloudfront を選択し Add ボタンクリック 以上で数分後 WAF+CloudFront によるリファラチェックが確認できる状態になります。 アクセス確認自環境ではローカルの /etc/hosts 修正し対象ドメインから Cloudfront CNAME へのリンクを貼って確認しました。 Cloudfront CNAME ドメインでのリソースを直接アクセスすると以下の様な エラーページが表示されることが確認できました。 もう少しユーザフレンドリーに上記のエラーページは Cloudfront &gt; Error Pages で Create Custom Error Response で S3 上のパスを指定することでカスタマイズが可能です。 是非サイトコンセプトに合ったエラーページをご用意されるとよりユーザフレンドリーな配信になるかと思います。 以上ご参考になれば幸いです。","link":"/2017/10/07/2017-10-08-waf-cloudfront-referer-check/"},{"title":"AWS Elasticsearch Service バージョンアップ 2.2 → 5.5","text":"概要AWS Elasticsearch Service (ES) 2.3 → 5.5 へバージョンアップを実施に際して以下記事をまとめました。 大まかな流れ ESバージョン2.3のドメインからSnapshot取得 ESバージョン5.5のドメイン作成 アプリの fluentd の向け先をESバージョン5.5へ変更 ESバージョン5.5のドメインにデータリストア ESバージョン2.3のドメイン削除 現状バージョン確認1234567891011121314$ curl https://&lt;Elasticsearch 2.3 Endpoint Domain&gt;{ \"name\" : \"Jackdaw\", \"cluster_name\" : \"&lt;Account ID&gt;:xxxxxxxxxx\", \"version\" : { \"number\" : \"2.3.2\", \"build_hash\" : \"72aa8010df1a4fc849da359c9c58acba6c4d9518\", \"build_timestamp\" : \"2016-11-14T15:59:50Z\", \"build_snapshot\" : false, \"lucene_version\" : \"5.5.0\" }, \"tagline\" : \"You Know, for Search\"} その他、AWS console のクラスターの設定確認その他クラスターへ設定している情報をメモ インスタンスタイプ アクセスポリシーの確認 AWS Elasticsearch Service スナップショットを S3 で管理するIAM role 作成 ec2 タイプで作成 アクセス権限は特に設定せず 「次のステップ」ボタンクリック ロール名を es-index-backups とし作成 ロールARN arn:aws:iam:::role/es-index-backups で作成されていることが確認できる 信頼関係の編集 Service を es.amazonaws.com に編集 12345678910111213{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"es.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ]} IAM User 作成 ユーザ &gt; ユーザ追加 選択 ユーザ名 es-index-backup-user とし プログラムによるアクセスにチェックを入れて 次のステップ クリック 特にポリシーをアタッチせず 次のステップ クリック es-index-backup-user を作成し独自ポリシーで先ほど作成した role へのアクセス許可設定をアタッチします。 123456789101112{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"iam:PassRole\" ], \"Resource\": \"arn:aws:iam::&lt;Account ID&gt;:role/es-index-backups\" } ]} 発行されたアクセスキー、シークレットアクセスキーをメモしておきます。 S3 バケット作成S3 &gt; バケットを作成する でバケット作成してください。 Elasticsearch にてスナップショットリポジトリ作成スナップショットを管理するリポジトリを Elasticsearch に作成する必要があります。 Elasticsearch へのアクセス可能なサーバにて以下スクリプト実行します。 register_es_repository.py 123456789101112131415161718192021222324252627from boto.connection import AWSAuthConnectionclass ESConnection(AWSAuthConnection): def __init__(self, region, **kwargs): super(ESConnection, self).__init__(**kwargs) self._set_auth_region_name(region) self._set_auth_service_name(\"es\") def _required_auth_capability(self): return ['hmac-v4']if __name__ == \"__main__\": client = ESConnection( region='ap-northeast-1', host='&lt;Elasticsearch 2.3 Endpoint Domain&gt;', aws_access_key_id='&lt;ACCESS KEY ID&gt;', aws_secret_access_key='&lt;SECRET ACCESS KEY&gt;', is_secure=False) resp = client.make_request( method='POST', path='/_snapshot/index-backups', data='{\"type\": \"s3\",\"settings\": { \"bucket\": \"&lt;bucket name&gt;\",\"region\": \"ap-northeast-1\",\"role_arn\": \"arn:aws:iam::&lt;Account ID&gt;:role/es-index-backups\"}}' ) body = resp.read() print body 123456$ chmod +x register_es_repository.py$ python register_es_repository.py// 成功{\"acknowledged\":true} リポジトリ登録完了しました。 Snapshot 取得snapshot 名を 20170926 とします。 1234$ curl -XPUT \"https://&lt;Elasticsearch 2.3 Endpoint Domain&gt;/_snapshot/index-backups/20170926\"// 成功{\"accepted\":true} Snapshot 一覧20170926 という snapshot 名で取得したことが確認できます。 12345678910111213141516171819202122232425262728$ curl -s -XGET \"https://&lt;Elasticsearch 2.3 Endpoint Domain&gt;/_snapshot/index-backups/_all\" | jq .{ \"snapshots\": [ { \"snapshot\": \"20170926\", \"version_id\": 2030299, \"version\": \"2.3.2\", \"indices\": [ \"nginx-access-2017.09.09\", \"nginx-access-2017.09.07\", \"nginx-access-2017.09.08\", \"nginx-error-2017.08.24\", \"nginx-error-2017.08.23\", \".kibana-4\",... ], \"state\": \"IN_PROGRESS\", \"start_time\": \"2017-09-26T03:58:51.040Z\", \"start_time_in_millis\": 1506398331040, \"failures\": [], \"shards\": { \"total\": 0, \"failed\": 0, \"successful\": 0 } } ]} Snapshot 削除スナップショット 20170926 を削除する場合、DELETE メソッドを実行します。 1234$ curl -XDELETE https://&lt;Elasticsearch 2.3 Endpoint Domain&gt;/_snapshot/index-backups/20170926// 成功{\"acknowledged\":true} S3 確認以下が作成されているのがわかります。 indices/* meta-* snap-* はじめ meta-* が作成できたら完了なのかなと思いきやsnap-* も作られるまで待つ必要がありました。 CLI 上でスナップショット完了確認した方が確実です。 123456$ curl -s -GET https://&lt;Elasticsearch 2.3 Endpoint Domain&gt;/_snapshot/index-backups/20170926... \"state\": \"SUCCESS\",...... Elasticsearch 5.5 Service 新規ドメイン作成Elasticsearch 2.3 の設定に倣って作成します。 リポジトリ作成 register_es55_repository.py register_es_repository.py の host 部分を新規ドメインに修正します。 12345678910111213141516171819202122232425262728from boto.connection import AWSAuthConnectionclass ESConnection(AWSAuthConnection): def __init__(self, region, **kwargs): super(ESConnection, self).__init__(**kwargs) self._set_auth_region_name(region) self._set_auth_service_name(\"es\") def _required_auth_capability(self): return ['hmac-v4']if __name__ == \"__main__\": client = ESConnection( region='ap-northeast-1', host='&lt;Elasticsearch 5.5 Endpoint Domain&gt;', aws_access_key_id='&lt;ACCESS KEY ID&gt;', aws_secret_access_key='&lt;SECRET ACCESS KEY&gt;', is_secure=False) print 'Registering Snapshot Repository' resp = client.make_request( method='POST', path='/_snapshot/index-backups', data='{\"type\": \"s3\",\"settings\": { \"bucket\": \"&lt;bucket name&gt;\",\"region\": \"ap-northeast-1\",\"role_arn\": \"arn:aws:iam::&lt;Account ID&gt;:role/es-index-backups\"}}' ) body = resp.read() print body 123456$ chmod +x register_es55_repository.py$ python register_es55_repository.py// 成功{\"acknowledged\":true} スナップショットからリストア20170926 のスナップショットをリストアします。 1234$ curl -XPOST \"https://&lt;Elasticsearch 5.5 Endpoint Domain&gt;/_snapshot/index-backups/20170926/_restore\"// 成功{\"accepted\": true} リストア確認1$ curl -XGET \"https://&lt;Elasticsearch 5.5 Endpoint Domain&gt;/_cat/indices\" スナップショットに失敗するケース .kibana index が既に存在しており、リストアできない。 12345678910111213{ \"error\":{ \"root_cause\":[ { \"type\":\"snapshot_restore_exception\", \"reason\":\"[index-backups:20170926/Hc4rLIoARCWqpyJXeP7edw] cannot restore index [.kibana] because it's open\" } ], \"type\":\"snapshot_restore_exception\", \"reason\":\"[index-backups:20170926/Hc4rLIoARCWqpyJXeP7edw] cannot restore index [.kibana] because it's open\" }, \"status\":500} 対応策123curl -XPOST https://&lt;Elasticsearch 5.5 Endpoint Domain&gt;/_snapshot/index-backups/20170926/_restore -d '{ \"indices\": \"nginx-*\"}' | jq . indices を用い、スナップショット内のインデックスの中からマッチする正規表現のみをリストアできます。 自身ではこの様な解決法を実施し回避できました。その他良い方法があれば御指南いただけますと幸いです。 ちなみに Terraform で ES 2.2 → 5.5 バージョンアップを実施した所1時間以上経過してようやくアップデートが完了しました。 1234aws_elasticsearch_domain.elasticsearch: Still destroying... (ID: arn:aws:es:ap-northeast-1:xxxxxxxxxxxx:domain/***, 59m11s elapsed)aws_elasticsearch_domain.elasticsearch: Still destroying... (ID: arn:aws:es:ap-northeast-1:xxxxxxxxxxxx:domain/***, 59m21s elapsed)aws_elasticsearch_domain.elasticsearch: Still destroying... (ID: arn:aws:es:ap-northeast-1:xxxxxxxxxxxx:domain/***, 59m31s elapsed)aws_elasticsearch_domain.elasticsearch: Destruction complete after 59m41s これは辛い (&gt;_&lt;) Terraform で管理している場合、スナップショットを取得したら aws console 上でドメイン削除した方が早い。 ブルーグリーン的に ES 5.5 作成して ES 2.2 から乗り換えてしばらく運用して問題なければ ES 2.2 を削除する方法が一番確実だなと思いました。","link":"/2017/10/01/2017-10-02-elasticsearch-service-version-up/"},{"title":"toda-tocochan-bus flask on IBM Bluemix へ引っ越し","text":"GCP から IBM Bluemix へ引っ越しました！ toco ちゃんバス あと何分？ 概要さくらVPS から GCP、そして今度は GCP から IBM Bluemix に引越ししました。 以前 GCP 運用時の話はコチラ GKEチュートリアルでサイト構築・運用概要以前さくらVPS上で tocoちゃんバスアプリを作成しました。 Flask Python3 で 戸田市 tocoちゃんバスあと何分？ Webアプリ作成♪ - 長生村本郷Engineers'BlogFlask(フラスク) とは Flask Official Site を参照する… GCP は GKE に LB かましたら価格がバコッと上がってしまい無料枠を逸脱してしまいました (&gt;_&lt;) なんとか低価格で運用したいという目論見です。 何故 Heroku でなく IBM Bluemix ？IBM Bluemix の良い所は機能が充実している所です。無料・デフォルトで kibana が見れます。 その他 Git との連携も可です。 以下 Mac で作業することを前提に手順まとめました。 事前準備 IBM Bluemix に Sign Up しときます Signup IBM Bluemix clone 12macOS%$ git clone https://github.com/kenzo0107/toda-tocochan-bus-on-ibmbluemixmacOS%$ cd toda-tocochan-bus-on-ibmbluemix cloudfoundry の CLI インストール 12macOS%$ brew tap cloudfoundry/tapmacOS%$ brew install cf-cli デプロイ123macOS%$ cf api https://api.ng.bluemix.netmacOS%$ cf loginmacOS%$ cf push &lt;application name&gt; API は Region によって変わります。 Region API URL 米国南部 https://api.ng.bluemix.net 英国 https://api.eu-gb.bluemix.net 総評Cloudfoundry の CLI のお陰で引っ越しも簡単でした♪ セキュリティとして特定 IP やドメインからアクセスさせないとか出来たら商用のメソッドとして利用出来そうかなと思いました。 その点質問してみましたが2週間ほど連絡がないので再度連絡してみます。↑質問は英語限定でした！ サポートが強化されると有難いなと思いました。 以上ご参考になれば幸いです。","link":"/2017/10/28/2017-10-29-toda-tocochan-bus-flask-on-ibm-bluemix/"},{"title":"iftop でネットワーク接続状況をリアルタイム監視","text":"iftop 概要CLI上で利用できるネットワークの接続状況をリアルモニタリングするツールです。→ ネットワークのボトルネックを特定する為に利用します。 単にネットワークのモニタリングであれば、モニタリングツールで良いですが 具体的にどこ（ドメイン・IP・ポート）にどれくらい（データ転送量）がわかります。 インストール方法 Ubuntu 1$ sudo apt-get install -y iftop CentOS 12$ sudo yum -y install epel-release$ sudo yum -y install iftop 使い方よく利用するのはこんな形です。※eth0 がない場合は -i eth0 を除いてください。 1$ sudo iftop -i eth0 -B -P -n -N -i インターフェース指定 -B 表示単位を Byte にする -P プロトコル or ポート表示 -n ドメインでなく ip で表示 -N プロトコルサービス名でなくポート番号で表示 表示項目=&gt; が送信、&lt;= が受信です 12345678910111213141516171819202122232425262728 24.4kB 48.8kB 73.2kB 97.7kB 122kB+--------------------------+---------------------------+---------------------------+---------------------------+---------------------------ip-10-13-1-101.ap-northeast-1.compute.internal:http =&gt; ip-10-13-100-41.ap-northeast-1.compute.internal:62635 559kB 121kB 67.1kB &lt;= 3.60kB 1.90kB 1.05kBip-10-13-1-101.ap-northeast-1.compute.internal:35244 =&gt; ip-10-13-102-56.ap-northeast-1.compute.internal:mysql 0B 2.18kB 1.21kB &lt;= 0B 23.1kB 12.8kBip-10-13-1-101.ap-northeast-1.compute.internal:35247 =&gt; ip-10-13-102-56.ap-northeast-1.compute.internal:mysql 0B 2.13kB 1.18kB &lt;= 0B 23.0kB 12.8kBip-10-13-1-101.ap-northeast-1.compute.internal:http =&gt; ip-10-13-0-231.ap-northeast-1.compute.internal:8239 0B 7.73kB 4.29kB &lt;= 0B 1.16kB 658Bip-10-13-1-101.ap-northeast-1.compute.internal:ssh =&gt; ip-10-13-0-11.ap-northeast-1.compute.internal:56320 612B 576B 522B &lt;= 26B 26B 32Bip-10-13-1-101.ap-northeast-1.compute.internal:http =&gt; ip-10-13-100-41.ap-northeast-1.compute.internal:62657 0B 49B 27B &lt;= 0B 92B 51Bip-10-13-1-101.ap-northeast-1.compute.internal:40069 =&gt; ip-10-13-103-247.ap-northeast-1.compute.internal:6379 0B 99B 55B &lt;= 0B 34B 19Bip-10-13-1-101.ap-northeast-1.compute.internal:40072 =&gt; ip-10-13-103-247.ap-northeast-1.compute.internal:6379 0B 99B 55B &lt;= 0B 34B 19Bip-10-13-1-101.ap-northeast-1.compute.internal:http =&gt; ip-10-13-100-73.ap-northeast-1.compute.internal:27698 0B 44B 25B &lt;= 0B 33B 18Bip-10-13-1-101.ap-northeast-1.compute.internal:53696 =&gt; ip-10-13-0-2.ap-northeast-1.compute.internal:domain 0B 21B 12B &lt;= 0B 31B 17Bip-10-13-1-101.ap-northeast-1.compute.internal:41975 =&gt; ip-10-13-0-2.ap-northeast-1.compute.internal:domain 0B 21B 12B &lt;= 0B 31B 17B-------------------------------------------------------------------------------------------------------------------------------------------TX: cum: 1.31MB peak: 560kB rates: 560kB 134kB 74.7kBRX: 505kB 117kB 3.69kB 49.8kB 28.1kBTOTAL: 1.81MB 564kB 564kB 184kB 103kB Item Value TX (Transmitter) 送信量 RX (Receiver) 受信量 TOTAL iftop 起動からの総量 cum 総量 peak 最大 右端3列 (各トラフィック, rates含む) 2秒、10秒、40秒の転送量平均値 ※TX,RX の 「X」 は省略しますよという意味 閲覧し続けると気になる処理があった時にはShift + P で一旦停止させます。 もう一度開始したい場合は Shift + P です。 実際のCLI以下見ていただくと白い帯グラフが左から伸びているのが見えるかと思います。この横棒が一番上のバーの目盛りに相応してぱっと見でどの程度かわかるのが便利です。 DB への接続を確かめるDB (MySQL) のデフォルトポート 3306 への送受信を調べたいとき 1$ sudo iftop -B -P -n -N -f &quot;port 3306&quot; 当然ながら受信の方が大きいです。 補足実際に負荷が高い時等、特定のインシデントがあった際に追記していこうと思います♪ Reference ミドルウェア性能検証の手引き Linux上でネットワークの帯域制限と遅延を設定する Linux TC (帯域制御、帯域保証) 設定ガイドライン","link":"/2017/11/07/2017-11-08-iftop/"},{"title":"Prometheus2.0 remote storage 検証","text":"いよいよ出ました Prometheus 2.0 ！ Announcing Prometheus 2.0 | Prometheus 先日モニタリング勉強会でも Paul Taylor さんの LT を拝聴させて頂きパフォーマンス向上とストレージフォーマット変更による圧縮・バックアップがしやすくなった等、良い話がたくさん出ていました。 Operating Prometheus 中でも最も期待していた機能が Remote Long-Term Storage、長期保存機能には歓喜しました♪ 1系以下では、短期間用と長期間用の Prometheus を別途用意する等、対策が必要で冗長な作りを余儀なくされたところがありましたが2.0 リリースでついに！ 早速試してみたく使用感をまとめました。 今回やりたかったことまとめ Prometheus 2.0 リリースに際して期待の長期保存機能 (Remote long-term storage) を早速試す！ 実際にローカル環境で構築してみて1系からの変更箇所を確認 DB 側にどんなデータが入るのか確認 システム概要あくまで使用感の検証をしたかったので docker-compose でお手軽に作れる環境にしました。 前提条件以下を Vagrant にインストール Ubuntu 16.04.3 LTS \\n \\l Docker version 17.09.0-ce, build afdb6d4 docker-compose version 1.12.0, build b31ff33 起動する Docker Container Prometheus 2.0.0 Node Exporter 0.15.1 AlertManager 0.9.1 cAdvisor 0.28.0 Prometheu Adapter PostgreSQL 9.6.3 Grafana 4.6.1 Nginx 1.13.6 Adminer 使い方以下手順通りです。 kenzo0107/vagrant-docker/tree/vagrant-docker-ubuntu16.04/docker/prometheus-grafana-on-ubuntu 123456789101112131415161718192021222324macOS%$ git clone https://github.com/kenzo0107/vagrant-dockermacOS%$ cd vagrant-dockermacOS%$ vagrant up// Install docker, docker-composemacOS%$ vagrant provisionmacOS%$ vagrant sshvagrant%$ cd /vagrant/prometheus-grafana-on-ubuntuvagrant%$ sudo docker-compose up -dName Command State Ports-------------------------------------------------------------------------------------------------------------------------------------adapter /prometheus-postgresql-ada ... Upadminer entrypoint.sh docker-php-e ... Up 8080/tcpalertmanager /bin/alertmanager -config. ... Up 9093/tcpcadvisor /usr/bin/cadvisor -logtost ... Up 8080/tcpgrafana /run.sh Up 3000/tcpnginx nginx -g daemon off; Up 0.0.0.0:18080-&gt;18080/tcp, 0.0.0.0:3000-&gt;3000/tcp, 80/tcp, 0.0.0.0:8080-&gt;8080/tcp, 0.0.0.0:9090-&gt;9090/tcpnode-exporter /bin/node_exporter Up 9100/tcppgsql docker-entrypoint.sh -csyn ... Up 5432/tcpprometheus /bin/prometheus --config.f ... Up 9090/tcp アクセスしてみるPrometheus http://192.168.35.101:9090. Grafana http://192.168.35.101:13000. ユーザアカウントが ./grafana/env にあります. 12GF_SECURITY_ADMIN_USER=admin-userGF_SECURITY_ADMIN_PASSWORD=admin-pass Datasource 設定 Datasource 設定フォームに以下情報を入力し Add ボタンをクリックします。 Item Value Name Prometheus Type Prometheus URL http://prometheus:9090 Access proxy Dashboard.json インポート グラフが表示されます。 cAdvisor http://192.168.35.101:8080. Adminer ログインフォームに以下情報を入力します。 Item Value Server pgsql Username prometheus Password password Database postgres PostgreSQL に保存されているメトリクス情報が確認できます。 PostgreSQL &gt;&gt; pgsql &gt;&gt; postgres &gt;&gt; prometheus &gt;&gt; Select: metrics AlertManager でアラート通知してみる例として node-exporter を停止 1vagrant%$ sudo docker-compose stop node-exporter ./alertmanager/config.yml で設定した Slack Channel にちゃんと通知がきました。 所感 2.0 になって設定の仕方が諸々変わり、公式サイトじっくり見る必要あります。 と思ったら、早速まとめ出てました！ありがとうございます！ Prometheus 2.0 の変更点と移行 今回は Prometheus ×1 台構成ですが、2台以上で冗長化する構成も試してみたい。 余談 バグなのか google/cadvisor で検出するメトリクスが重複表示されて grafana で絞るのに困りました。 Issue これ？ Inconsistent container metrics in prometheus route #1704 あとがきMackerel の様なマネージドな監視サービスで運用コストを削減する以上にPrometheus をマネージドすれば、さらにトータルコストを抑えられる様になる、と睨んでます。 ですが、Datadog は APM 付きプランも適度なコスト感で提供しておりマネージドサービスの魅力は尚大きいです。 モニタリングの棲み分けをできる様にするにも、選択肢の一つにするにも Prometheus 挑戦しがいがあるのでは？と思っています。 Prometheus、今後さらに広まることを期待しています。 参考 Configuration | Prometheus prometheus/config/testdata/conf.good.yml","link":"/2017/11/12/2017-11-13-prometheus-remote-storage/"},{"title":"terraform workspace で環境毎に tfsate 管理","text":"terraform workspace で環境毎に tfsate 管理した話です。 追記 2019/04/17追記時点で workspace は運用時点の問題が多くあった為、利用していません。以下記事ご参考いただければと思います。 概要Terraform tfstate の管理をかつて0.8 系では -backend-config でせっせと環境(stg,prod) 毎に bucket を変えて、なんてコードを見てきました。 ですが、workspace で 1つの bucket に 環境毎に保管できる様になりました。 厳密には環境毎でなくともリソースの集合毎、module 毎等で管理するイメージですが 今回はイメージを捉えやすく環境毎で分けました。 歴史 0.5 で S3 で管理、 &lt; 0.9 では、 remote config で管理場所を設定 = 0.9 では、terraform workspace で同一ディレクトリで複数のリソース群を管理 とより利用しやすくなりました。 前提以下条件とします。 tfstate は backend.tf で s3 管理 移行手順既存 terraform で tfstate 確認 想定の実行計画通りか確認します。 異なる場合は、そもそも現環境と差分が生じている、及び、tfstate が正しく取得できていない等、問題ありなのでそちらを修正します。 1$ terraform plan tfstate ファイル取得local に terraform.tfstate を取得します。中身を確認してリソースの設定がある程度問題ないか確認しておきます。 0.8 系 123456$ terraform remote config \\-backend=s3 \\-backend-config=&quot;bucket=tfstate.bucket&quot; \\-backend-config=&quot;key=terraform.tfstate&quot; \\-backend-config=&quot;region=ap-northeast-1&quot; \\-backend-config=&quot;profile=aws-hogehoge&quot; 0.9 系以降 1macOS%$ terraform state pull &gt; terraform.tfstate terraform 0.11.x (2017年12月現在最新) へバージョンアップHomebrew ならば upgrade で！ 1macOS%$ brew upgrade terraform state 管理を backent.tf で記述既にこの様に設定されている方はスキップです。特に普遍的な書き方です。 123456789terraform { backend &quot;s3&quot; { bucket = &quot;tfstate.bucket&quot; key = &quot;terraform.tfstate&quot; region = &quot;ap-northeast-1&quot; encrypt = true profile = &quot;aws-hogehoge&quot; }} Workspace 作成 Workspace stg 作成 1$ terraform workspace new stg workspace リスト一覧 123$ terraform workspace list default* stg tfstate を push1$ terraform state push -force .terraform/terraform.tfstate これで S3 tfstate.bucket の env:/stg/ ディレクトリ以下に terraform.tfstate が push されました。実際に S3 を見て確認してみてください。 env でなく env: なのが肝です。 実行計画確認1$ terraform plan 想定の実行計画通りか確認して問題なければ移行完了です。 おまけterraform を指定したバージョンで実行するにはone-off Container で実行できる様に Makefile でラップする、が今の所自分の中のベストプラクティスです。 これによって local 環境に依存せず指定したバージョンの terraform 実行が可能となります。 one-off Container とはone-off Container は Docker コンテナを run --rm で1度のコマンド実行の為だけに起動する手法です。 Makefile で Docker コマンドをラップしておくとTERRAFORM_VERSION を変更するだけで指定の terraform バージョンを利用できます。 以下は 0.11.1 の例です。 123456789101112131415161718192021222324252627TERRAFORM_VERSION=0.11.1DOCKER=docker run --rm -v ~/.ssh:/root/.ssh:ro -v ~/.aws:/root/.aws:ro -v ${PWD}:/work -w /work hashicorp/terraform:${TERRAFORM_VERSION}$(eval ENV := $(shell ${DOCKER} workspace show))ifeq (${ENV}, default)$(error select workspace ! ex: make init ENV=&lt;stg|prod&gt;)endifdefault: initinit: # tfstate ファイル初期化 ${DOCKER} init # workspace 作成. &quot;; true&quot; は既に作成済みエラーをスキップする為 ${DOCKER} workspace new ${ENV}; true # 作成した workspace を選択 ${DOCKER} workspace select ${ENV} # 作成した workspace の tfstate ファイルを同期 ${DOCKER} initplan: ${DOCKER} planapply ${DOCKER} apply -auto-approve make init ENV=stg 実行で以下まとめてました tfstate 初期化 workspace stg 作成 選択したworkspace の tfstate で初期化 きっとさらに素敵なベストプラクティスがあれば教えてください！ 参考になれば幸いです。","link":"/2017/12/04/2017-12-05-terraform-workspace-tfsate/"},{"title":"Datadog で Rails Unicorn の Memory, Idle|Busy Worker 監視 〜呉越同舟〜","text":"概要Rails の乗っているホストへ Datadog で Unicorn を監視しようとした所、それらしい Integration がありません((あったら教えてください &gt;_&lt; ))。 ということで独自スクリプトを作成しようと思いました！ 独自スクリプトを書こうとしてたら…同僚「Mackerel なら plugin ありますよ？」 自分「えっ？…」 Mackerel 入ってるMackerel に unicorn 監視用の plugin がありました。 mackerel-plugin-unicorn はてなさんもOSSで出して頂いている、車輪の再開発は時間の無駄、人生は一度しかないのでこの Mackerel プラグインを Datadog で使わせて頂こうと思いました。 Mackerel + Datadog 呉越同舟スクリプト /etc/dd-agent/unicorn_check.py 12345678910111213141516171819202122232425262728from checks import AgentCheckimport subprocessimport reclass UnicornCheck(AgentCheck): def check(self, instance): pidfile = instance['pidfile'] cmd = \"/usr/bin/mackerel-plugin-unicorn -pidfile=%s\" % (pidfile) res = self.exeCmdWithStripLF(cmd) for r in res: y = re.split(r'\\t+', r.rstrip('\\t')) metrics = y[0] out = y[1] self.gauge(metrics, out) # コマンド実行結果から改行コードから取り除く def exeCmdWithStripLF(self, cmd): res = self.exeCmd(cmd) return [str(x).rstrip(\"\\n\") for x in res] # コマンド実行 def exeCmd(self, cmd): return subprocess.Popen( cmd, stdout=subprocess.PIPE, shell=True ).stdout.readlines() /etc/dd-agent/conf.d/unicorn_check.yaml Unicorn の PID ファイルを指定します。 12345init_config:instances: - pidfile: /path/to/rails_project/shared/tmp/pids/unicorn.pid Datadog Agent 設定ファイルチェック123$ sudo dd-agent configcheckunicorn_check.yaml is valid Datadog Agent 再起動1$ sudo service datadog-agent restart 数分後グラフを見てみる出てきた！ 総評これで呉越同舟型モニタリングができました！ 自分自身が呉でも越でもない所に若干の背徳感がありますが手っ取り早く舟をこしらえたことに本記事の意味があるかと筆を取りました。 参考になれば幸いです。","link":"/2017/12/22/2017-12-23-monitoring-rails-unicorn-by-datadog-using-mackerel/"},{"title":"Datadog Agent 6系にアップデートして Logging 機能を試す！","text":"Datadog Agent 6系にアップデートして Logging 機能を試す！ 2017年末にβ版ですが、Datadog の Log 可視化ツールの利用が発表されました。 Unifying the views でグラフの高負荷時刻付近のログを参照する機能があったり Elasticsearch+Fluentd の代替として期待できそう と思い早速導入してみました。 datadog-agent インストール方法2018年1月10日時点では 5系がインストールされます。 5系、6系とで主に変わった点 Datadog 設定ファイルパス変更 5系 6系 ベースディレクトリ /etc/dd-agent /etc/datadog-agent 各種設定ファイル /etc/dd-agent/conf.d/nginx.yaml /etc/dd-agent/conf.d/nginx.d/conf.yaml メトリクス情報 dd-agent info datadog-agent status 6系では dd-agent コマンドがありませんでした。 dd-agent configcheck に該当するコマンドが見当たらない？どこにあるのか教えてください(;&gt;_&lt;) 5系からのアップグレード方法https://github.com/DataDog/datadog-agent/blob/master/docs/beta.md 自身の環境は Ubuntu 16.04.2 LTS だったので以下方法でアップグレードしました。 12345$ DD_UPGRADE=true bash -c \"$(curl -L https://raw.githubusercontent.com/DataDog/datadog-agent/master/cmd/agent/install_script.sh)\"...Error: /etc/datadog-agent/datadog.yaml seems to contain a valid configuration, run the command again with --force or -f to overwrite itAutomatic import failed, you can still try to manually run: datadog-agent import /etc/dd-agent /etc/datadog-agent Error と出るので一瞬ハッとしましたが、Error Message をよく見ると6系の /etc/datadog-agent/datadog.yaml は問題ない設定となっている様に見えますが、上書きしたい場合は –force を使ってね、とあります。 datadog-agent のアップグレードは無事完了していました。 123456789$ sudo datadog-agent statusGetting the status from the agent.===================Agent (v6.0.0-rc.2)===================...... また各種設定(/etc/datadog-agent/conf.d, checks.d)ファイルも問題なく移行できていました。 5系の設定ファイルを 6系へオーバーライド特に上記の手法で問題ないですが強制的にオーバーライドする方法を明記しておきます。 123456789101112131415// /etc/dd-agent/conf.d 以下のファイルを 6系へ移行$ /opt/datadog-agent/bin/agent/agent import /etc/dd-agent /etc/datadog-agent --forceSuccess: imported the contents of /etc/dd-agent/datadog.conf into /etc/datadog-agent/datadog.yamlCopied conf.d/http_check.yaml over the new http_check.d directoryCopied conf.d/network.yaml over the new network.d directoryCopied conf.d/nginx.yaml over the new nginx.d directoryCopied conf.d/process.yaml over the new process.d directoryCopied conf.d/process_check.yaml over the new process_check.d directoryCopied conf.d/ssl_check_expire_days.yaml over the new ssl_check_expire_days.d directoryCopied conf.d/unicorn_check.yaml over the new unicorn_check.d directoryError: unable to list auto_conf files from /etc/dd-agent: open /etc/dd-agent/conf.d/auto_conf: no such file or directory// /etc/dd-agent/checks.d/ 以下のファイルを 6系へ移行$ sudo -u dd-agent -- cp /etc/dd-agent/checks.d/*.py /etc/datadog-agent/checks.d/ nginx log を Logging へ送付 /etc/datadog-agent/conf.d/nginx.d/conf.yaml 1234567891011121314151617init_config:instances: - nginx_status_url: http://localhost/nginx_status/logs: - type: file service: hogehoge path: /var/log/nginx/access.log source: nginx sourcecategory: nginx_access - type: file service: hogehoge path: /var/log/nginx/error.log source: nginx sourcecategory: nginx_error 基本的に logs ディレクティブを記述することで OK /etc/datadog-agent/conf.d/fluentd.d/conf.yaml 123456789101112init_config:instances: - monitor_agent_url: http://localhost:24220/api/plugins.json tag_by: typelogs: - type: file service: hogehoge path: /var/log/td-agent/td-agent.log source: td-agent sourcecategory: td-agent datadog.conf 修正/etc/datadog-agent/datadog.yaml に以下設定を加えます。 1log_enabled: true 設定反映1$ sudo systemctl restart datadog-agent うまく Datadog に反映されないときはログを見てみます。 12345$ sudo tail -f /var/log/datadog/agent.log...2018-01-07 11:01:58 JST | INFO | (logs-agent.go:75 in func1) | open /var/log/nginx/access.log: permission denied... パーミッションエラーが発生しておりdatadog-agent を起動している dd-agent ユーザからアクセスできない状態となっていました。 対処単純に /var/log/nginx/access.log に 0644 (-rw-r–r–) を付与するだけでなく、logrotate で生成される新たな log のパーミッションにも注意します。 123456789101112131415161718/var/log/nginx/*.log { daily missingok rotate 14 compress delaycompress notifempty create 0644 www-data adm sharedscripts prerotate if [ -d /etc/logrotate.d/httpd-prerotate ]; then \\ run-parts /etc/logrotate.d/httpd-prerotate; \\ fi \\ endscript postrotate invoke-rc.d nginx rotate &gt;/dev/null 2&gt;&amp;1 endscript} 元々 0640 でしたが 0644 で生成するようにしました。これにて解決♪ Datadog Logging で確認ログが流れてくるのを確認できました。Kibana の Discover ページのような作りです。 今後フィルタリングしてグラフを作ったりできたりしてくるのか、Pro版なら無料で使わせてもらえないかな、なんて期待が高まっております お願い、Datadog さん(-人-)","link":"/2018/01/09/2018-01-10-update-datadog-and-try-loggin-feature/"},{"title":"Linux に rbenv をセットアップして ruby バージョンを切り替える","text":"概要サーバの ruby のバージョンが古かった為、rbenv で ruby のバージョンを切り替える様にした際の設定メモです。 setup rbenv123456789$ git clone https://github.com/sstephenson/rbenv.git ~/.rbenv$ git clone https://github.com/sstephenson/ruby-build.git ~/.rbenv/plugins/ruby-build$ echo 'export PATH=\"$HOME/.rbenv/bin:$PATH\"' &gt;&gt; ~/.bash_profile$ echo 'eval \"$(rbenv init -)\"' &gt;&gt; ~/.bash_profile$ source ~/.bash_profile$ rbenv --versionrbenv 1.1.1-30-gc8ba27f rbenv 経由で Ruby 2.5.0 インストール1234567891011121314151617181920$ rbenv install 2.5.0// 現 version は system. まだ 2.5.0 に切り替わっていない$ rbenv versions* system (set by /home/vagrant/.rbenv/version) 2.5.0// 2.5.0 へ切り替え$ rbenv global 2.5.0// 切り替え確認$ rbenv versions system* 2.5.0 (set by /home/vagrant/.rbenv/version)$ ruby -vruby 2.5.0p0 (2017-12-25 revision 61468) [x86_64-linux]// リフレッシュしないと .rbenv/versions/2.5.0/bin 以下のパスを通らない$ rbenv rehash","link":"/2018/03/06/2018-03-07-change-ruby-version-via-rbenv/"},{"title":"ECR にログイン(aws ecr get-login)無しでプッシュする","text":"概要Docker version 1.11 で実装された credential-helper を利用しECR へのプッシュを安全に簡易的に行う仕組みを実装します。 Docker ver 1.11 以上にアップグレード1234567$ sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D$ sudo sh -c \"echo deb https://apt.dockerproject.org/repo ubuntu-trusty main\\&gt; /etc/apt/sources.list.d/docker.list\"$ sudo apt-get purge lxc-docker docker$ sudo apt-get update$ sudo apt-get install docker-engine$ sudo service docker restart pull Dockerized ECR credential helper1$ docker pull pottava/amazon-ecr-credential-helper 認証設定以下3つの中から1つ利用ください。EC2 であれば、1. インスタンスロールで認証 が一番すっきりしていてコードの見通しが良いです。 インスタンスロールで認証 credential で認証 環境変数で認証 1. インスタンスロールで認証123docker run --rm \\ -e REGISTRY=123457689012.dkr.ecr.us-east-1.amazonaws.com \\ pottava/amazon-ecr-credential-helper 12345678910sudo sh -c 'cat &lt;&lt; EOF &gt; /usr/bin/docker-credential-ecr-login#!/bin/shSECRET=\\$(docker run --rm \\\\ -e METHOD=\\$1 \\\\ -e REGISTRY=\\$(cat -) \\\\ pottava/amazon-ecr-credential-helper)echo \\$SECRET | grep SecretEOF'sudo chmod +x /usr/bin/docker-credential-ecr-login 2. credential で認証1234docker run --rm \\ -e REGISTRY=123457689012.dkr.ecr.us-east-1.amazonaws.com \\ -v $HOME/.aws/credentials:/root/.aws/credentials \\ pottava/amazon-ecr-credential-helper 1234567891011sudo sh -c 'cat &lt;&lt; EOF &gt; /usr/bin/docker-credential-ecr-login#!/bin/shSECRET=\\$(docker run --rm \\\\ -e METHOD=\\$1 \\\\ -e REGISTRY=\\$(cat -) \\\\ -v $HOME/.aws/credentials:/root/.aws/credentials \\\\ pottava/amazon-ecr-credential-helper)echo \\$SECRET | grep SecretEOF'sudo chmod +x /usr/bin/docker-credential-ecr-login 3. 環境変数で認証 環境変数セット 12export AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLEexport AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY 12345docker run --rm \\ -e REGISTRY=123457689012.dkr.ecr.us-east-1.amazonaws.com \\ -e AWS_ACCESS_KEY_ID \\ -e AWS_SECRET_ACCESS_KEY \\ pottava/amazon-ecr-credential-helper 1234567891011sudo sh -c 'cat &lt;&lt; EOF &gt; /usr/bin/docker-credential-ecr-login#!/bin/shSECRET=\\$(docker run --rm \\\\ -e METHOD=\\$1 \\\\ -e REGISTRY=\\$(cat -) \\\\ -e AWS_ACCESS_KEY_ID \\\\ -e AWS_SECRET_ACCESS_KEY \\\\ pottava/amazon-ecr-credential-helper)echo \\$SECRET | grep SecretEOF'sudo chmod +x /usr/bin/docker-credential-ecr-login credential 保存設定1234567mv $HOME/.docker/config.json $HOME/.docker/config.json.orgcat &lt;&lt; EOF &gt; $HOME/.docker/config.json{ \"credsStore\": \"ecr-login\"}EOF これで aws ecr get-login から解放されます♪","link":"/2018/03/06/2018-03-07-push-to-ecr-without-login-ecr/"},{"title":"docker build 時に Text file busy で shell が実行できない対策","text":"概要Dockerfile 内に以下のように shell の実行を記述していました。 12RUN chmod +x hoge.sh \\ &amp;&amp; hoge.sh 上記記述のある状態で docker build 実行した所、以下のようなエラーに遭遇しました。 1/bin/sh: hoge.sh: Text file busy What is Text file busy ?書き込みのために現在開いている手続きのみの (共用テキスト) ファイルを実行しようとした場合や、実行中の手続きのみのファイルを書き込みのために開こうとしたり、削除しようとしたりする場合に発生します。 上記鑑みるとchmod +x hoge.sh 実行中に hoge.sh を実行しようとしたが為に発生しているということ？？と推測。 環境情報 Ubuntu 14.04.5 LTS \\n \\l Docker version 17.05.0-ce, build 89658be Base Image: ruby:2.5-alpine 対策以下 sync 処理を追加し無事問題解決できました。 123RUN chmod +x hoge.sh \\ &amp;&amp; sync \\ &amp;&amp; hoge.sh What is sync command ?sync - システム管理コマンドの説明 - Linux コマンド集 一覧表sync - システム管理コマンドの説明。sync - ディスク上のデータをメモリと同期させる。 参考 cpとmvとinodeの話 - Qiita実行中のファイルに対して、 cpで上書きする場合だと、Text file busyで置き換えられず、 mvだと何も聞かれず置き換えられます。 この挙動の違いについて、まとめと実験。 inodeを見ると、それとなく分かります。 h...","link":"/2018/04/17/2018-04-18-fix-text-file-busy-on-docker-build/"},{"title":"curl で FTPS (File Transfer Protocol over SSL&#x2F;TLS) 接続確認","text":"以下コマンドで FTPS 接続確認ができます。 1curl -u &lt;user&gt; --ftp-ssl -k ftp://&lt;ftp domain&gt;/ 概要備忘録記事です。 社外向けに FTPS で接続許可をする必要があり設定しました。 単純に作成・更新した user, password で認証をパスできるか、の確認だけができれば良いので、その確認方法を模索している時に程よいコマンドがありました。 その接続確認を FileZilla, Cyberduck でしましたがどうもうまくいかず。。 改めて、 lftp とか色々 ftp だけでもコマンドは多々あるんだなと実感しました。","link":"/2018/04/17/2018-04-18-ftps-by-curl/"},{"title":"続　ECR にログイン(aws ecr get-login)無しでプッシュする","text":"概要前回 ECR への Docker イメージをプッシュする際の認証コマンドを実行せずにプッシュできる様にしました。 ECR にログイン(aws ecr get-login)無しでプッシュする ですが、設定が手間というのがあり、CircleCI, AWS CodeBuild 等でワンライナーでささっと書きたいときには不便です。 解決awscli profile で設定した profile を利用し ecs-cli を利用することで認証をよろしくやってくれます。 1ecs-cli push &lt;image&gt; --aws-profile &lt;profile&gt; --region &lt;region&gt; 設定 Step123aws configure set --profile hogehoge aws_access_key_id $ACCESS_KEY_IDaws configure set --profile hogehoge aws_secret_access_key $SECRET_ACCESS_KEYaws configure set --profile hogehoge region ap-northeast-1 123ecs-cli push 123456789012.dkr.ecr.ap-northeast-1.amazonaws.com/stg-mogemoge-rails:latest \\ --aws-profile hogehoge \\ --region ap-northeast-1 以上で aws ecr get-login を使用せず、ECR へプッシュができる様になりました♪","link":"/2018/05/09/2018-05-10-ecr-nologin-push/"},{"title":"AWS Vault で複数アカウントにMFA認証通過","text":"AWS Vault とは？AWS Vault は IAMの認証情報 (Access Key Id, Secret Access Key) を安全に OS のキーストアに保存しアクセスできる仕組みを提供するツールです。 Vault = 金庫 というだけあってPC 落としても秘匿情報が漏れにくい仕組みにしてくれます。 今回の目的AWS Vault で複数アカウントのコンソールログインを簡単にしたいと思います。 AWS IAM を頂く際に MFA 設定をしているかと思います。 デバイス認証することでセキュアなアカウントを管理をする為です。 その際に、以下の様なアプリをインストールし1分置きに更新される 6桁 の数字で認証する仕組みにします。 AppStoreitunes.apple.com/jp/app/google-authenticator/id388497605 GooglePlayplay.google.com/store/apps/details?id=com.google.android.apps.authenticator2&hl=ja MFA 自体はその会社の開発ポリシーにも依りますが、入れておいて損なしの仕組みです。 ただ、毎回 6桁の数字をコピぺするのは面倒です。 それを簡易的に認証通過する様にしました。 aws-vault インストール1macOS%$ brew cask install aws-vault profile 設定123aws-vault add &lt;profile&gt;Enter Access Key ID: &lt;Access Key ID 入力&gt;Enter Secret Access Key: &lt;Secret Access Key 入力&gt; 事前準備12brew tap peco/pecobrew install peco bash 設定ご利用の .bashrc, .zshrc 等に設定してください。 123456789101112function peco-login-aws-account() { local account=$(aws-vault ls | awk 'NR&gt;2 {if ($2 != \"-\") print $2}' | peco) aws-vault login $account}function peco-aws-exec() { local account=$(aws-vault ls | awk 'NR&gt;2 {if ($2 != \"-\") print $2}' | peco) echo -e \"aws-vault exec \\\"$account\\\" -- \\\\\" | pbcopy}alias avl='peco-login-aws-account'alias ave='peco-aws-exec' 使い方 設定した profile を選択して Login 1avl 設定した profile を選択してコマンド実行 1ave 実際のコマンドを rec して出すのは憚れる内容でしたので貼り付けられず汗 是非一度利用し使用感を試してみてください♪ 毎回パスワード入力というのも辛いところではありますがMFA の手間に比べれば数段楽です。 AWS Vault は本来はそういう意図ではないと思いますが楽になるならば良し♪","link":"/2018/05/13/2018-05-14-aws-vault-mfa/"},{"title":"AWS ECS prefix 指定してまとめてタスク登録解除","text":"概要awscli でタスク定義のまとめて登録解除がなかったので簡単に Shell 化しました 実行1sh deregister_all_tasks_filtered_by_family_prefix.sh &lt;profile&gt; &lt;task definition family prefix&gt; サクッと削除","link":"/2018/05/17/2018-05-18-ecs_prefix/"},{"title":"Rails × Redis でスレッドセーフなアクセス数ランキング実装","text":"概要メディアサイトで記事ページへアクセス数ランキングを実装しました。 Rails 5.1 Redis (AWS ElastiCache 3.2.10) その際にマルチスレッド環境を考慮してスレッドセーフな実装を心がけました。 スレッドセーフとはスレッドセーフとは複数のスレッドが同時並行的に実行しても問題が発生しないことを意味します。スレッドセーフでない場合は、あるスレッドで変更した共有データが、他のスレッドによって上書きされてしまう可能性があります。 Webサーバーやデータベースなどのサーバー用ソフトウェアは、マルチスレッド（マルチプロセス）で動作しているので、サーバー向けアプリケーションを開発するときは、マルチスレッドで動作するように実装することが望ましいです。 参照スレッドセーフ JavaのThreadLocalとスレッドセーフについて 仕様メディアサイトで記事詳細ページへアクセスした際にその記事 ID に対して閲覧数を +1 インクリメントします。 そして、その閲覧数 TOP 10 のランキングを表示する、というものです。 その際の Rails, Redis の設定についてまとめました。 実装方法検討config/initializers/redis.rb((host, port は secrets.yml なり ENV で設定してください)) で Redis の初期設定の実装方法を検討しました。 global 変数として設定123require 'redis'REDIS = Redis.new(host: host, port: port) 上記の場合、グローバルで Redis クライアントを持っておりマルチスレッド環境では、複数のスレッドが上書きされる可能性があります。 Thread.current12345require 'redis'def redis Thread.current[:redis] ||= Redis.new(host: host, port: port)end 現在実行中のスレッドを取得しスレッド毎のデータを担保します。 が、以下 2 点の問題があります。 他人が上書いてしまう 構造化されていない ActiveSupport::PerThreadRegistry12345678910require 'redis'class RedisRegistry extend ActiveSupport::PerThreadRegistry attr_accessor :redis, :current_permissionsenddef redis RedisRegistry.redis ||= Redis.new(host: host, port: port)end redis をスレッドローカル変数として定義し、そのアクセスをカプセル化し上書きされるのを防止しています。 ですが、Rails 5.2 で deprecated となっておりました (TへT) thread_mattr_accessor以下を見てみると thread_mattr_accessor の挙動が Fix していました。Fix thread_mattr_accessor share variable superclass with subclass thread_mattr_accessor を利用して書き換えます。 config/initializers/redis.rb 123456789require 'redis'class RedisRegistry thread_mattr_accessor :redisenddef redis RedisRegistry.redis ||= Redis.new(host: host, port: port)end アクセス数インクリメントrescue 設定は Redis に接続できなくなった場合でもサイト自体が落ちることはなく、ランキングだけが表示されなくなる様にする為に設定しました。 12345def increment_access_count(id) redis.zincrby \"entries/daily/#{Time.zone.today}\", 1, idrescue SocketError =&gt; e logger.error eend アクセスランキング取得Redis の zrevrangebyscore によりスコアの大きい順に 10 個、ID を取得します。もし取得できなかった場合、 [] を返します。decorate で体良く整形して View に渡します。 ((decorate のコードは省略してます)) 123456789101112def access_ranking limit = 10 ids = redis.zrevrangebyscore \"entries/daily/#{Time.zone.today}\", '+inf', 0, limit: [0, limit] if ids.any? where(id: ids).order(['field(id, ?)', ids]).limit(limit).decorate else [] endrescue SocketError =&gt; e logger.error e []end 以上です。","link":"/2018/06/05/2018-06-06-rails-redis-threadsafe/"},{"title":"ecs-cli バージョン指定してインストール","text":"完全な備忘録です。 経緯6月に入って数日、ecs-cli の latest をインストールすると latest が 1.6.0 となりecs-cli compose ... を実行すると以下のようなエラーが出るようになりました。 1level=error msg=\"Unable to open ECS Compose Project\" error=\"Volume driver is not supported\" 1.4.0 では問題なかったタスク定義でしたが1.6.0 では Volume driver is not supported となったそうで処理がこけるようになりました。 その対応として 1.4.0 にバージョン固定した設定です。 結論latest の部分を &lt;version&gt; に変更することで指定のバージョンでインストールが可能です。 for Linux OS 1curl -s https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-linux-amd64-v1.6.0 -o /usr/bin/ecs-cli &amp;&amp; chmod +x /usr/bin/ecs-cli for MacOS 1curl -s https://amazon-ecs-cli.s3.amazonaws.com/ecs-cli-darwin-amd64-v1.6.0 -o /usr/bin/ecs-cli &amp;&amp; chmod +x /usr/bin/ecs-cli AWS Documentation では最新の ecs-cli のみの説明の為、若干ハマりました。 GitHub には Download specific version とあるのでこちらの情報を追っておくと良かったです。 ecs-cli - Download specific version 以上本当に備忘録でした。","link":"/2018/06/05/2018-06-06-specify-ecscli-version/"},{"title":"ElastiCache メンテナンス対応 ~2018年梅雨~","text":"2018年6月頃 AWS ElastiCache のメンテナンス通知が大量発生した時の備忘録です。 メンテ時に参照したリンクRedis 用 Amazon ElastiCache 基本、ノードのリプレイスが必要です。 以下、手動実行する時の手順です。 node = 1 の場合スタンドアローンな ElastiCache の場合、 バックアップをとる リードレプリカを追加 レプリカを昇格しプライマリにする レプリカとなった元ノードを削除 これでノードのリプレイスが完了しました。 node = N (&gt;=2)複数ノードある場合、 バックアップをとる フェールオーバー実施 AWS Support 曰く、 フェールオーバー API は障害をシミュレートするので、フェイルオーバー後にノードの置き換えも行われます。 とのこと メンテナンス完了の確認方法正確にステータス確認するにはサポートに確認する以外はないかなと思います。 イベントには操作ログが残りメンテが実施されたというログは一切残りません。 また、アラートもすぐさま消えません。正確にはわかりませんが、数時間程度経過したら消えていました。 手動実行せず放っておいた場合メンテナンスはメンテナンスウィンドウで指定した時間帯に実施されました。 ちなみに、放っておいた時のイベントログは以下のようになっていました。 n = 1 n = 2 n= 2 でフェールオーバーが自動実行されているのがわかります。 メンテナンス自体の経過時間はデータ量にもよるので一概には言えませんので本番前に一度リハーサルして概算とっておくのが良いかなと思います。 以上ご参考になれば幸いです。","link":"/2018/06/23/2018-06-24-maintenance_aws_elasticache/"},{"title":"Docker 不要リソースお掃除 compose","text":"概要ECS EC2 で一部コンテナが 起動開始→失敗→起動開始→失敗 を繰り返しサーバが容量不足 no space left に陥る事象がありました。 その時の対応をまとめました。 docker 不要リソース削除docker 不要リソース削除処理コマンドは以下の様なものを実行します。 12345678// コンテナ削除$ docker ps -aq | xargs docker rm// イメージ削除$ docker images -aq | xargs docker rmi// タグ無しイメージ一括削除$ docker volume ls -qf dangling=true | xargs docker volume rm dangling … ぶら下がる、 ぶらぶら揺れる Spotify のお掃除イメージを使う12$ docker run --rm -v /var/run/docker.sock:/var/run/docker.sock -v /etc:/etc spotify/docker-gc$ docker run --rm -v /var/run/docker.sock:/var/run/docker.sock -v /var/lib/docker:/var/lib/docker martin/docker-cleanup-volumes docker-compose.yml にしてみるdocker-compose.yml フォーマットにすることでイメージ管理が容易になります。 1234567891011121314version: '2'services: docker-gc: image: spotify/docker-gc volumes: - /var/run/docker.sock:/var/run/docker.sock - /etc:/etc docker-cleanup-volumes: image: martin/docker-cleanup-volumes volumes: - /var/run/docker.sock:/var/run/docker.sock - /var/lib/docker:/var/lib/docker ECS EC2 で one-off Container 実行してお掃除12345678ecs-cli compose \\ --debug \\ --project-name &lt;project name&gt; \\ --file docker-compose.yml \\ run \\ --cluster &lt;cluster name&gt; \\ --region ap-northeast-1 \\ --aws-profile &lt;profile&gt; ToDo初めて利用する Public Container に対しての脆弱性をチェックする仕組みを導入する必要があるかと思います。この辺りまとめてまた執筆したいと思います。","link":"/2018/07/10/2018-07-11-cleanup_docker_no_required_resource/"},{"title":"Datadog NTP 監視でアラート鳴りまくり対応","text":"概要サーバ時刻の監視を Datadog で実施する際、標準時刻の参照先が異なることで不要なアラートが発生する事象がありました。 Datadog はデフォルトで pool.ntp.org を参照しています。 AWS EC2 に設定した Chrony ではデフォルトで ntp.nict.jp を参照する様にしていた為、ある日突然アラートがなりまくる事象がありました。 この対策として、 Datadog と Chrony の参照先を統一して管理する様に設定しました。 タイムサーバホストを統一する今回は、AWS を利用しており、 AWS にも NTP サーバがある為、そちらを参照することとしました。 AWS Time Sync Service のホストは 169.254.169.123 です。 169.254.169.123 のリンクローカル IP アドレスを介してアクセス可能な為、プライベートサブネットからでもアクセス可能です。ip アドレスという辺りがある日変更されたとかあると辛いので怖いですが、今の所、そういうことはないです。 /etc/datadog-agent/conf.d/ntp.d/conf.yaml 12345init_config:instances: - offset_threshold: 60 host: 169.254.169.123 # 追加 /etc/chrony/chrony.conf 12# server ntp.nict.jp minpoll 4 maxpoll 4 # コメントアウトserver 169.254.169.123 prefer iburst # 追加 上記設定後、リスタート 12$ sudo systemctl restart chrony$ sudo systemctl restart datadog-agent 上記によりアラート解消されました。 参照 Amazon Time Sync Service で時間を維持する どうやったらpool.ntp.orgを利用出来るのでしょうか?","link":"/2018/07/29/2018-07-30-datadog_ntp_alert/"},{"title":"顔検出 3分クッキング on MacOSX","text":"概要ラズパイ使って家族と判断したら「こんにちはご主人様」家族以外なら「通報しまーす」と話してくれるおもちゃを作ろうと思ってます。 その前段の前段として静止画で顔検出してみます。 ちなみに顔検出と顔認識は意味が全く異なります。 顔検出 … 顔部分を検出すること。 顔認識 … 特定の人物の顔と判断すること。 環境構築は前回記事を参照してください。 MacOSX に Python2, Python3 仮想環境構築 - 長生村本郷Engineers'Blog経緯 MacOSX デフォルトでは python 2系。 python 2.7 は 2020年までのサポート なので python 3 系 に慣れておこうということで 3 系環境を構築しようと思いました。 ですが dlib など Python 2系でないとうまく設定ができなかった… 環境 MacOSX 10.11.5 Python 3 CV2 インストール homebrew で cv2 インストール 12$ brew tap homebrew/science$ brew install opencv3 --with-python3 Python 3 を利用する 12$ source ~/py3env/bin/active(py3env)$ cv2.so を pip の site-package へコピー 12(py3env)$ cd ~/py3env/lib/python3.4/site-packages(py3env)$ cp /usr/local/Cellar/opencv3/3.1.0_3/lib/python3.4/site-packages/cv2.so . cv2 import 確認 Version が表示されれば成功 12(py3env)$ python -c 'import cv2; print(cv2.__version__)'3.1.0 スクリプトインストール123(py3env)$ cd ~/py3env(py3env)$ git clone https://gist.github.com/kenzo0107/5d174797a5a222295b5a39f6fa435777(py3env)$ cp ./5d174797a5a222295b5a39f6fa435777/trimming.py . スクリプト実行1(py3env)$ python trimming.py &lt;img_path&gt; Before After 革パンも顔認識されてしまう… が、一応まずできました。 今回のスクリプト要点 正確な検知数を向上させるべく、スクリプト上の以下 detectMultiScale メソッドのパラメータを調整します。 1facerect = cascade.detectMultiScale(image_gray, scaleFactor=1.02, minNeighbors=3, minSize=(7,7)) Item Value scaleFactor 画像解析する際に随時縮小し解析するその尺度 minNeighbors 最小近傍矩形数 minSize 顔部分を認識するためのサイズ (縦,横) まず最初の第一歩ができました♪","link":"/2018/07/31/2018-08-01-face-detector/"},{"title":"子供の笑顔と笑い声を聞く為に ffmpeg + Nginx + RTMP on RaspberryPI","text":"概要RaspberryPI 上で rtmp モジュール付きの nginx をビルドし WebCamera で撮影した 動画+音声付き を HLS 配信する際の手順をまとめました。 経緯はじめは外出中にペットのうさぎ用に mjpeg-streamer でモニターしていました。子供が生まれると、子供が元気にしてるかな、とふとモニターするようになりました。 ですが、うさぎは鳴きませんが、子供は泣き叫びます。mjpeg-streamer では表情こそわかりますが、我が子の声が聞こえてきません。 その為、動画＋音声付きで低負荷で 動画＋音声 配信ができないものかと探していた際に ffmpeg に出会いました。*1 購入したものRaspberry Pi 3 Model B V1.2 (日本製) 国内正規代理店品出版社/メーカー: Raspberry Pi発売日: 2016/02/29メディア: Tools & Hardwareこの商品を含むブログを見る Punasi Raspberry Pi?源 5V 3A 1.8m Raspberry Pi 3 2Model B/B+ Pi A/A+ Zero Micro USB Androidタブレット スマートフォン出版社/メーカー: Punasiメディア: エレクトロニクスこの商品を含むブログを見る LOGICOOL HDウェブカム フルHD動画対応 C615出版社/メーカー: ロジクール発売日: 2011/09/22メディア: Personal Computers購入: 2人 クリック: 2回この商品を含むブログを見る Kinobo???USB 2.0ミニマイクマイク\" Makio\"ノートパソコン/デスクトップPC用???VoIP/Skype/音声認識ソフトウェア出版社/メーカー: Kinoboメディア: Personal Computersこの商品を含むブログを見る Nginxhttps://nginx.org/en/download.html で現時点で最新バージョンをダウンロードしました。 各種ダウンロード以下ダウンロードしています。 `nginx-http-auth-digest` は Nginx で Digest 認証すべくモジュール追加しました。 Nginx nginx-rtmp-module openssl nginx-http-auth-digest ffmpeg // home ディレクトリで作業するとします。 pi$ cd ~ // nginx pi$ wget https://nginx.org/download/nginx-1.15.2.tar.gz // nginx-rtmp-module pi$ wget -O rtmp.zip https://github.com/arut/nginx-rtmp-module/archive/master.zip pi$ wget -O ssl.zip https://github.com/openssl/openssl/archive/master.zip // nginx-http-auth-digest pi$ git clone https://github.com/samizdatco/nginx-http-auth-digest.git pi$ cd nginx-http-auth-digest pi$ git clone https://gist.github.com/frah/3921741 pi$ patch -u &lt; 3921741/patch-ngx_http_auth_digest_module.diff // ffmpeg pi$ git clone git://source.ffmpeg.org/ffmpeg.git pi$ wget ftp://ftp.alsa-project.org/pub/lib/alsa-lib-1.1.6.tar.bz2 解凍pi$ tar xvzf nginx-1.15.2.tar.gz pi$ unzip rtmp.zip pi$ unzip ssl.zip pi$ tar xjvf alsa-lib-1.1.6.tar.bz2 Nginx ビルドpi$ cd nginx-1.15.2/ pi$ sudo ./configure --with-http_ssl_module --with-http_realip_module --add-module=../nginx-rtmp-module-master --with-openssl=../openssl-master --add-module=../nginx-http-auth-digest pi$ sudo make pi$ sudo make install Nginx Version 確認pi$ /usr/local/nginx/sbin/nginx -V nginx version: nginx/1.15.2 built by gcc 4.9.2 (Raspbian 4.9.2-10+deb8u1) built with OpenSSL 1.1.1-pre9-dev xx XXX xxxx TLS SNI support enabled configure arguments: --with-http_ssl_module --with-http_realip_module --add-module=../nginx-rtmp-module-master --with-openssl=../openssl-master --add-module=../nginx-http-auth-digest Nginx をシンボリックリンクでパスが通っている場所から参照できるようにする。pi$ sudo ln -s /usr/local/nginx/sbin/nginx /usr/bin/nginx pi$ which nginx ffmpeg ビルドpi$ sudo apt-get install libomxil-bellagio-dev pi$ cd alsa-lib-1.1.6 pi$ ./configure --prefix=/home/pi/ffmpeg pi$ sudo make pi$ sudo make install pi$ cd /home/pi/ffmpeg pi$ sudo ./configure --enable-gpl --enable-nonfree --enable-mmal --enable-omx-rpi --enable-omx --extra-cflags=\"-I/home/pi/ffmpeg/include\" --extra-ldflags=\"-L/home/pi/ffmpeg/lib\" --extra-libs=-ldl pi$ sudo make -j4 pi4 sudo make install `sudo apt-get install libomxil-bellagio-dev` を実行していない場合に以下のエラーが出ました。 ERROR: OMX_Core.h not found 録音してみる動画撮影デバイス一覧確認pi$ v4l2-ctl --list-device HD Webcam C615 (usb-3f980000.usb-1.3): /dev/video0 上記コマンド実行時に以下のようなエラーが出る時は、 Failed to open /dev/video0: No such file or directory 以下コマンドを試してください。 pi$ sudo pkill /dev/video0 音声入力デバイス一覧確認自分の場合は カード 1 は WebCam、 カード 2 は マイク、 です。 pi$ arecord -l **** ハードウェアデバイス CAPTURE のリスト **** カード 1: C615 [HD Webcam C615], デバイス 0: USB Audio [USB Audio] サブデバイス: 1/1 サブデバイス #0: subdevice #0 カード 2: Device [USB PnP Sound Device], デバイス 0: USB Audio [USB Audio] サブデバイス: 1/1 サブデバイス #0: subdevice #0 いざ録音カード 2 が入力デバイスである為、 `hw:2` としました。 pi$ ffmpeg -f alsa -ac 1 -i hw:2 -f v4l2 -s 640x480 -i /dev/video0 output.mpg 生成された output.mpg ファイルを mac 上にダウンロードし再生を試してみてください。 これで再生されれば、ffmpeg が問題なく動作していることを確認できたことになります。 次は配信する為の設定です。 Nginx 設定設定ファイル格納用ディレクトリ作成pi$ sudo mkdir -p /usr/local/nginx/conf.d HLS ファイル生成用ディレクトリ作成pi$ sudo mkdir -p /var/www/html/live/hls HLS 配信用 index.html hls.min.js 取得 pi$ cd /var/www/html pi$ wget https://cdn.jsdelivr.net/hls.js/latest/hls.min.js /var/www/html/index.html &lt;!DOCTYPE html&gt; &lt;html lang=\"ja\"&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"/&gt; &lt;script src=\"./hls.min.js\"&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;video id=\"video\"&gt;&lt;/video&gt; &lt;script&gt; if(Hls.isSupported()) { var video = document.getElementById('video'); var hls = new Hls(); hls.loadSource('/live/hls/stream.m3u8'); hls.attachMedia(video); hls.on(Hls.Events.MANIFEST_PARSED,function() { video.play(); }); } &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; Digest 認証設定pi$ cd /var/www pi$ sudo htdigest -c .htdigest 'digest AuthNginx' hoge password: &lt;enter password&gt; 各種設定ファイル配置 /usr/local/nginx/conf.d/default.conf server { listen 8090; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; access_log /var/log/nginx/access.log combined; error_log /var/log/nginx/error.log warn; location = /favicon.ico { access_log off; empty_gif; expires 30d; } location / { auth_digest \"digest AuthNginx\"; auth_digest_user_file /var/www/.htdigest; root /var/www/html; index index.html; set_real_ip_from 127.0.0.1; real_ip_header X-Forwarded-For; } } /usr/local/nginx/conf.d/rtmp rtmp { server { listen 1935; chunk_size 4096; allow play all; access_log /var/log/nginx/rtmp_access.log; application live { live on; hls on; record off; hls_path /var/www/html/live/hls; hls_fragment 1s; hls_type live; } } } /usr/local/nginx/conf/nginx.conf user www-data; worker_processes 1; pid /var/run/nginx.pid; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; include /usr/local/nginx/conf.d/*.conf; } include /usr/local/nginx/conf.d/rtmp; Nginx 起動設定ファイル /lib/systemd/system/nginx.service [Unit] Description=The NGINX HTTP and reverse proxy server After=syslog.target network.target remote-fs.target nss-lookup.target [Service] Type=forking PIDFile=/var/run/nginx.pid ExecStartPre=/usr/local/nginx/sbin/nginx -t ExecStart=/usr/local/nginx/sbin/nginx ExecReload=/usr/local/nginx/sbin/nginx -s reload ExecStop=/bin/kill -s QUIT $MAINPID PrivateTmp=true [Install] WantedBy=multi-user.target Nginx 起動pi$ sudo systemctl daemon-reload pi$ sudo systemctl start nginx pi$ sudo systemctl status nginx この時はまだ HLS ファイルが生成されていませんので `https://&lt;RaspberryPI IP&gt;:8090` にアクセスしても HLS 配信されていません。 ffmpeg を起動することで `/var/www/html/live/hls/` ディレクトリ以下に `stream.m3u8` が生成されます。 ffmpeg 起動pi$ sudo ffmpeg \\ -f alsa -ac 1 -thread_queue_size 8192 -i hw:2 \\ -f v4l2 -thread_queue_size 8192 -input_format yuyv422 -video_size 432x240 -framerate 30 -i /dev/video0 \\ -c:v h264_omx -b:v 768k -bufsize 768k -vsync 1 -g 16 \\ -c:a aac -b:a 128k -ar 44100 \\ -af \"volume=30dB\" \\ -f flv rtmp://localhost/live/stream; アクセスしてみる`https://&lt;RaspberryPI IP&gt;:8090` にアクセスしてみます。 Digest 認証を問われるので設定した ID/PW を入力します。 HLS 配信されていることを確認できました！ 負荷状況としては CPU 25 - 30% 程度に収まっています。 まとめffmpeg + Nginx + RTMP で HLS 配信を RaspberryPI 上に構築できました。 実際の運用では、常時起動してはおらず、見たいときだけ起動するような仕組みにしており、負荷は極力抑えています。 監視は Mackerel の無料プランで今のところ十分です。 育児ハックの一環として参考になれば何よりです。 *1:勿論のことですが、家族の了承を得た上で設定しています。","link":"/2018/08/14/2018-08-15-ffmpeg-nginx-rtmp-on-raspberrypi/"},{"title":"Flask+Service Worker on Heroku で PWA チュートリアル","text":"概要自分にとっては dev.to でバズった Service Worker。その概要と機能性をなぞってみようとチュートリアル的に学んだ内容をまとめました。 掲題の通り、Flask + Service Worker を Heroku で動作させ、PWA(Progressive Web Apps) してみました。 toco ちゃんバス あと何分？戸田市コミュニティバス tocoちゃんバスの各循環・停留所にてあと何分で到着するかが簡単にわかるアプリです。 さらなる sleep 対策としてService Worker があればオフラインでもサービス動作させられるし、sleep し続けてもいいのでは？と思い、導入してみました。 Flask に Service Worker 導入ソースは git にあります。 kenzo0107/toda-tocochan-busContribute to kenzo0107/toda-tocochan-bus development by creating an account on GitHub. 簡単に導入時のポイント app.py というメインスクリプトに /sw.js へのアクセスできるようにします。 123@app.route('/sw.js', methods=['GET'])def sw(): return app.send_static_file('sw.js') static ディレクトリ内に空の sw.js を配置 基本、上記 2 step をしてから Service Worker の各処理を実装していきます。 Install以下の install イベントでは、指定したキャッシュさせたいファイルパスを全てキャッシュさせています。挙動のイメージとしては、トップページにアクセスした際に Service Worker がブラウザに導入（install）されるイベントの発生時にキャッシュを生成しています。 12345678910111213141516171819202122232425var urlsToCache = [ '/', '/static/img/favicon.ico', '/static/img/logo.png', '/static/css/bootstrap.min.css', '/static/css/flickity.org.css', '/static/js/async_set_circuit.js', '/static/js/bootstrap.min.js', '/static/js/flickity.pkgd.min.js', '/static/js/jquery-3.1.0.min.js', '/static/js/jquery.countdown.min.js', '/static/js/superagent.js', '/static/js/tether.min.js']self.addEventListener('install', event =&gt; { console.log('install') event.waitUntil( caches.open(cacheName) .then(function(cache) { console.log('Opened cache'); return cache.addAll(urlsToCache) }) )}) Chrome &gt; Developer Tool &gt; Application &gt; Cache Storage を見るとキャッシュされているのがわかります。 fetch以下処理は、fetch イベントでブラウザでキャッシュしたファイルを呼び出しています。 12345678910111213self.addEventListener('fetch', function(event) { console.log('fetch') event.respondWith( caches.match(event.request) .then(function(response) { if (response) { return response; } return fetch(event.request) } ) );}); activateService Worker は active 状態になってもすぐにブラウザ上のリソースを操作できず、もう一度ページにアクセスした際にできるようになっています。 その為、一度しかアクセスしないユーザにとっては Service Worker によるパフォーマンスの向上を体験できないことになります。 その為、 以下 activate イベントによって直ちに操作できるようにします。 1234self.addEventListener('activate', event =&gt; { console.log('activate') event.waitUntil(self.clients.claim());}) 基本、以上の設定で Service Worker 導入完了でした。 前後を比較すると Waterfall で見る、リソースのロードタイムがキュッと縮んでいるのがわかります。 Before After まとめService Worker で一度キャッシュさせた後はオフラインでも動作するような仕組みが作れました。オフラインでも動作する、というのは魅力的♪ ただし、クエリパラメータのパターンの多い URL がある場合などはキャッシュされにくく、この場合のキャッシュ戦略としては、ひとまず静的ファイルのみキャッシュするなどで対応するのが良いのか、等考えさせられるところがありました。 例） 123/ts?circuit_id=1&amp;station_id=1 はアクセスしたけど/ts?circuit_id=1&amp;station_id=2 はアクセスしてないという場合はオフラインにしたら /ts?circuit_id=1&amp;station_id=2 は閲覧できなくなる また、POST method は Service Worker は未対応で issue が上がっているようです。Workaround として以下提案がされているブログがありましたが、実装が複雑で、まだこの辺りは開発の余地がある印象です。 Offline POSTs with Progressive Web AppsA Microsoft intern’s take on Progressive Web Apps, and providing a great offline experience with offline POSTs. 以下 Service Worker 導入時の苦労した点があり、涙無くして見られない内容でした。日経電子版 サイト高速化とPWA対応 他趣味アプリで Workbox を利用していますが、こちらも書いていきたいと思います。 参考How to queue post request using workbox?Using following js in my service worker from workboxjs sample for my testing: importScripts('https://unpkg.com/workbox-sw@0.0.2/build/impor… Service Worker、はじめの一歩 - Service Workerとは | CodeGridService Workerは、Webページの裏側で働く独立したJavaScript環境です。今回はその概要と、どのようなライフサイクルを持っているのかを解説します。全体像が把握しやすくなるはずです。","link":"/2018/08/13/2018-08-14-flask-service-worker-on-heroku-pwa/"},{"title":"食洗機かけ終わったかわからなくなる問題を RaspberryPI + BlueButton + LINE Notify + Google Home で解決した話","text":"概要食洗機かけ終わったかどうかわからなくなる問題が我が家で多発していました。 それを RaspberryPI + BluetButton + LINE Notify + Google Home で解決した話です。 何が問題？我が家では、食洗機がある程度溜まったらかける、という風にしている為、必ずかける時間が決まっていません。 その為、時折、妻or自分が食洗機を引き出した時に、「これ洗ったやつだっけ？」と疑心暗鬼・一触即発状態となり、 駆け寄った息子にドラゴンストップしていただく事態が多発していました。 記録が取れる様な仕組み、何か見ればわかる！ という状態にさえしといてもらえたら助かる、ということでエンジニアリングで解決できないか、と考えました。 解決法開始した時刻を LINE グループに通知して記録する様にしました。*1 その通知の仕組みをボタンを一押しで済ませられる様な、シンプルなものにできないか、と考え、 家にある Raspberry PI と組み合わせられる BlueButton で実装しようと考えました。 仕組み 食洗機を開始したら、BlueButton を押す BlueButton を押すと Bluetooth ペアリングしている Raspberry PI にボタン押したよ情報を渡す。 Raspberry PI 側で起動させておいた BlueButton が押されたか判定するスクリプトが検知 3をトリガーに LINE Notify で LINE に通知。 3をトリガーに Google Home に食洗機開始したよメッセージを喋ってもらう。*2 押す長さによって処理分けしています。 長押し=食洗機開始をGoogle Home, Line へ通知 短押し=食洗機開始時刻読み上げ 購入したものRaspberry Pi 3 Model B V1.2 (日本製) 国内正規代理店品出版社/メーカー: Raspberry Pi発売日: 2016/02/29メディア: Tools & Hardwareこの商品を含むブログを見る Punasi Raspberry Pi3/2/B+/B 5V 2.5A アダプタ 電源 マイクロUSB Boseスピーカー Androidタブレット スマートフォン出版社/メーカー: Punasiメディア: エレクトロニクスこの商品を含むブログを見る GOCOUP スマートフォン用 カメラリモコン A-Bシャッター Bluetoothリモートfor iPhone & Android 日本語説明書付き(黒)出版社/メーカー: GOCOUPメディア: エレクトロニクスこの商品を含むブログを見る Google Home出版社/メーカー: googleメディア: オフィス用品この商品を含むブログを見る BlueButton は Amazon で 100 円程度でした♪ お財布に優しい！ Google Home はあくまでスピーカー代わりでボタンが押された時の確認用として使ってます。 USB スピーカーを接続するでも、Lチカで反応させる、でも確認できるものがあれば、それで良いと思います。 Raspberry PI でのセットアップRaspberry PI 情報$ uname -a Linux raspberrypi 4.9.35-v7+ #1014 SMP Fri Jun 30 14:47:43 BST 2017 armv7l GNU/Linux 事前準備 LINE Notify に Signup し token 生成します。 LINE Notify BlueButton を Bluetooth ペアリングしときます。 ペアリングは既にいくつも記事があります。以下記事参考になるかと思います。 Bluetoothシャットダウンボタンを作る #300円でIoTボタン google-home-notifiler インストール nodejs, npm インストール pi$ sudo apt-get update pi$ sudo apt-get install -y nodejs npm pi$ sudo npm cache clean pi$ sudo npm install npm n -g pi$ sudo n stable google-home-notifiler 設定 pi$ cd ~ pi$ git clone https://github.com/noelportugal/google-home-notifier pi$ cd google-home-notifier/ pi$ npm install example.js 編集 ... const serverPort = 8091; // default port + var deviceName = 'ファミリールーム'; // Google Home's device name + var ip = '&lt;Google Home's IP&gt;'; // ex. 192.168.11.5 ... Google Home's IP の確認はこちら google-home-notifler サーバ起動スクリプト作成 /etc/systemd/system/googlehomenotifier.service [Unit] Description=google-home-notifier Server After=syslog.target network-online.target [Service] Type=simple User=root ExecStart=/usr/bin/node example.js Restart=on-failure RestartSec=10 KillMode=process WorkingDirectory=/home/pi/google-home-notifier [Install] WantedBy=multi-user.target 上記追加後、daemon-reload し、`googlehomenotifier.service` を認識させます。 pi$ sudo systemctl daemon-reload いざ google-home-notifiler 起動pi$ sudo systemctl start google-home-notifiler 試しに hello と言わせる♪pi$ curl -X POST -d \"text=hello\" https://127.0.0.1:8091/google-home-notifier BlueButton セットアップpi$ sudo gem install bluebutton 食洗機 Notify スクリプト インストール元々 python で書きましたが、shell の方が速度が出たので shell にしています。 pi$ cd ~ pi$ git clone https://github.com/kenzo0107/dishwasher dishwasher.sh の以下箇所を先に生成したものと変更してください。 readonly LINENOTIFY_TOKEN=\"&lt;please change yours&gt;\" 起動スクリプト設定 &amp; 実行pi$ sudo cp bluebutton.service /etc/systemd/system/ pi$ sudo systemctl daemon-reload pi$ sudo systemctl start bluebutton.service いざ実行 できました♪ BlueButton についてちょっとだけ注意BlueButton のボタンのトリガーの種類は以下ですが、 key down ボタンを押す key up 押したボタンを離す long key down ボタン長押し long key up 長押しボタンを離す long key down (長押し)していると、まず最初に key down イベントが発生し、その後、 long key down のイベント発生となります。 key down イベントに渡した処理がある場合、 long key down のイベントに渡した処理のみを実行する、 というのはできないので注意が必要です。 総評食洗機開始ボタンができたおかげで家庭から争いがなくなり イヤイヤ期だった息子も朗らかになった様に思います。 是非イヤイヤ期の子供と食洗機問題を抱えている方のお力になれば何よりです。 以上です。 *1:curl 叩ければ、後々 Slack でも何でも、家族みんなが見る所に通知すれば良いかなと思ったので *2:Google Home はあくまでスピーカー代りです。","link":"/2018/08/19/2018-08-20-raspberrypi-bluebutton-line-notify-google-home/"},{"title":"プロセスの起動経過時間・CPU使用時間","text":"備忘録です。 プロセスがいつ頃から起動しているものか、全然再起動してないと再起動するのもやや不安になるので 一旦確認しておこう、という気持ちから以下コマンドを使っています。 $ ps -eo pid,comm,etime,time | grep node 591 node 51-20:47:21 00:01:54 2255 node 23-18:07:58 00:01:18 OPTION Explain pid プロセス ID comm コマンド etime プロセス開始からの経過時間 time CPU使用時間 もちろん CPU の使用頻度等も把握できるので何かと便利♪","link":"/2018/09/07/2018-09-08-cpu/"},{"title":"S3 に5分毎に出力される AWS LB ログファイルを時間帯を指定してまとめてダウンロード","text":"概要AWS で LB のログを S3 に保存設定をしている場合に、 インシデントがあった時間帯のログがまとめて欲しいという時に awscli でまとめてログ取得しています。 その時の手順を備忘録としてまとめました。 事前準備 awscli インストール 本件の実行環境は以下になります。 macOS%$ sw_vers ProductName: Mac OS X ProductVersion: 10.12.6 BuildVersion: 16G1036 macOS%$ aws --version aws-cli/1.15.50 Python/3.7.0 Darwin/16.7.0 botocore/1.10.49 ログファイルダウンロード例) 2018年9月12日 14時台の ALB ログファイルをダウンロード // 2018年9月12日 14時台のログファイルをダウンロード macOS%$ aws s3 --profile &lt;profile&gt; cp s3://&lt;log bucket name&gt;/&lt;lb name&gt;/AWSLogs/123456789012/elasticloadbalancing/ap-northeast-1/2018/09/12/ . --recursive --exclude \"*\" --include \"*20180912T05*\" // ログファイル解凍 gunzip *.gz // log を1ファイルにまとめ cat *.log &gt; all.log // HTTP コードが 50x のものを 50x.log にまとめる awk '{if($9 ~ 50) print $0}' all.log &gt; 50x.log `--profile &lt;profile&gt;` は `aws configure --profile &lt;profile&gt;` で設定した場合の設定です。 default を利用する場合は `--profile &lt;profile&gt;` の指定は不要です。 以上です。","link":"/2018/09/11/2018-09-12-s3-5-aws-lb/"},{"title":"AWS EC2 t2 から t3 へ移行する為の step by step","text":"概要AWS EC2 に t3 系インタスタンスが登場した為、サクッとできるかと思いきや、つまづいた箇所をまとめました。 今回対象のインスタンスは HVM で ubuntu 16.04.5 LTS を使用しました。 t2 と比べて t3 は何がいいの？ 安い！ *1 t2.small 以下は仮装コア数が 1 でしたが、 t3 では 2 倍！ t2 と t3 、同じインスタンスサイズでクレジットが倍！ クレジットが無制限モードデフォルトで有効！ EBS 最適化がデフォルトで有効 t2 から t3 へ移行する大まかな流れ要は、ena モジュールをインストールし、EC2 ENA サポートを有効化する必要がありました。 t2 インスタンス停止 AMI 作成 t2 インスタンス起動 t2 インスタンスでカーネルモジュール(ena) のインストール ena モジュールインストール確認 t2 インスタンス停止 インスタンスタイプを t3 へ変更 (credit: unlimited もしたい場合はここで) t3 インスタンス起動 ENA って？Elastic Network Adapter – Amazon EC2 向けの高性能パフォーマンスネットワークインターフェイス プロセッサのワークロードを軽くし、ネットワークパケットと生成または処理を行う vCPU 間で短く効率的なパスを作成するために構築されています。 Linux インスタンスにおける Elastic Network Adapter (ENA) を使用した拡張ネットワーキングの有効化 には以下のように記載があります。 Amazon EC2 は、Elastic Network Adapter (ENA) を介して C5, C5d, F1, G3, H1, I3, m4.16xlarge, M5, M5d, P2, P3, R4, R5, R5d, X1, X1e, and z1d インスタンスに拡張されたネットワーキング機能を提供します。 拡張ネットワーキングは、Amazon EC2 コンソールから管理することはできません。 HVM インスタンスでのみサポート まとめると、 Amazon EC2 向けの高性能パフォーマンスネットワークインターフェイス HVM (Hardware-assited VM:完全仮想化) 環境でサポートされている。 PV (ParaVirtual:準仮想化) 環境ではサポートされない。 ENA というカーネルモジュールを介す事で、インスタンスに拡張されたネットワーキング機能が利用できる。 pv/hvm は AWS コンソール＞EC2 説明の「仮想化」の項目で確認できます。 pv の場合は、 hvm の移行を検討する必要があります。 以下から設定に進みます。 ENA 有効化設定手順 以下前提とします。 AMI を取る等のバックアップが済んでいる。 t2.small から t3.small に移行する。 Ubuntu での拡張ネットワーキングの有効化ubuntu:~$ sudo apt-get update && sudo apt-get upgrade -y linux-aws 他OSの対応法も先ほどの Linux インスタンスにおける Elastic Network Adapter (ENA) を使用した拡張ネットワーキングの有効化 に記載されています。 W: mdadm: /etc/mdadm/mdadm.conf defines no arrays. エラーが発生した場合`/etc/mdadm/mdadm.conf` ファイルに以下一文を追記します。 ARRAY &lt;ignore&gt; devices=&lt;ルートデバイス&gt; 自分の場合は以下の一文を一番下に追記して、もう一度 コマンド実行したら通りました。 ARRAY &lt;ignore&gt; devices=/dev/sda1 ena カーネルモジュールに関する情報表示`modinfo ena` を実行し以下のように表示されれば OK です。 ubuntu:~$ modinfo ena filename: /lib/modules/4.4.0-81-generic/kernel/drivers/net/ethernet/amazon/ena/ena.ko version: 1.1.2 license: GPL description: Elastic Network Adapter (ENA) author: Amazon.com, Inc. or its affiliates ... EC2 ENA サポート有効化// インスタンス停止 macOS%$ aws ec2 stop-instances --instance-ids &lt;instance id&gt; // ENA サポート設定 macOS%$ aws ec2 modify-instance-attribute --instance-id &lt;instance id&gt; --ena-support true // EBS 最適化 (任意) macOS%$ aws ec2 modify-instance-attribute --instance-id &lt;instance id&gt; --ebs-optimized // credit unlimited 設定 (任意) macOS%$ aws ec2 modify-instance-credit-specification --instance-credit-specification \"InstanceId=i-&lt;instance id&gt;,CpuCredits=unlimited\" // インスタンスタイプ変更 macOS%$ aws ec2 modify-instance-attribute --instance-id &lt;instance id&gt; --instance-type t3.small // インスタンス起動 macOS%$ aws ec2 start-instances --instance-ids &lt;instance id&gt; これで t3 デビューを飾ることができました♪ 参照 Linux インスタンスにおける Elastic Network Adapter (ENA) を使用した拡張ネットワーキングの有効化 Stack Exchange:I don't have a RAID but get the warning mdadm.conf defines no arrays","link":"/2018/09/13/2018-09-14-aws-ec2-t2-t3-step-by-step/"},{"title":"No space left on device が発生して i-node 枯渇してた時の原因調査法","text":"Linux Server で No space left on device が発生した時の対処まとめです。 とりあえず df -h してみるdf -h しても 最大で 77%no space left on device が発生することでもなさそう 12345678910111213$ df -hFilesystem Size Used Avail Use% Mounted onudev 1.9G 0 1.9G 0% /devtmpfs 385M 40M 346M 11% /run/dev/nvme0n1p1 15G 11G 3.3G 77% /tmpfs 1.9G 0 1.9G 0% /dev/shmtmpfs 5.0M 0 5.0M 0% /run/locktmpfs 1.9G 0 1.9G 0% /sys/fs/cgrouptmpfs 385M 0 385M 0% /run/user/1022tmpfs 385M 0 385M 0% /run/user/1128tmpfs 385M 0 385M 0% /run/user/1098tmpfs 385M 0 385M 0% /run/user/6096 -h = --human-readable 読みやすいサイズ表示をしてます。 df -i してみるdf -i で i-node 情報表示。最大 95%これでした。 12345678910111213$ df -iFilesystem Inodes IUsed IFree IUse% Mounted onudev 490419 351 490068 1% /devtmpfs 492742 521 492221 1% /run/dev/nvme0n1p1 983040 927212 55828 95% /tmpfs 492742 1 492741 1% /dev/shmtmpfs 492742 3 492739 1% /run/locktmpfs 492742 16 492726 1% /sys/fs/cgrouptmpfs 492742 4 492738 1% /run/user/1022tmpfs 492742 4 492738 1% /run/user/1128tmpfs 492742 4 492738 1% /run/user/1098tmpfs 492742 4 492738 1% /run/user/1142 i-node とは？と思ったら、 「分かりそう」で「分からない」でも「分かった」気になれるIT用語辞典 i-node編 辺りを見てみてください。 簡単に言うと、ファイルの属性情報を管理しているデータです。 要は、ファイル数が増えると、ファイルを管理するデータが増え、 i-node はどんどん増えていきます。 その調査法をまとめました。 どのディレクトリのファイル数が多いか調査以下は「現ディレクトリでのファイル数多い順ランキング」です。 1sudo find . -xdev -type f | cut -d \"/\" -f 2 | sort | uniq -c | sort -r ※ find の -xdev オプションはマウント先のファイルシステムを検索しない様にしてます。-type f はファイルのみ検索。 このワンライナーで原因となるファイル数の多いディレクトリを探索します。 当たりが付いている場合はそのディレクトリで実行例えば、ユーザ毎にディレクトリが用意されている場合等に、個々人が home directory で git clone してるとか、個々人が bundle install してて vendor ディレクトリ以下がファイル数が激増してたとか。 そういった事象があり得そうなら、 /home/ ディレクトリ以下でワンライナー実行して原因調査をするのが良いです。 各ユーザ毎が原因なら相談して消して良いかも確認できるし！ 一番手っ取り早いのは、root path 「/」 で実行どのディレクトリのファイル数が多いのかを探るのなら、一番上位階層の「/」(root) から実行した方が特定しやすいです。 但し、root から全てのディレクトリ内のファイルを検索するとなると非常に cpu を食います。実行してしばらくレスポンスが返ってこなくてドキドキします。 top コマンド等で cpu 状況を監視しつつ、実行することをオススメします。 本番環境の web サーバで直ちにユーザ影響が出そうな場合は、LBから一旦外して、とか、ユーザアクセスの少ない時間に実行する様に影響範囲を最小限にしたい所。 状況見た上で進めましょう。 実際にあった i-node 枯渇原因/usr ディレクトリ以下に linux-headers-*** ファイルが溜まっており、30% 近く食ってました。 以下記事に救われました。ありがとうございます。 古いカーネルの削除方法メモ 追記 2020-07-02linux-headers-*** ファイルの削除について、不要な利用されていないファイルを削除するには以下コマンドで削除されます。 1sudo apt autoremove 自動的に削除したい場合123456// 自動アップデートパッケージインストール$ sudo apt-get install -y unattended-upgrades// 自動アップデート有効化$ sudo dpkg-reconfigure -plow unattended-upgradesYes を選択 /etc/apt/apt.conf.d/50unattended-upgrades を以下の様に編集し、 unattended-upgrade 時に autoremove する処理を追加する。 1sudo vim /etc/apt/apt.conf.d/50unattended-upgrades 12//Unattended-Upgrade::Remove-Unused-Dependencies &quot;false&quot;;Unattended-Upgrade::Remove-Unused-Dependencies &quot;true&quot;; 自動アップデートのログは /var/log/unattended-upgrades/ に出力される。 以上です。","link":"/2018/10/14/2018-10-15-no-space-left-on-device-i-node/"},{"title":"boto3 の AssumeRole をしたアカウントスイッチ credentials 利用時の MFA 突破対応","text":"概要備忘録です。 AssumeRole でのアカウントスイッチで credentials 情報を持っている場合に対応した boto3.Session での認証の仕方です。 MFA 設定してる場合も付けときました。 実装123456789101112131415161718192021222324252627# MFA 入力待ちmfa_TOTP = raw_input(\"Enter the MFA code: \")# sts クライアントclient=boto3.client( 'sts' )# 認証response = client.assume_role( RoleArn='arn:aws:iam::123456789:role/admin_full', RoleSessionName='mysession', DurationSeconds=3600, SerialNumber='arn:aws:iam::987654321:mfa/myaccount', TokenCode=mfa_TOTP,)# 認証情報credentials = response['Credentials']# session に 認証情報付加session = boto3.Session(profile_name=session_name, aws_access_key_id = credentials['AccessKeyId'], aws_secret_access_key = credentials['SecretAccessKey'], aws_session_token = credentials['SessionToken'],)ec2Client = session.client('ec2', region_name='ap-north-east1')resources = ec2.describe_instances() boto3.Session に sts で AssumeRole で得た credentials 情報を渡してます。 以上です。","link":"/2018/12/05/2018-12-06-boto3-assumerole-credentials-mfa/"},{"title":"ECS EC2 上で起動する Datadog Agent コンテナが unhealthy になる時の処方箋","text":"概要1234$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES8baa0e2cff47 datadog/docker-dd-agent:latest \"/entrypoint.sh supe…\" 31 hours ago Up 31 hours (unhealthy) 8125/udp, 8126/tcp ecs-dd-agent-task-1-dd-agent-f6d3d5eb9febcab9c601 ある日、ECS で起動させている Datadog Agent コンテナが unhealthy になってしまう事象が発生しました。その原因と対応法をまとめました。 結論Datadog Agent イメージを現時点の最新バージョン 6 系にすることで解決できました。 Datadog サポートに問い合わせた所、今回のケースでは Datadog Agent イメージのバージョンが 5 系だったことに起因していました。 datadog/docker-dd-agent:latest は 5系の最新だった！バージョン5が最新だった時には設定手続きは以下に沿って実施していました。https://docs.datadoghq.com/integrations/faq/agent-5-amazon-ecs/ 上記手順にて登場する datadog agent の ECS での起動用タスクが以下になります。ここで指定しているイメージ (datadog/docker-dd-agent:latest) が 5系でした。 https://docs.datadoghq.com/json/dd-agent-ecs.json datadog/docker-dd-agent:latest は 5系の最新だった！ datadog/agent:latest が 2019.01.10 時点最新の 6系 ！現最新バージョン 6系を扱うには以下設定手続きを参照します。https://docs.datadoghq.com/integrations/amazon_ecs 手続きで変更点はタスク定義の変更くらいです。 https://docs.datadoghq.com/json/datadog-agent-ecs.json 今の所、datadog/agent:latest が6系の最新になっています。7系になった際には是非とも互換維持してほしいです。 おまけサポートへの問い合わせサポートに問い合わせると、 caseID という問い合わせの ID をいただけます。その後、caseID を設定し、起動時のログファイル (tar.gz) を取得し、サポート宛に添付しました。 ECS の管理下にある EC2 に ssh ログインし以下実行します。 1234567$ docker run --rm -v /tmp:/tmp -e API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx datadog/docker-dd-agent:latest /etc/init.d/datadog-agent flare &lt;caseID&gt;2019-01-03 12:27:44,472 | ERROR | dd.collector | utils.dockerutil(dockerutil.py:148) | Failed to initialize the docker client. Docker-related features will fail. Will retry 0 time(s). Error: Error while fetching server API version: ('Connection aborted.', error(2, 'No such file or directory'))...2019-01-03 12:27:45,807 | INFO | dd.collector | utils.flare(flare.py:161) | Saving all files to /tmp/datadog-agent-2019-01-03-12-27-44.tar.bz2/tmp/datadog-agent-2019-01-03-12-27-44.tar.bz2 is going to be uploaded to Datadog.... EC2 ホスト上に /tmp/datadog-agent-2019-01-03-12-27-44.tar.bz2 ファイルが取得できるので、それをサポート宛にメール添付しました。 上記でログも含めサポートに連絡した所、API バージョンにより接続中止されている、という指摘を受け、バージョン上げて！という話になりました。 12019-01-03 12:27:44,472 | ERROR | dd.collector | utils.dockerutil(dockerutil.py:148) | Failed to initialize the docker client. Docker-related features will fail. Will retry 0 time(s). Error: Error while fetching server API version: ('Connection aborted.', error(2, 'No such file or directory')) サポートさんありがとう♪ 以上です。参考になれば幸いです。","link":"/2019/01/09/2019-01-10-ecs-ec2-datadog-agent-unhealthy/"},{"title":"AWS ECS トラブルシューティング","text":"ECS を利用していて幾つかはまったポイントがあったのでまとめました。 started 1 task が複数回実行されるが、コンテナが起動しない12345$ ecs-cli compose service up ...level=info msg=\"(service hogehoge) has started 1 tasks ...\"level=info msg=\"(service hogehoge) has started 1 tasks ...\"level=info msg=\"(service hogehoge) has started 1 tasks ...\" ecs-cli compose service up でデプロイ時にタスク起動を実行するものの、起動が正しくできていない状態です。こちらはコンテナ起動時の処理に問題がある場合があります。 コンテナログを確認して、コンテナ起動失敗時刻付近のログを確認してください。 例えば、Nginx の設定ファイル, Rails のコードに typo, syntax error がある等です。 already using a port required by your task12service hogehoge was unable to place a task because no container instance met all of its requirements.The closest matching container-instance a1b2c3d4-e5f6-g7h8-j9k0-l1m2n3o4p5q6 is already using a port required by your task port mapping を以下の様に設定していた。 1234567\"portMappings\": [ { \"hostPort\": 0, \"protocol\": \"tcp\", \"containerPort\": 80 } ], 新しいタスクでも 0:80 のポートを利用しようとする為、エラーとなります。以下の様に設定することで回避できました。 12345\"portMappings\": [ { \"containerPort\": 80 } ], insufficient memory available1INFO[0031] (service hogehoge) was unable to place a task because no container instance met all of its requirements. The closest matching (container-instance a1b2c3d4-e5f6-g7h8-j9k0-l1m2n3o4p5q6) has insufficient memory available. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide. timestamp=2018-03-09 15:45:24 +0000 UTC タスク更新（ecs-cli compose service up）実行時、上記の様なメモリ不足が出る場合はインスタンスタイプを上げる、また、他タスクを削除する等、メモリーリソースを増やす対応が必要です。 no space on deviceno space on device で イメージを pull できない。 df -hT コマンドで 容量の使用状況確認 未使用のコンテナ・ボリュームを強制削除しお掃除 1docker system prune -af --volumes msg=”Couldn’t run containers” reason=”RESOURCE:CPU”1msg=\"Couldn't run containers\" reason=\"RESOURCE:CPU\" タスクで指定している cpu (vCPU) が不足しています。インスタンスタイプを上げる、もしくは、他タスクを削除する等、 CPU リソースを増やす対応が必要です。 Fargate - Port Mapping Error1level=error msg=\"Create task definition failed\" error=\"ClientException: When networkMode=awsvpc, the host ports and container ports in port mappings must match.\\n\\tstatus code: 400, request id: a1b2c3d4-e5f6-g7h8-j9k0-l1m2n3o4p5q6\" 起動タイプ Fargate で以下の様な設定だと、NG 12ports: - \"80\" こちらだと OK。 12ports: - \"80:80\" ホストポートとコンテナポートのマッピングが必要です。 Fargate volume_from は利用できないvolume_from は Fargate では使用できません。 1level=error msg=\"Create task definition failed\" error=\"ClientException: host.sourcePath should not be set for volumes in Fargate.\\n\\tstatus code: 400, request id: a1b2c3d4-e5f6-g7h8-j9k0-l1m2n3o4p5q6\" 指定された IAM Role が適切なパーミッションを与えられていないIAM Role に権限を適宜付与します。 12level=info msg=\"(service hogehoge) failed to launch a task with (error ECS was unable to assume the role 'arn:aws:iam::123456789012:role/ecsTaskExecutionRole' that was provided for this task. Please verify that the role being passed has the proper trust relationship and permissions and that your IAM user has permissions to pass this role.).\" timestamp=2018-06-21 08:15:43 +0000 UTC イメージ pull できないというエラーも権限を付与していないことに起因することが主です。 1CannotPullContainerError: API error (500): Get https://123456789012.dkr.ecr.ap-northeast-1.amazonaws.com/v2/: net/http: request canceled while waiting for connection\" 現在稼働している ECS の IAM Role の権限を参考してください。変更される可能性があるのであくまで参考にし、適宜最新の情報を以ってご対応ください。 123456789101112131415161718192021222324252627282930313233343536373839404142{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"\", \"Effect\": \"Allow\", \"Action\": [ \"logs:PutLogEvents\", \"logs:CreateLogStream\", \"logs:CreateLogGroup\", \"elasticloadbalancing:RegisterTargets\", \"elasticloadbalancing:Describe*\", \"elasticloadbalancing:DeregisterTargets\", \"ecs:UpdateService\", \"ecs:Submit*\", \"ecs:StartTelemetrySession\", \"ecs:StartTask\", \"ecs:RunTask\", \"ecs:RegisterTaskDefinition\", \"ecs:RegisterContainerInstance\", \"ecs:Poll\", \"ecs:ListTasks\", \"ecs:DiscoverPollEndpoint\", \"ecs:DescribeTasks\", \"ecs:DescribeServices\", \"ecs:DescribeContainerInstances\", \"ecs:DeregisterContainerInstance\", \"ecs:CreateService\", \"ecr:UploadLayerPart\", \"ecr:PutImage\", \"ecr:InitiateLayerUpload\", \"ecr:GetDownloadUrlForLayer\", \"ecr:GetAuthorizationToken\", \"ecr:CompleteLayerUpload\", \"ecr:BatchGetImage\", \"ecr:BatchCheckLayerAvailability\", \"ec2:Describe*\" ], \"Resource\": \"*\" } ]} 以上です。 また何か発生したら追記していきたいと思います。 Reference Amazon ECS エージェントが切断状態で表示されるのは、なぜですか?","link":"/2018/02/07/2019-02-08-aws-ecs/"},{"title":"Rails に reCAPTCHA v3 導入して bot 対策","text":"概要Rails で構築した Webサービスで bot 攻撃を定期的に受けた為、問い合わせフォームに reCAPTCHA v3 を導入しました。 何故 v2 でなく、reCAPTCHA v3 ? v2 は I'm not a robot チェックボックスにチェックを入れた後に画像選択させる仕様があります。 例えば、看板が写ってるのはどれ？と選ばせる問いが出てきた場合、「どこまでが看板としたらいいの？」と心理的負担も高く、ユーザが離脱する可能性もあります。 v3 だと嬉しいことは何？v3 *1 は設置したページのユーザ行動をスコア化し bot か判断します。 アクセスが増えるとより精度が高まってくる、という仕様です。 非 bot ユーザへの負担は全くなく、 bot を遮断できる様になるという、世の中進んでるなぁ感満載です。 GoogleのreCAPTCHAはどうやって人間とボットを見分けているのか？Googleが開発を進めるウェブサイトの認証システム「reCAPTCHA」は、人間とボットを区別するためのシステムです。これまでの人間とボットを見分けるシステムの多くは歪んだ文字を読んで手入力したり、条件にふさわしい画像を複数選択したりといった作業で見分けられていましたが、こうし… gem ある？今回 gem は使用しませんでした。 というのも、 以下理由からでした。 gem 'recaptcha' が v3 非対応。 gem 'new_google_recaptcha' は v3 対応してますが、スコアが返ってこないのでテストし辛い。 その他に既にあるのかもわかりませんが、記事執筆時には探し出すことはできませんでした。 まず reCAPTCHA v3 発行以下 reCAPTCHA コンソールにアクセスし発行してください。 https://g.co/recaptcha/v3 v3 を選択し、今回導入するドメインを登録します。*2 発行されたサイトキー・シークレットキーを保存しておきます。 サイトキー ユーザがサイトにアクセスした際にトークンを取得する際に必要なキーです。こちらはユーザ公開して問題ありません。 シークレットキー トークンを元に Google に問い合わせする際に必要なキーです。こちらは秘密情報として扱います。 Rails 側実装Rails >= 5.2 を想定しています。 config/credentials.yml.encrecaptcha: secret_key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx シークレットを秘密情報に保存します。 app/controllers/application_controller.rbrequire 'net/http' require 'uri' class ApplicationController &lt; ActionController::Base ... RECAPTCHA_MINIMUM_SCORE = 0.5 RECAPTCHA_ACTION = 'homepage' ... def verify_recaptcha?(token) secret_key = Rails.application.credentials.recaptcha[:secret_key] uri = URI.parse(&quot;https://www.google.com/recaptcha/api/siteverify?secret=#{secret_key}&response=#{token}&quot;) r = Net::HTTP.get_response(uri) j = JSON.parse(r.body) j['success'] && j['score'] &gt; RECAPTCHA_MINIMUM_SCORE && j['action'] == RECAPTCHA_ACTION end end 共通メソッドとして、recaptcha の認証メソッド `verify_recaptcha?` を設定しています。 ここで、bot となるスコアを 0.5 以下としています。 通常通り操作していれば、十分超える数値です。 config/locales/en.ymlen: recaptcha: errors: verification_failed: 'reCAPTCHA Authorization Failed. Please try again later.' local en 設定です。 config/locales/ja.ymlja: recaptcha: errors: verification_failed: 'reCAPTCHA 認証失敗しました。しばらくしてからもう一度お試しください。' local ja 設定です。 app/controllers/hoges_controller.rbclass HogesController &lt; ApplicationController def new; end def create unless verify_recaptcha?(params[:recaptcha_token]) flash.now[:recaptcha_error] = I18n.t('recaptcha.errors.verification_failed') return render action: :new end # something to do redirect_to hoge_finish_path end def finish; end end new から create に post して reCAPTCHA で bot 判定して OK → finish へ進む NG → new に戻る という設計です。 app/views/hoges/new.html.erb&lt;% if flash[:recaptcha_error] %&gt; &lt;div class=&quot;text&quot;&gt; &lt;p&gt;&lt;spacn class=&quot;error&quot;&gt;&lt;%= flash[:recaptcha_error] %&gt;&lt;/span&gt;&lt;/p&gt; &lt;/div&gt; &lt;% end %&gt; &lt;%= form_tag({action: :create}, {method: :post}) do %&gt; ... &lt;input id=&quot;recaptcha_token&quot; name=&quot;recaptcha_token&quot; type=&quot;hidden&quot;/&gt; &lt;%= submit_tag &quot;送信する&quot;, :class =&gt; &quot;submit-recaptcha btn&quot;, :disabled =&gt; true %&gt; &lt;% end %&gt; &lt;script src=&quot;https://www.google.com/recaptcha/api.js?render=&lt;%= Settings.recaptcha.site_key %&gt;&ver=3.0&quot;&gt;&lt;/script&gt; &lt;script&gt; grecaptcha.ready(function() { grecaptcha.execute('&lt;%= Settings.recaptcha.site_key %&gt;', {action: 'homepage'}).then(function(token) { $('#recaptcha_token').val(token); $('.submit-recaptcha').prop('disabled', false); }); }); &lt;/script&gt; エラーメッセージ表示&lt;% if flash[:recaptcha_error] %&gt; &lt;div class=&quot;text&quot;&gt; &lt;p&gt;&lt;spacn class=&quot;error&quot;&gt;&lt;%= flash[:recaptcha_error] %&gt;&lt;/span&gt;&lt;/p&gt; &lt;/div&gt; &lt;% end %&gt; &amp;lt;form&amp;gt; ~ &amp;lt;/form&amp;gt; 内に以下 name=recaptcha_token input タグを追加します。&lt;input id=\"recaptcha_token\" name=\"recaptcha_token\" type=\"hidden\"/&gt; ページアクセス時に reCAPTCHA の token を取得すべく、スクリプトを仕込みます。&lt;script src=&quot;https://www.google.com/recaptcha/api.js?render=&lt;%= Settings.recaptcha.site_key %&gt;&ver=3.0&quot;&gt;&lt;/script&gt; &lt;script&gt; grecaptcha.ready(function() { grecaptcha.execute('&lt;%= Settings.recaptcha.site_key %&gt;', {action: 'homepage'}).then(function(token) { $('#recaptcha_token').val(token); $('.submit-recaptcha').prop('disabled', false); }); }); &lt;/script&gt; reCAPTCHA トークン取得が成功した場合に以下実行します。 id=\"recaptcha_token\" input タグの value に トークンを設定 submit ボタンの有効化 `&lt;%= Settings.recaptcha.site_key %&gt;` について `gem 'settingslogic'` をインストールしている前提で設定しています。 導入していない場合は、簡易的に処理を試す程度であれば、 `&lt;%= Settings.recaptcha.site_key %&gt;` を取得したサイトキーに置き換えて下さい。*3 以上で設定は完了です。 ページにアクセスしてみるページ右下に reCAPTCHA マークが常に表示される様になります。 集計情報を見るreCAPTCHA コンソールを見ると、以下の様な表示が出ていてすぐには集計情報が反映されていないと思います。 しばらく経つと以下の様なグラフが表示される様になります。 注意例えば、社内 IP 等固定された IP からテストで頻繁にアクセスすると、 bot 扱いされます。 reCAPTCHA 側で IP のホワイトリストはないので、その場合、 Rails 側で許可 IP リストを作る必要があります。 以上 参考になれば幸いです。 *1:2019年2月現在最新バージョン *2:ドメインは複数登録可能です。ドメイン毎に集計や、 bot 対策の傾向を変えたい場合は、個々に発行します。 また、 RAILS_ENV = production とそれ以外で発行する方が本番への影響がないので推奨されます。 *3:前にもお伝えしましたが、サイトキーの管理は直指定でなく、何かしら管理が推奨です。","link":"/2019/02/16/2019-02-17-rails-recaptcha-v3-bot/"},{"title":"Rails (gem &#39;sendgrid-ruby&#39;) × SendGrid の Event Notification で API Key ごとの独自メタ情報を設定する","text":"SendGrid の Event Notification の使い所SendGrid には Event Notification という Webhook を設定することでメールの送信状態をイベント情報として取得することができます。 メールを SendGrid が受信した、送信先に届いた、等の情報です。 SendGrid - メールが届いているか確認する 例えば、未達だったメールの情報を取得したい場合等に、この Webhook を利用し、イベント情報を保存することで調査や集計が可能です。 AWS API Gateway + Lambda で構築したエンドポイントに投げ、S3 に保存し、送信失敗件数を Athena で検索集計する、ということができます。 何か問題でも？ SendGrid は 1アカウントで複数のプロジェクト毎の API Key を発行することができます。 ですが、 イベント情報にはどの API Key を利用してメール送信したか、の記録がありません。 複数の API Key がある場合に、どのプロジェクトのどの環境で送信したのか、調査や集計ができません。 これを解決する手段として、メール送信時にメタ情報を登録する方法があります。 Rails 5.2 で試してみました。 まずはセットアップ Gemfile 1gem 'sendgrid-ruby' config/initializers/sendgrid.rb 12sendgrid_api_key = Rails.application.credentials.dig(Rails.env.to_sym, :sendgrid_api_key)ActionMailer::Base.add_delivery_method :sendgrid, Mail::SendGrid, api_key: sendgrid_api_key api_key は credentials に設定し、そこから取得。※ RAILS_ENV=development でお試し可なので直に設定でも可。そこは自己責任で lib/mail/send_grid.rb 12345678910111213141516171819202122232425# frozen_string_literal: trueclass Mail::SendGrid def initialize(settings) @settings = settings end def deliver!(mail) sg_mail = SendGrid::Mail.new sg_mail.from = SendGrid::Email.new(email: mail.from.first) sg_mail.subject = mail.subject personalization = SendGrid::Personalization.new personalization.add_to(SendGrid::Email.new(email: mail.to.first)) personalization.subject = mail.subject sg_mail.add_personalization(personalization) sg_mail.add_content(SendGrid::Content.new(type: 'text/plain', value: mail.body.raw_source)) // ここでカテゴリー情報として登録 sg_mail.add_category(SendGrid::Category.new(name: \"#{Rails.env}-#{Settings.project_name}\")) sg = SendGrid::API.new(api_key: @settings[:api_key]) response = sg.client.mail._('send').post(request_body: sg_mail.to_json) Rails.logger.info response.status_code endend #{Rails.env}-#{Settings.project_name} の部分は適宜変更してください。 メール送信してみるrails c して 1ActionMailer::Base.mail(to: \"nakayama.kinnikunn@hogehoge.jp\", from: \"info@&amp;lt;sender authentication で認証したドメイン&amp;gt;\", subject: \"メールタイトル\", body: \"すいません、テスト送信です\").deliver_now すると、Event Notification では以下の様なイベント情報が取得できます。 1{\"email\":\"nakayama.kinnikunn@hogehoge.jp\",\"timestamp\":1551964210,\"ip\":\"12.345.67.89\",\"sg_event_id\":\"xxxxxxxxxxxxxxxx\",\"sg_message_id\":\"xxxxxxxxxxxxxxxxxxxx.yyyyyyyyyyyyyyyyyyyyyy\",\"category\":[\"staging-kenkoboys\"],\"useragent\":\"Mozilla/5.0 (Windows NT 5.1; rv:11.0) Gecko Firefox/11.0 (via ggpht.com GoogleImageProxy)\",\"event\":\"open\"} 注目すべきは category&quot;:[&quot;staging-kenkoboys&quot;] です。 add_category したカテゴリ情報が取得でき、これで どの API Key のイベント情報であるかの紐付けができます。 以上参考になれば幸いです。","link":"/2019/03/12/2019-03-13-rails-gem-sendgrid-ruby-x-sendgrid-event-notification-api-key/"},{"title":"ProxySQL で DB の Read&#x2F;Write Endpoint スイッチング","text":"docker-compose 上で ProxySQL で primary DB と secondary DB への SQL 毎にアクセス先をスイッチングする環境を構築し、試験してみました。 kenzo0107/proxysql-mysql-group-replication ProxySQL とは？ProxySQL はハイパフォーマンスな MySQL の SQL プロキシです。 MySQLのフォークである Percona Server や MariaDB だけでなく、Galera Cluster にも対応しています。 今回やってみようと思ったのは今回注目したのは ProxySQL の SQL プロキシの機能です。 ProxySQL は SQL によって、Read/Write エンドポイントをスイッチングしてくれます。 SELECT なら Read エンドポイントへ INSERT, UPDATE, DELETE なら Write エンドポイントへ という感じです。 使おうと思った経緯Rails に関わらず、アプリケーション側の問題で、Read/Write のスイッチングができない場合があります。 Rails で特定の gem に依存して switch_point が効かないところがあるとか。。 独自フレームワークで DB 側の処理が複雑すぎて手が出せないとか。。 なまじっか、サービスが成長していくと、アプリケーション側で DB のスイッチングができないことが、直接的に DB のボトルネックへ繋がることになりかねません。 この解決の為に ProxySQL を利用しようと思いました。 実際に試してみる。冒頭のリポジトリを git clone して docker-compose up して頂ければ、起動します。 README の通りに実施してみてください。 UPDATE で primary DB へ SELECT で secondary DB へ アクセスしているのがわかります。 TODO Rails 等フレームワークで Transaction 処理がどう扱われるか調査 むしろここが肝心ですね。すでにお調べいただいている方、ご教示くださいましたら幸いです。 以上参考になれば幸いです。","link":"/2019/03/24/2019-03-25-proxysql-db-read-write-endpoint/"},{"title":"Terraform 運用ベストプラクティス 2019 ~workspace をやめてみた等諸々~","text":"2020-05-05 追記 2020年春のベストプラクティス更新しています。 Terraform ベストプラクティス 2020 春 ~moduleやめてみた~ToC 概要 結論 module やめてみた これ modules/stg, modules/common どっち？ 問題 そもそも何故分けた？ だから module やめてみた module が便利な例 ベストプラクティスを探す旅は続く 概要#Infra… 以前 terraform で workspace 毎に tfstate 管理する方法を執筆しましたが、実運用上いくつかの問題がありました。 結論、現在は workspace 運用をやめています。 terraform workspace で環境毎に tfsate 管理terraform workspace で環境毎に tfsate 管理した話です。 追記 2019/04/17追記時点で workspace は運用時点の問題が多くあった為、利用していません。以下記事ご参考いただければと思います。 Terraform 運用ベストプラクティス 20… workspace 運用例まずは実際の運用例です。 もっとうまいことやってるぞ！という話はあろうかと思いますが、まずはありがちなケースを紹介します。 例) セキュリティグループ作成以下の要件を実現するセキュリティグループを作成するとします。 要件 stg では、社内で Wifi の ip からのみアクセス可 prd では、ip 制限なくアクセス可 サンプルコード variables.tf variable \"ips\" { type = \"map\" default = { stg.cidrs = \"12.145.67.89/32,22.145.67.89/32\" prod.cidrs = \"0.0.0.0/0\" } } security_group.tf resource \"aws_security_group\" \"hoge\" { name = \"${terraform.workspace}-hoge-sg\" vpc_id = \"${aws_vpc.vpc_main.id}\" } resource \"aws_security_group_rule\" \"https\" { security_group_id = \"${aws_security_group.hoge.id}\" type = \"ingress\" from_port = 443 to_port = 443 protocol = \"tcp\" cidr_blocks = [\"${split(\",\", lookup(var.ips, \"${terraform.workspace}.cidrs\"))}\"] } resource \"aws_security_group_rule\" \"https\" { security_group_id = \"${aws_security_group.hoge.id}\" type = \"egress\" from_port = 0 to_port = 0 protocol = \"-1\" cidr_blocks = [\"0.0.0.0/0\"] } 実際に terraform を plan/apply する前にまずは terraform workspace を定義する必要があります。 terraform workspace new stg // 既に作成されていたらエラーとなります。 terraform workspace select stg // terraform workspace = stg とした場合の tfstate をローカルのメモリ上で管理します。 terraform init 上記のような処理があって、初めて、 `variable \"ips\"` の `stg.cidrs`, `prd.cidrs` が利用できるようになります。 こちらを運用しようとしてみると以下の様な問題にぶつかりました。 実運用との相性が悪いステージングのみに反映させたい、という時にどう運用したら良いでしょうか。 ステージング用、本番用に設定して、プルリクエストが通って、サンプルのコードを master にマージしていたらどうでしょう？ 本番にもデプロイして良さそうに見えます。 いや、むしろ反映されていなければ、混乱します。 その後に master にマージして、本番に反映させたいコードがあった時に、サンプルコードの部分は反映させたくない！と言っても反映されてしまいます。 かといって、以下の様なコードを複数リソースに入れていくのは、余計なステップ数も増え、脳内でリソースが消費されます。レビューするのも辛いです。 count = \"${terraform.workspace == \"stg\" ? 1: 0}\" では、本番用は設定しなければいいじゃないか！と言って設定しないと、本番用はエラーを出す様になり、その他の反映が何もできなくなります。 これはステージングも本番も同じファイルを参照している為に発生しています。 また、 workspace を利用していると以下の様な問題もありました。 stg, prd 以外に新たに workspace を追加したい場合以下要望があった場合にどうでしょうか。 「負荷試験をする為に本番同様の環境を用意してください」 「外部 API との連携試験をしたいので環境を別途増やして欲しいです！」 例えば、 負荷試験環境を用意しようとすると、 loadtst という workspace を用意するとしたら variables.tf を以下のように修正が必要です。 variable \"ips\" { type = \"map\" default = { loadtst.cidrs = \"12.145.67.89/32,22.145.67.89/32\" // 追加 stg.cidrs = \"12.145.67.89/32,22.145.67.89/32\" prod.cidrs = \"0.0.0.0/0\" } } 上記例ですと variable \"ips\" に 1行加えただけで良いですが、実際は あらゆる変数に `loadtst.*** = ***` というコードを追加していく必要があります。 workspace が増える毎に step 数が増え、ファイルの見通しが悪くなります。 また、以下の様なコードがあると、こちらも脳内リソースを消費し、疲弊します。 lookup(var.ips, \"${terraform.workspace}.cidrs\") \"${terraform.workspace == \"stg\" ? hoge: moge}\" workspace 運用をまとめるとworkspace の利用はリソースを複数環境で共有する ことで運用する想定の為に、可読性の悪化、実運用との乖離がありました。 新たに workspace 追加する際に、全ての変数 map に追加しなければならない。 → コードの見通しが悪くなる。 → 新規環境の構築難易度が上がる。 ステージングのみに反映という時の実運用が困難 → ステージングも本番も同じファイルを参照している為、ファイルの中でステージングの場合は？と処理を分ける必要が出てきてしまう。 今、どの workspace なのかがわかりずらく、 terraform apply する際にかなり躊躇してしまう。 → 実際 `terraform apply` 実行前に `terraform workspace show` で workspace 確認しても、実行中で少し時間が経つと、「あれ？どっちだっけ？」と不安になり、 Terminal を遡って確認することがあったりしました。 ではどうすると良いか？徹底的に workspace をやめます。 = DRY な設計しよう！ これに尽きます。 実際にどうしたか以下まとめました。 ディレクトリ構成は以下のようにしました。modules/common ... stg, prd どちらの環境でも共通して同構成で作成するリソースを置きます。 modules/stg,prd ... 個々に異なる構成となるリソースを置きます。*1 . ├── README.md ├──envs/ │ ├── prd │ │ ├── backend.tf │ │ ├── main.tf │ │ ├── provider.tf │ │ ├── region.tf │ │ ├── templates │ │ │ └── user-data.tpl │ │ └── variable.tf │ └──stg/ │ ├── backend.tf │ ├── main.tf │ ├── provider.tf │ ├── region.tf │ ├── templates │ │ └── user-data.tpl │ └── variable.tf │ └──modules ├── common │ ├── bastion.tf │ ├── bucket_logs.tf │ ├── bucket_static.tf │ ├── certificate.tf │ ├── cloudfront.tf │ ├── cloudwatch.tf │ ├── codebuild.tf │ ├── codepipeline.tf │ ├── network.tf │ ├── output.tf │ ├── rds.tf │ ├── redis.tf │ ├── security_group.tf │ └── variable.tf ├── prd │ ├── admin.tf │ ├── admin_autoscaling_policy.tf │ ├── api.tf │ ├── app.tf │ ├── ecr.tf │ ├── iam_ecs.tf │ ├── output.tf │ ├── variable.tf │ └── waf.tf └── stg ├── admin.tf ├── api.tf ├── app.tf ├── ecr.tf ├── iam_ecs.tf ├── output.tf ├── variable.tf └── waf.tf 前出のセキュリティグループの作成を例にするとどうなるか以下の様になります。 envs/prd/variables.tf variable \"cidrs\" { default = [ \"0.0.0.0/0\", ] } envs/stg/variables.tf variable \"cidrs\" { default = [ \"12.145.67.89/32\", \"22.145.67.89/32\", ] } envs/common/security_group.tf resource \"aws_security_group\" \"hoge\" { name = \"${terraform.workspace}-hoge-sg\" vpc_id = \"${aws_vpc.vpc_main.id}\" } resource \"aws_security_group_rule\" \"https\" { security_group_id = \"${aws_security_group.hoge.id}\" type = \"ingress\" from_port = 443 to_port = 443 protocol = \"tcp\" cidr_blocks = [\"${var.cidrs\"))}\"] } resource \"aws_security_group_rule\" \"https\" { security_group_id = \"${aws_security_group.hoge.id}\" type = \"egress\" from_port = 0 to_port = 0 protocol = \"-1\" cidr_blocks = [\"0.0.0.0/0\"] } もし stg だけに反映させたいセキュリティグループであれば、 `envs/stg/security_group.tf` に作成したいセキュリティグループを記述します。 これで stg だけ反映という実運用をカバーできます。 また、負荷試験環境 ( `loadtst` ) という環境を用意したい場合は、以下の様にコピーし、変数を修正すれば良いです。 `envs/prd` → `envs/loadtst` `modules/prd` → `modules/loadtst` 多少構成に変更があろうとも、 loadtst 関連のリソースが prd, stg に影響することはない様に作成できます。 terraform コーディングルール以下のような workspace の切り替えを利用したコードを利用しないことです。 lookup(var.ips, \"${terraform.workspace}.cidrs\") \"${terraform.workspace == \"stg\" ? hoge: moge}\" また、以下も NG とします。 stg だけ異なるのであれば、 modules/stg,prd と分けるべきです。 \"${var.env == \"stg\" ? hoge: moge}\" terraform 実行手順stg, prd 各環境構築は `envs/stg`, `envs/prd` ディレクトリに移動し、 以下実行します。 terraform init terraform get -update terraform plan terraform apply AWS credentials の扱いstg, prd で同じ AWS Account を利用する場合、プロジェクトの root に direnv 等、 .envrc を置いて、運用するのが良いと思います。 stg, prd で異なる AWS Account を利用する場合、 `envs/(stg,prd)` 以下に .envrc をそれぞれ配置し、上記 `terraform 実行手順` を実行すれば良いです。 プロジェクト毎の terraform バージョンの違いの対応tfenv で対応します。 macOS%$ brew install tfenv 以前の執筆記事では terraform を one-off container で実行しバージョン差異を吸収する様にしていましたが、コマンドが長くなり、管理も煩雑になるので、tfenv が望ましいです。 こちらも運用してみての実感です。 その他これはしといた方がオススメ？レベルですが、 provider で バージョン固定外した方が良かったです。 provider aws { version = \"1.54.0\" region = \"ap-northeast-1\" } 固定されていて、最新のリソースが利用できない時があります。*2 その時は、バージョン固定でなく、アップデートしていく方向で修正した方が、最新に追従できます。 総評実運用をしてみて、 workspace はやめておいた方がいいかなと感じたことをまとめました。 勿論、 workspace の良さを知り尽くしてないからこういう意見になっているとも思いますので、一概に否定する意図はありません。 リポジトリの整理がついたら現段階で公開できるところをしていこうと思います！ 以上 Terraform 運用されてる方の知見になりましたら幸いです。 Infrastructure as Code ―クラウドにおけるサーバ管理の原則とプラクティス作者: Kief Morris,宮下剛輔,長尾高弘出版社/メーカー: オライリージャパン発売日: 2017/03/18メディア: 単行本（ソフトカバー）この商品を含むブログ (2件) を見る *1:ECS + RDS + Redis 構成で CodePipeline からデプロイするサンプル terraform です。 *2:Aurora MySQL が作れない！と思ったら、バージョン固定してた為だったことがありました。","link":"/2019/04/16/2019-04-17-terraform-2019-workspace/"},{"title":"Nginx IP 直アクセス不許可 &amp; LB ヘルスチェック設定","text":"よく設定している Nginx の configure file のアクセス元によっての振り分け方をまとめました。 1LB → Nginx → Rails Nginx 設定 conf.d/default.conf 1234567891011121314151617181920212223# cannot allow ip directserver { listen 80; server_name _; return 444;}# healthcheck from LBserver { listen 80 default_server; listen [::]:80 default_server; root /work/app/public; location = /healthcheck.html { access_log off; proxy_pass https://puma; }}server { listen 80; server_name example.com; ... IP 直アクセス禁止server_name _ とすることで、ip 直アクセスをターゲットにしています。 12345server { listen 80; server_name _; return 444;} LB からのヘルスチェックLB からヘルスチェックを向ける先を default_sever 設定することで、この server ディレクティブを参照します。 12345678910server { listen 80 default_server; listen [::]:80 default_server; root /work/app/public; location = /healthcheck.html { access_log off; proxy_pass https://puma; }} 上記 config file は、AWS ALB のヘルスチェックパスを /healthcheck.html とし、その向け先を Rails puma にしています。 Rails 側で以下の様に gem ‘ok_computer’ に向けるのも良し、独自にレスポンス返すも良しです。 1get 'healthcheck.html', to: 'ok_computer/ok_computer#index' ドメイン指定example.com でアクセスされた際にこちらの server ディレクティブを参照します。 1234server { listen 80; server_name example.com; ... ドメイン指定の悪い例以前は以下の様に指定し、ip 直アクセス、ヘルスチェック対応していました。 1234567891011121314server { listen 80; server_name example.com; if ($host != &amp;#34;example.com&amp;#34;) { return 444; } location = /healthcheck.html { access_log off; proxy_pass https://puma; } ...} 勿論これでも動作します。ですが、やや可読性が悪いです。 マルチドメインでの IP 直アクセス不許可に対応をする際にも、この if 文がどんどん長くなります。 その為、向け先の意図毎に server {} を小まめに分ける運用の方が可読性が高く、実運用していてメンテナンサビリティが高いと感じました。 以上参考になれば幸いです。","link":"/2019/04/23/2019-04-24-nginx-ip-lb/"},{"title":"EC2 Instance Connect で AWS EC2 への ssh 管理を IAM User or Group で簡単に♪","text":"概要2019-06-28 に EC2 Instance Conncet が発表されました！ これによって、セキュリティグループと IAM 権限で ssh アクセス許可が可能になります。 例えば、 会社の IP からのみ、特定の IAM User Group に所属している IAM User に ssh アクセス権限を付与、 別のプロジェクトへ異動した、退職した場合は、その IAM User Group から削除で ssh アクセス権限を剥奪できます。 試験環境macOS 10.14.3 で試しました。 事前準備$ pip install -U awscli $ aws s3api get-object --bucket ec2-instance-connect --key cli/ec2instanceconnectcli-latest.tar.gz ec2instanceconnectcli-latest.tar.gz $ sudo pip install ec2instanceconnectcli-latest.tar.gz 発行した IAM User のパーミッション権限に以下を追加 { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"EC2InstanceConnect\", \"Action\": [ \"ec2:DescribeInstances\", \"ec2-instance-connect:SendSSHPublicKey\" ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ] } この辺りは terraform 管理案件ですね。 EC2 Instance Conncect 対応 OS Ubuntu>=16.04 AmazonLinux2>=2.0.20190618 ssh ログインする EC2側の設定Ubuntu&gt;=16.04ec2-instance-connect をインストールしておく必要があります。 $ sudo apt-get update && sudo apt-get install ec2-instance-connect $ dpkg -l | grep ec2-instance-connect ii ec2-instance-connect 1.1.9-0ubuntu3~18.04.1 all Configures ssh daemon to accept EC2 Instance Connect ssh keys AmazonLinux2&gt;=2.0.20190618ec2-instance-connect は設定済みです。 セキュリティグループssh ログイン先となる EC2 インスタンスのセキュリティグループはアクセス元から ssh (22 port) を開けておく必要があります。 ssh ログインしてみるlocal%$ mssh ubuntu@i-0f123456abcdefg --profile &lt;profile&gt; --region ap-northeast-1 一見、誰しもが ubuntu でログインしていて監査が不安になりますが、 CloudTrail はちゃんと誰がログインしたか見ています。 CloudTrailCloudTrail 以下イベントでログが残っています。 SendSSHPublicKey DescribeInstances SendSSHPublicKey の「イベントの表示」ボタンクリックで JSON が表示されますが、その中で、アクセス元 IP, IAM User Arn、アクセス先 インスタンスIDがわかります。 { \"eventVersion\": \"1.05\", \"userIdentity\": { \"type\": \"IAMUser\", \"principalId\": \"ABCDEFGHIJK....\", \"arn\": \"arn:aws:iam::123456789012:user/hogehoge\", \"accountId\": \"123456789012\", \"accessKeyId\": \"AKIxxxxxxxxxxxxxxxx\", \"userName\": \"hogehoge\", \"sessionContext\": { \"attributes\": { \"mfaAuthenticated\": \"false\", \"creationDate\": \"2019-06-28T06:18:50Z\" } } }, \"eventTime\": \"2019-06-28T06:18:51Z\", \"eventSource\": \"ec2-instance-connect.amazonaws.com\", \"eventName\": \"SendSSHPublicKey\", \"awsRegion\": \"ap-northeast-1\", \"sourceIPAddress\": \"xx.xxx.xxx.xxx\", \"userAgent\": \"aws-ec2-instance-connect-cli/1.0.0 Python/2.7.16 Darwin/18.2.0 Botocore/1.12.179\", \"requestParameters\": { \"instanceId\": \"i-0f.......\", \"osUser\": \"ubuntu\", \"SSHKey\": { \"publicKey\": \"ssh-rsa AAAAB....rHb\" } }, \"responseElements\": null, \"requestID\": \"01234567-890a-1234-5b6d-......\", \"eventID\": \"f51...\", \"eventType\": \"AwsApiCall\", \"recipientAccountId\": \"123456789012\" } こちらで EC2 インスタンスのアクセス履歴等はわかります。 まとめこれまで ssh アカウント管理は手間でしたが、IAM 権限での管理によって非常に楽になりました♪ CloudTrail で監査もバッチリ！","link":"/2019/06/27/2019-06-28-ec2-instance-connect-aws-ec2-ssh-iam-user-or-group/"},{"title":"EC2 Instance Connect API で ssh ログインできるインタラクティブ cli tool &quot;omssh&quot; を作ってみました。","text":"概要oreno-mssh、またの名を omssh という AWS EC2 Instance Connect API を利用した ssh ログインツールを作成しました。 View post on imgur.comimgur.com 作ろうと思った経緯以前 EC2 Instance Connect API の登場により、EC2 Instance ID 指定で ssh ログインできる様になりました。 これにより、ssh ログイン接続するメンバーに秘密鍵を渡す、公開鍵を登録する等の作業が不要となりました。 EC2 Instance Connect で AWS EC2 への ssh 管理を IAM User or Group で簡単に♪概要2019-06-28 に EC2 Instance Conncet が発表されました！ これによって、セキュリティグループと IAM 権限で ssh アクセス許可が可能になります。 例えば、 会社の IP からのみ、特定の IAM User Group に所属している IA… mssh を使用した場合、以下の様なコマンドで ssh ログインできます。 // Amazon Linux への ssh mssh &lt;EC2 Instance ID&gt; --profile &lt;profile&gt; // Ubuntu mssh ubuntu@&lt;EC2 Instance ID&gt; --profile &lt;profile&gt; mssh を利用するには `&lt;EC2 Instance ID&gt;` 情報が必要で、毎回 Instance ID を調べる手間がありました。 もちろん踏み台サーバであれば、そうそう再起動され Instance ID が変更されることはないのでメモっておけば良いのでしょうが、 数が多くなると、管理が大変です。 その手間を fuzzyfinder でインタラクティブに解決しようと思いました。 使用方法README.md にもありますが、以下ステップでインストールできます。 $ git clone https://github.com/kenzo0107/omssh $ cd omssh $ make build && make install 今後現在、EC2 Instance Connect を利用した運用に切り替えきれないところがあります。 理由は、EC2 インスタンスを Public Subnet に配置していないと EC2 Instance Connect API が利用できない為、踏み台までは EC2 Instance Connect API を利用し ssh ログインできたけど、その先は、秘密鍵が必要になる為です。 おそらく近々 Private Subnet でも EC2 Instance Connect が利用できる様になるのでは？と期待しています。 そうなれば、踏み台にも omssh を置いて、鍵を意識せず、 IAM の権限だけで、 ssh 権限を管理できる様な世界が実現できます。 AWS サポートに願いを伝えておきます♪","link":"/2019/08/03/2019-08-04-ec2-instance-connect-api-ssh-cli-tool-omssh/"},{"title":"puppeteer on Docker","text":"概要備忘録です。 Puppeteer をローカル環境を汚さず、 Docker 環境上で実行すべくまとめました。 サンプルスクリプトは example.com のスクリーンショットを取得する、というシンプルなものです。 kenzo0107/puppeteer-on-dockerContribute to kenzo0107/puppeteer-on-docker development by creating an account on GitHub.","link":"/2019/08/15/2019-08-16-puppeteer-on-docker/"},{"title":"puppeteer で radio ボタンチェック","text":"概要puppeteer というスクレイピングツールで radio ボタンをチェックする為の備忘録です。 例題以下のような radio ボタングループがあるとします。 &lt;input type=&quot;radio&quot; name=&quot;maker&quot; value=&quot;1&quot;&gt; クリスタル映像 &lt;input type=&quot;radio&quot; name=&quot;maker&quot; value=&quot;2&quot;&gt; ポセイドン企画 答えクリスタル映像 を選びたい場合は、以下のようにします。 page.evaluate でブラウザ内での操作結果を返すようにします。 ブラウザ内なので、 document.querySelector が使えます。 そこで、 `checked = true` しています。 const selectedRadioSelector = `input[type=&quot;radio&quot;][value=&quot;1&quot;]` await page.evaluate( s =&gt; (document.querySelector(s).checked = true), selectedRadioSelector ) 失敗例click 処理は軒並み失敗しました。 `page.WaitFor` すると成功する、という記事を見ましたが、自身の環境 puppeteer (version=1.19.0) では、失敗してしまいました。 page.click const selectedRadioSelector = `input[type=&quot;radio&quot;][value=&quot;1&quot;]` page.click(selectedRadioSelector) radio 要素を捕まえて、click r = page.$(selectedRadioSelector) r.click() document.querySelector().click() const selectedRadioSelector = `input[type=&quot;radio&quot;][value=&quot;1&quot;]` await page.evaluate( s =&gt; (document.querySelector(s).click()), selectedRadioSelector ) 応用以下のような radio ボタングループがあるとします。 上のと比べると label タグが追加されてます。 &lt;input type=&quot;radio&quot; id=&quot;group1&quot; name=&quot;maker&quot; value=&quot;1&quot;&gt; &lt;label for=&quot;group1&quot;&gt;クリスタル映像&lt;/label&gt; &lt;input type=&quot;radio&quot; id=&quot;group2&quot; name=&quot;maker&quot; value=&quot;2&quot;&gt; &lt;label for=&quot;group1&quot;&gt;ポセイドン企画&lt;/label&gt; このラベルを正規表現でマッチする方をチェックしてみます。 \"映像\" という文字を含む方をチェックする const regex = &quot;映像&quot; const regexpLabel = new RegExp(regex, 'g') const r = await page.$$('input[type=&quot;radio&quot;]') label: for (const i in r) { // radio ボタンの id 要素 const id = await (await r[i].getProperty('id')).jsonValue() // radio ボタンの value 要素 const value = await (await r[i].getProperty('value')).jsonValue() // ラベル textContent 取得 (&quot;クリスタル映像&quot;, &quot;ポセイドン企画&quot; を取得) const label = await page.$(`label[for=&quot;${id}&quot;]`) const labelContent = await (await label.getProperty( 'textContent' )).jsonValue() // ラベルの textContent が &quot;映像&quot; を含む場合、 true if (labelContent.match(regexpLabel)) { const selectedRadioSelector = `input[type=&quot;radio&quot;][value=&quot;${value}&quot;]` await page.evaluate( s =&gt; (document.querySelector(s).checked = true), selectedRadioSelector ) // radio ボタンにチェック入れたので処理終了 break label } } まとめ非常につまづきやすい点だったので備忘録として残しました。 参考になれば幸いです。 WEB+DB PRESS Vol.109作者: 佐藤歩,加藤賢一,原一成,加藤圭佑,大塚健司,磯部有司,村田賢太,末永恭正,久保田祐史,吉川竜太,牧大輔,ytnobody(わいとん),前田雅央,浜田真成,竹馬光太郎,池田拓司,はまちや2,竹原,原田裕介,西立野翔磨,田中孝明出版社/メーカー: 技術評論社発売日: 2019/02/23メディア: 単行本この商品を含むブログを見る","link":"/2019/09/05/2019-09-06-puppeteer-radio/"},{"title":"Ansible FAILED! &#x3D;&gt; {&quot;msg&quot;: &quot;to use the &#39;ssh&#39; connection type with passwords, you must install the ssh pass program&quot;} on MacOS","text":"MacOS で Ansible を利用した所、掲題のようなエラーが発生しました。 その際の対策です。 1brew install https://git.io/sshpass.rb インストール完了まで少々時間かかりました。 以上参考になれば幸いです。 参照Ansible 2.3.1 - sshpass Error","link":"/2019/09/18/2019-09-19-ansible-failed-msg-to-use-the-ssh-connection-type-with-passwords-you-must-install-the-ssh-pass-program-on-macos/"},{"title":"AWS ApplicationLoadBalancerリスナールールで特定 IP 以外をメンテナンスページ表示","text":"概要AWS で運用している Web サービスでメンテナンスが必要となり、ALB でメンテ切り替えをした際の対応をまとめました。 手順ALB Listener 一覧からルール変更をします。 ※ 今回 2 ポートのみ解放しており、80 は 443 に転送してるので、443 のみ対応しました。 その後、 送信元IP = 社内IP (ex. 11.22.33.44/32 ) → default のTargetGroup へ転送 で「保存」 社内IP以外の送信元 IP 全て ( 0.0.0.0/0 ) → 503 text/html メンテ文言をレスポンス で 「保存」 以上で 社内 IP は、通常通りアクセス可、それ以外はメンテナンスページを表示させることができました。 まとめてルールを追加して保存が出来ず、1つずつルール追加で保存になります。 レスポンスできる Content-Type って何があるの？Content-Type に application/json 等も返せるので、 API サーバのメンテ時にはこちらを利用して文言を渡しました。 ちょっとした注意最大文字数が 1024 文字でした♪ CSS を レスポンス本文 に追加すると文字数 1024 を超えてしまいそうなので、S3 にアップロードし公開し、そちらを参照するようにしたりしました。","link":"/2019/09/29/2019-09-30-aws-applicationloadbalancer-ip/"},{"title":"terraform 0.11 系に対応した GitHub Actions 作った &amp;  tflint も入れてみた♪","text":"概要Terraform 用の GitHub Actions として hashicorp 社にて以下リポジトリが用意されています。 hashicorp/terraform-github-actions ですが、上記のリポジトリでは、 terraform の最新版 (2019-09-30 時点 0.12.9) にのみ適用しています。 hashicorp/terraform-github-actions を folk して0.11 系がなかった為、 0.11 系に対応した terraform-github-actions を以下リポジトリに作成しました。 kenzo0107/terraform-github-actions terraform の古いバージョンについての対応は、 terraform 公式に以下リンクにて記載があります。 Terraform Versions - Terraform GitHub Actions - Terraform by HashiCorp 古いバージョンは folk して自分で作ってね♪ と書いてあります。 ついでに以下追加してみました。 tflint 追加 実行するディレクトリを表示 使い方以下のような terraform プロジェクトがあるとします。 12345678910111213├── envs│ ├── prd│ │ ├── backend.tf│ │ ├── main.tf│ ...│ │ └── variable.tf│ └── stg│ ├── backend.tf│ ├── main.tf│ ...│ └── variable.tf└── modules ├── ... GitHub Actions 設定方法以下 2 ファイルを root ディレクトリに配置します。 .github/workflows/main.yml .github/workflows/fmt.yml 123456789101112131415161718├── .github│ └── workflows│ ├── main.yml│ └── fmt.yml│├── envs│ ├── prd│ │ ├── backend.tf│ │ ├── main.tf│ ...│ │ └── variable.tf│ └── stg│ ├── backend.tf│ ├── main.tf│ ...│ └── variable.tf└── modules ├── ... .github/workflows/main.yml12345678910111213141516171819202122232425262728293031323334353637383940414243444546name: Terraformon: [pull_request]jobs: on-pull-request: name: On Pull Request strategy: matrix: env: [stg, prd] runs-on: ubuntu-latest steps: - name: Checkout Repo uses: actions/checkout@v1 - name: ${{ matrix.env }} Terraform Init uses: kenzo0107/terraform-github-actions/init@v0.6.0 env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} TF_ACTION_WORKING_DIR: './envs/${{ matrix.env }}' AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }} AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }} - name: ${{ matrix.env }} Terraform Validate uses: kenzo0107/terraform-github-actions/validate@v0.6.0 env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} TF_ACTION_WORKING_DIR: './envs/${{ matrix.env }}' AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }} AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }} - name: ${{ matrix.env }} Terraform Lint uses: kenzo0107/terraform-github-actions/lint@v0.6.0 env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} TF_ACTION_WORKING_DIR: './envs/${{ matrix.env }}' - name: ${{ matrix.env }} Terraform Plan uses: kenzo0107/terraform-github-actions/plan@v0.6.0 env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} TF_ACTION_WORKING_DIR: './envs/${{ matrix.env }}' AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }} AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }} ちょっと解説Pull Request をトリガーに実行されます。1on: [pull_request] リポジトリをチェックアウト12- name: Checkout Repo uses: actions/checkout@v1 matrix で stg, prd を並列実行します。123strategy: matrix: env: [stg, prd] terraform init, validate, lint, plan1234567 - name: ${{ matrix.env }} Terraform Init... - name: ${{ matrix.env }} Terraform Validate... - name: ${{ matrix.env }} Terraform Lint... - name: ${{ matrix.env }} Terraform Plan 言わずもがな、以下を実行しています。 terraform init terraform validate terraform lint terraform plan kenzo0107/terraform-github-actions/init@v0.6.0 が terraform v0.11.14 に対応しています。 init, validate, lint は指摘事項がある場合は、Pull Request にコメントしてくれます。 terraform plan は必ず実行結果を貼り付けてくれます。 これはレビュワーに有難い機能です。 コードの変更内容と terraform plan 内容の整合性が取れているかどうかが重要なレビュー観点となる為です。 実行パス指定パスを移動してから terraform plan 等を実行したい場合に以下環境変数に指定します。 1TF_ACTION_WORKING_DIR: './envs/${{ matrix.env }}' kenzo0107/terraform-github-actions/plan@v0.6.0 では、上記の設定した TF_ACTION_WORKING_DIR を terraform plan 実行内容と共に表示するようにしています。((hashicorp/terraform-github-actions では、実行したディレクトリパスは Pull Request コメントに乗らない様になってます。)) これは terraform plan 実行内容から stg, prd どちらで実行したかわかりずらく、レビュワーを困惑させる可能性がある為です。 secrets の設定123GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }} secrets.GITHUB_TOKEN は secrets にトークンを設定する必要がありません。 個人的に GitHub が実行する CI/CD だからこそ実現できる秘匿性のある管理方法で、 Actions の大きな利点だと思います。 その他は、 settings &gt; secrets で設定します。 .github/workflows/fmt.ymlterraform fmt を stg, prd 等は関係なく、リポジトリのルートディレクトリで実行するので.github/workflows/main.yml とは分けました。 こちらも Pull Request をトリガーとして実行されます。 1234567891011121314151617name: Terraformon: [pull_request]jobs: on-pull-request: name: On Pull Request runs-on: ubuntu-latest steps: - name: Checkout Repo uses: actions/checkout@v1 - name: Terraform fmt uses: kenzo0107/terraform-github-actions/fmt@v0.6.0 env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} 総評GitHub Actions に触れてみようとした題材として非常に簡易だったのでとっつきやすかったです。 そして何より、手間かけましたが、 0.12 系に対応した方が早かったかもしれない… やんごとなき理由で 0.11 にしている場合以外は、最新に追従した方が良いですね♪ 以上参考になれば幸いです。","link":"/2019/09/30/2019-10-01-terraform-0-11-github-actions-tflint/"},{"title":"Datadog Agent for ECS Launch Type&#x3D;EC2","text":"概要ECS 起動タイプ EC2 にてタスク定義に datadog/agent:latest を設定したがメトリクスが取得できない事象がありました。 Infrastructure &gt; Containers には datadog/agent:latest を設置したタスク定義内のコンテナ情報は一覧に表示されてますが、メトリクスが取れていない、という状況でした。 結論ttps://docs.datadoghq.com/json/datadog-agent-ecs.json 参考に、以下の様な volume mount の設定が必要でした。 1234567891011121314datadog: image: datadog/agent:latest environment: DD_API_KEY: ${DD_API_KEY} logging: driver: awslogs options: awslogs-group: ${LOG_GROUP} awslogs-region: ${REGION} awslogs-stream-prefix: datadog volumes: - /var/run/docker.sock:/var/run/docker.sock:ro - /proc/:/host/proc:ro - /sys/fs/cgroup/:/host/sys/fs/cgroup:ro ドキュメントよく読もう、を身につまされる想いでした。 参考Amazon Elastic Container Service (ECS)","link":"/2019/10/16/2019-10-17-datadog-agent-for-ecs-launch-type-ec2/"},{"title":"Golang で関数のデフォルト引数を指定する","text":"概要Ruby で関数のデフォルト引数を設定する場合は以下のように指定できます。 12345678def hoge(a, b = 2) c = a + bendc = hoge(1)p c// c = 3 Golang だと以下の様なデフォルト引数の定義ができません。 1234func hoge(a, b = 2 int) string { c := a + b return c} こんな時にどうしようかと探っていると、 Functional Option Pattern に当たりました。 Functional Option Pattern以下 2 記事が有名で必読です。 Rob Pike: Self-referential functions and the design of options at 2014-01-24 Dave Cheney: Functional options for friendly APIs | Dave Cheney at 2014-10-17 任意の値を定義する・しないで関数を分けると引数分、メソッドが増え、煩雑になります。 123456789const defaultB = 2func Hoge(a int) int { return a + defaultB}func HogeWithB(a, b int) int { return a + b} こういった煩雑さを解決する Golang の特性を活かした解決法が Functional Option Pattern です。 Functional Option Pattern で書き換え12345678910111213141516171819202122232425262728293031323334353637383940414243package mainconst ( defaultB = 2)type configs struct { b int}type Option func(c *configs)func WithB(v int) Option { return func(c *configs) { c.b = v }}var args *configsfunc init() { args = &amp;configs{ b: defaultB, }}func Hoge(a int, options ...Option) int { for _, option := range options { option(args) } c := a + args.b return c}func main() { c := Hoge(1) fmt.Println(c) // 3 d := Hoge(1, WithB(9)) fmt.Println(d) // 10} デフォルト引数の指定が効いているのがわかります。 Option という引数に *configs の設定を持つ関数を定義し、 その関数で各値を任意で設定することで、デフォルトの設定を上書いています。 こうすることで、 設定したい任意の値だけを設定し、その他はデフォルト値を参照する、ということができます。 よし完璧だ！と思ったら…テストでこける。。。1fmt.Println(reflect.DeepEqual(WithB(12), WithB(12)) // false WithB は関数型が返るので、 reflect.DeepEqual では false になります。 reflect.DeepEqual のコードを見てみると関数型 (reflect.Func) で Nil でなければ、 false が返るようになってます。 コメントにある // Can't do better than this: が推して知るべし。 https://github.com/golang/go/blob/master/src/reflect/deepequal.go#L126-L131 Functional Option Pattern にもう一手間加える123456789101112131415161718192021222324252627282930313233343536373839404142434445464748const ( defaultB = 2)type configs struct { b int}type Option interface { Apply(*configs)}type B intfunc (o B) Apply(c *configs) { c.b = int(o)}func WithB(v int) B { return B(v)}var args *configsfunc init() { args = &amp;configs{ b: defaultB, }}func Hoge(a int, options ...Option) int { for _, option := range options { option.Apply(args) } c := a + args.b return c}func main() { c := Hoge(1) fmt.Println(c) // 3 d := Hoge(1, WithB(9)) fmt.Println(d) // 10 fmt.Println(reflect.DeepEqual(WithB(12), WithB(12))) // true} WithB は int 型の値を返し、 reflect.DeepEqual は true を返します。 Option を interface で定義することで、どの様な型にでも呼び出せるのを利用しつつ、Apply メソッドを定義して、 configs の上書きを図る、という算段です。 以下 googleapis/google-api-go-client で定義されている Functional Option Pattern が素敵だったので、こちらとても参考になります。 https://github.com/googleapis/google-api-go-client/blob/master/option/option.go まとめ軽い気持ちで関数のデフォルト引数の設定どうやるんだろうか？と調べたら思わぬ展開になって、奥深さを感じました。 以上ご参考になれば幸いです。","link":"/2019/11/10/2019-11-11-golang/"},{"title":"Ruby inject (Enumerable) メソッドを学ぶ","text":"概要お題を通して Ruby で inject メソッドを学びました。その備忘録です。 お題123,456 円を紙幣・硬貨が一番少なくなる様になる組み合わせを求めてください。硬貨 … 1, 5, 10, 50, 100, 500 円玉紙幣 … 1000, 2000, 5000, 10000 円札 答え 10,000 円札 × 12 5,000 円札 × 0 2,000 円札 × 1 1,000 円札 × 1 500 円玉 × 0 100 円玉 × 4 50 円玉 × 1 10 円玉 × 0 5 円玉 × 1 1 円玉 × 1 inject メソッド配列の組み合わせで答えを出すとシンプルに書けます。 大きい値から順に divmod の結果 [商, 余り] を不定数個の配列 (a) の後ろに足して、r に配列の一番後ろの要素 余り が入るので、それ次の iterator の計算に利用する方法です。 12345678910[] + 123456.divmode(10000) # [12, 3456][12, 3456] + 3456.divmod(5000) # [12, 0, 3456][12, 0, 3456] + 3456.divmod(2000) # [12, 0, 1, 1456][12, 0, 1, 1456] + 1456.divmod(1000) # [12, 0, 1, 1, 456][12, 0, 1, 1, 456] + 456.divmod(500) # [12, 0, 1, 1, 0, 456][12, 0, 1, 1, 0, 456] + 456.divmod(100) # [12, 0, 1, 1, 0, 4, 56][12, 0, 1, 1, 0, 456] + 56.divmod(50) # [12, 0, 1, 1, 0, 4, 1, 6][12, 0, 1, 1, 0, 4, 1, 6] + 6.divmod(10) # [12, 0, 1, 1, 0, 4, 1, 1, 0, 6][12, 0, 1, 1, 0, 4, 1, 1, 0, 6] + 6.divmod(5) # [12, 0, 1, 1, 0, 4, 1, 1, 0, 1][12, 0, 1, 1, 0, 4, 1, 1, 0, 1] + 1.divmod(1) # [12, 0, 1, 1, 0, 4, 1, 1, 0, 1, 1, 0] 最後の 0 が余計ですが、非常にシンプルな形で答えを求られました。 ただ、この inject の計算式が初手でスッと出すのは慣れが必要な印象。 なので、一旦 each で計算の手順を確認してから inject でコードがシンプルになりそうならやってみる、っていうのが慣れるのに良さそうでした。 each メソッド each でやると loop の外側で b = [] と最終的に返す配列を定義する必要があります。 b &lt;&lt; r if p.size - 1 == index は、不要ですが、回答の配列を inject メソッドの場合と合わせました。 benchmark とってみる1234567891011121314151617require &amp;quot;benchmark&amp;quot;r = 123_456Benchmark.bmbm do |x| x.report(&amp;quot;div_inject&amp;quot;) do 10000.times do div_inject(r) end end x.report(&amp;quot;div_each&amp;quot;) do 10000.times do div_each(r) end endend each の方が速かったです。 123 user system total realdiv_inject 0.037365 0.000524 0.037889 ( 0.040733)div_each 0.024333 0.000257 0.024590 ( 0.025320) 個人的には inject の方が速くなってくれると本記事が締まったんですが今回の計算ロジックでは、each の方がパフォーマンスがよかったです。 hash を扱ってみるinject each benchmark とると、 each, hash どちらもほぼ同等でした。 まとめinject メソッドを例題を通して学びました。 例題では特にシンプルに書けましたが、パフォーマンスが格段に上がるという話ではないので使い所は見定める必要があるかなと思いました。 以上ご参考になれば幸いです。","link":"/2019/11/25/2019-11-26-ruby-inject-enumerable/"},{"title":"Fix: can&#39;t find gem bundler (&gt;&#x3D; 0.a) with executable bundle (Gem::GemNotFoundException)","text":"rbenv で複数 ruby バージョンが存在する環境下で bundle install しようとすると以下のエラーが出てしまいました。 1can't find gem bundler (&gt;= 0.a) with executable bundle (Gem::GemNotFoundException) ruby バージョンは合ってる、 Gemfile もある、 gem install bundler して bundle もある ← ここがダメだった けど、エラー ちょいちょいハマってたので備忘録とりました。 結論bundle のバージョン (2.0.2) が Gemfile.lock (1.17.1) と異なることで発生していました。 gem インストール時の bundler は 2.0.2 123456$ gem install bundlerSuccessfully installed bundler-2.0.2Parsing documentation for bundler-2.0.2Done installing documentation for bundler after 2 seconds1 gem installed Gemfile.lock での bundler は 1.17.1 123456... VERSION ruby 2.5.3p105BUNDLED WITH 1.17.1 なので、実行する bundle のバージョンを Gemfile.lock 側に合わせてあげれば実行できるようになりました。 対応12$ gem install bundler -v 1.17.1$ gem uninstall bundler -v 2.0.2 (&gt;= 0.a) というのがパッと見、ん？となってしまい、あれ、設定したのにな、と思ってるとハマるので、このエラーメッセージを見たら反応できるようにしておきたい内容でした。 以上参考になれば幸いです。","link":"/2019/12/05/2019-12-06-fix-can-t-find-gem-bundler-0-a-with-executable-bundle-gem-gemnotfoundexception/"},{"title":"Golang errcheck による defer 警告対応","text":"概要このようなコードを書いていると errcheck を実行した場合、 defer f.Close() と指摘されてしまいます。 12345678910func hoge() error { ... f, err := os.Open(fpath) if err != nil { return err } defer f.Close() ...} f.Close() は返り値が error であり、その error の返り値をチェックしていない、という警告です。 対応その為、以下のように修正することで回避 123456789101112131415func hoge() error { ... f, err := os.Open(fpath) if err != nil { return err } defer func() { err = f.Close() if err != nil { log.Fatalln(err) } }() ...} 但し、上記の場合だと error 発生した時に panic が発生します。 log.Println すると error がログ出力こそされますが、その error によって後続の処理をハンドリングすることができません。 その為、さらに以下の様に修正してみました。 1234567891011121314func hoge() (err error) { ... f, err := os.Open(filepath.Clean(fpath)) if err != nil { return err } defer func() { if er := f.Close(); er != nil { err = er } }() ...} 以下対応手順で f.Close() の error を hoge() の戻り値として返すことができます。 hoge() 関数の戻り値で err error と変数名を指定する defer している func(){} 内で f.Close() の戻り値 error を err に格納する Go Playground 参考随分前に既に掲題について話をしていた ignore defer calls #55","link":"/2019/12/08/2019-12-09-golang-errcheck-defer/"},{"title":"gosec で警告される os.Open() 対応","text":"gosec で警告される os.Open() に対応した話です。 gosec で以下のようなコードがあると 1os.Open(fname) 以下のように警告されます。 1G304 (CWE-22): Potential file inclusion via variable (Confidence: HIGH, Severity: MEDIUM) 変数でファイルパスを指定するのは、意図しないファイルパスを指定される危険性があります。 対応filepath.Clean() を使用し、よろしくないパスを綺麗に整えるようにします。 1os.Open(filepath.Clean(fname)) 以上ご参考になれば幸いです。","link":"/2019/12/08/2019-12-09-gosec-os-open/"},{"title":"Go 静的解析 &amp; 単体テスト on GitHub Actions","text":"以前、複数の AWS Account EC2 インスタンスへの接続を EC2 Instance Connect を使用しインタラクティブに ssh 接続できるツールを作成しました。 EC2 Instance Connect API で ssh ログインできるインタラクティブ cli tool “omssh” を作ってみました。 自分用にチャチャッと作ってアップしたもので手元でしかテストしておらず 、 lint や test もしてなかったのですが、 後学の為、改めて lint, test した上でコードを管理できる様になりたい 自分のリポジトリとして公開しているし、愛情持ちたい という気持ちで取り組んでみたいと思いました。 成果物kenzo0107/omssh 静的解析 on GitHub Actionsgrandcolline/golang-github-actions を利用させていただきまして以下解析しました。 goimports errcheck golint shadow staticcheck gosec GitHub Actions での設定ファイルは以下です。 kenzo0107/static_check_go.yaml 開発時にも静的解析開発時に解析できる様、 Makefile に設定しました。 123456789.PHONY: lintlint: devel-deps go vet ./... go vet -vettool=$(shell which shadow) staticcheck ./... errcheck ./... # exclude G106: Audit the use of ssh.InsecureIgnoreHostKey gosec -quiet -exclude=G106 ./... golint -set_exit_status ./... goimports に関してはエディタの設定で保存時にチェックする様にしてます。 実際に設定して動かしてみたらわかりやすいですが、以下にどんなことを指摘してくるかまとめます。 go vet ./…構文エラーを指摘してくれます。 12a := fmt.Sprintf(\"%s\", 1)fmt.Println(a) go vet -vettool=$(shell which shadow)スコープの外側で定義した変数と同名の変数がスコープの内部で使用されている場合に警告されます。 潜在的にエラーが発生する可能性があり、変数名を変更する等の対応が必要です。 1234567891011121314func hoge() error { err := errors.New(\"error occured\") { err := errors.New(\"error occured\") if err != nil { return err } } if err != nil { return err } return nil} 1./main.go:14:3: declaration of \"err\" shadows declaration at line 12 上記の場合、以下の様にスコープ内でスコープ外の変数を利用することで回避できます。 123err := errors.New(\"error occured\"){ err = errors.New(\"error occured\") ですが、この場合だと最初は err := で定義し、その後は err = で定義しなければならず、わかりづらいので、変数 err を定義した直後に処理を実施する様にすると回避できます。 1234567891011121314func hoge() error { err := errors.New(&quot;error occured&quot;) if err != nil { return err } { err := errors.New(&quot;error occured&quot;) if err != nil { return err } } return nil} staticcheck ./…バグの検出、コードの簡素化の提案、デッドコードの指摘などをしてくれます。 以下のコードで staticcheck ./... を実行すると 1234import \"github.com/aws/aws-sdk-go/aws/session\"...session.New(&amp;config) session.New 使うのは非推奨だよ、と警告してくれました。 deprecated を指摘してくれるのありがたい！ 1session.New is deprecated: Use NewSession functions to create sessions instead. NewSession has the same functionality as New except an error can be returned when the func is called instead of waiting to receive an error until a request is made. (SA1019) 以下の様に修正することで、対応できました。 1session.Must(session.NewSession(&amp;config)) errcheck ./…error を返す function の処理をチェックしているか警告してくれます。 以下のコードで errcheck ./... を実行すると 123f, err := os.Open(fpath)...defer f.Close() 以下の様に警告されます。 1pkg/utility/profile.go:19:15: defer f.Close() f.Close() が error を返しますが、その error がチェックされていないですよ、という指摘です。 123Close closes the File, rendering it unusable for I/O. On files that support SetDeadline, any pending I/O operations will be canceled and return immediately with an error.func (*os.File).Close() error 無名関数で wrap してその中で error check する様にし対応しました。 12345defer func() { if err := f.Close(); err != nil { log.Fatalln(err) }}() gosec -quiet ./…使用コードでの脆弱性を指摘してくれます。 現状の omssh では gosec -quiet ./... を実行してみると以下の様な警告が出ます。 G106 として管理されている ssh.InsecureIgnoreHostKey() を利用していることへの脆弱性が指摘されています。 123456789101112Results:[/Users/kenzo.tanaka/src/github.com/kenzo0107/omssh/omssh.go:50] - G106 (CWE-322): Use of ssh InsecureIgnoreHostKey should be audited (Confidence: HIGH, Severity: MEDIUM) &gt; ssh.InsecureIgnoreHostKey()Summary: Files: 9 Lines: 718 Nosec: 0 Issues: 1 対応法を把握できていないので -exclude=G106 オプションで回避しています。 gosec で警告されがちなコードよくありがちな gosec に警告されがちなコード例としては以下ではないでしょうか。 引数として、ファイルパスを渡して、そのファイルを開こうとしています。 123func GetProfiles(credentialsPath string) (profiles []string, err error) { f, err := os.Open(credentialsPath) ... このコードを gosec -quiet ./... してみると以下の様に警告されます。 12[/Users/kenzo.tanaka/src/github.com/kenzo0107/omssh/pkg/utility/profile.go:16] - G304 (CWE-22): Potential file inclusion via variable (Confidence: HIGH, Severity: MEDIUM) &gt; os.Open(credentialsPath) こちらの対処法としては、ファイルパスを綺麗にしてくれる filepath.Clean(string) を噛ませると回避できました。 12- f, err := os.Open(credentialsPath)+ f, err := os.Open(filepath.Clean(credentialsPath)) ../kenzo\\hoge/moge というファイルパスだと ../kenzo\\hoge/moge というファイルパスが返ります。 golint -set_exit_status ./…-set_exit_status を指定しているのは、終了ステータスを返してくれます。 以下の様なコードがあるとします。 123type EC2Iface interface { ...} golint -set_exit_status ./... すると以下の様なエラーを出力されます。 12pkg/awsapi/ec2.go:12:6: exported type EC2Iface should have comment or be unexportedFound 1 lint suggestions; failing. EC2Iface と大文字始まりなので、他パッケージからも参照できる exported type なのでコメントを書きましょうね、という指摘です。 VSCode を使っていますが、この様にカーソルを合わせるとコメントが表示されてくれます。 丁寧に書いて置いて上げるとこんな値を返すよ〜とかわかりやすいですね。 面倒くさがらずコメント書きましょう。 単体テスト on GitHub Actionsこちらはシンプルで test を実行し coverage を CodeCov に上げる様にしました。また、そのカバレッジを README にラベルとして表示できます。 CODECOV_TOKEN は GitHub の Settings &gt; Secrets で設定しておきます。 kenzo0107/test_go.yaml まとめ静的解析と単体テストを追加したことで コード変更がしやすくなった。 Golang の求めるコードの書き方を学ぶことができた。 aws-sdk-go の mock の作成の仕方を学ぶことができた。 というご利益がありました。 単体テストで 100% を目指しましたが、ssh 接続周りで手こずってカバレッジが微増でした。 ssh 接続する際に仮想的な ssh server を起動する所まではよかったんですが、(*ssh.Session).RequestPty するとそこで処理待ちが発生し、テストが進まなくなってしまいました。 良い案ありましたらご教示いただけましたら幸いです m(_ _)m 今回の知見を活かしてAWS の EC2 や RDS 等の Reserved Instance の使用率・カバレッジ率を Datadog にプロットする Lambda を生成する SAM プロジェクトを Go で作ってみました。 プロットしたメトリクスに監視することで使用率・カバレッジ率低下をアラートできます。 kenzo0107/ri-utilization-plotter また、より OSS らしくロゴを作ってみました♪ 愛情が増します。 参照Unit Testing an SSH Client in Go","link":"/2019/12/25/2019-12-26-go_static_analytics_and_unit_test_on_github_actions/"},{"title":"ディスク使用量が増加した際の調査方法","text":"備忘録ディスク使用量増加のアラートが上がったので調査した際の手順をまとめました。 123456$ df -hFilesystem Size Used Avail Use% Mounted ondevtmpfs 2.0G 60K 2.0G 1% /devtmpfs 2.0G 0 2.0G 0% /dev/shm/dev/xvda1 20G 8.6G 11G 90% / ★ ここ高い！ 調査法123456789101112131415161718$ cd /// ① 現ディレクトリで ディスク使用率の高いベスト 10 を発表$ sudo du -ms ./* | sort -nr | head -105047 ./var1355 ./usr1249 ./home642 ./opt194 ./lib70 ./boot42 ./tmp20 ./lib6413 ./etc12 ./sbin// ② 使用率の一番高いディレクトリへ移動$ cd ./var 以上の ①, ② の繰り返しによってどのディレクトリが高いか調査してます。 du の option は以下 -m : MB 表示 -s : 総計表示 普段は -h で見やすくしてますが、 今回の調査時には sort した際に MB や kB が混ざって表示され、直感的にわかり辛くなるので、 -ms にしてます。","link":"/2020/02/09/2020-02-10-disk-usage/"},{"title":"GitHub Actions で job を 直列 と 並列 実行どっちにしよう？","text":"概要GitHub Actions で go の errcheck や lint 等、静的解析を実行していますが、その job の直列構成と並列構成、どちらがいいんだろう？と悩んだ時の話です。 悩んだポイント 並列構成だと、各 job でコンテナロードが発生し、実行時間は短いが、トータルの実行時間は長くなる。 直列構成だと、コンテナロードは 1 回で済み、実行時間は長くなるが、トータルの実行時間は短くなる。 あまりお金かけず実行したいな、というとやはり直列だろうか。 実際の構成job 直列123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869name: static checkon: pushjobs: imports: name: Imports runs-on: ubuntu-latest steps: - uses: actions/checkout@master - name: check uses: grandcolline/golang-github-actions@v1.1.0 with: run: imports token: ${{ secrets.GITHUB_TOKEN }} errcheck: name: Errcheck runs-on: ubuntu-latest steps: - uses: actions/checkout@master - name: check uses: grandcolline/golang-github-actions@v1.1.0 with: run: errcheck token: ${{ secrets.GITHUB_TOKEN }} lint: name: Lint runs-on: ubuntu-latest steps: - uses: actions/checkout@master - name: check uses: grandcolline/golang-github-actions@v1.1.0 with: run: lint token: ${{ secrets.GITHUB_TOKEN }} shadow: name: Shadow runs-on: ubuntu-latest steps: - uses: actions/checkout@master - name: check uses: grandcolline/golang-github-actions@v1.1.0 with: run: shadow token: ${{ secrets.GITHUB_TOKEN }} staticcheck: name: StaticCheck runs-on: ubuntu-latest steps: - uses: actions/checkout@master - name: check uses: grandcolline/golang-github-actions@v1.1.0 with: run: staticcheck token: ${{ secrets.GITHUB_TOKEN }} sec: name: Sec runs-on: ubuntu-latest steps: - uses: actions/checkout@master - name: check uses: grandcolline/golang-github-actions@v1.1.0 with: run: sec token: ${{ secrets.GITHUB_TOKEN }} job 並列123456789101112131415161718192021222324252627282930313233343536373839name: static checkon: pushjobs: imports: name: Imports runs-on: ubuntu-latest steps: - uses: actions/checkout@master - name: imports uses: grandcolline/golang-github-actions@v1.1.0 with: run: imports token: ${{ secrets.GITHUB_TOKEN }} - name: errcheck uses: grandcolline/golang-github-actions@v1.1.0 with: run: errcheck token: ${{ secrets.GITHUB_TOKEN }} - name: lint uses: grandcolline/golang-github-actions@v1.1.0 with: run: lint token: ${{ secrets.GITHUB_TOKEN }} - name: shadow uses: grandcolline/golang-github-actions@v1.1.0 with: run: shadow token: ${{ secrets.GITHUB_TOKEN }} - name: staticcheck uses: grandcolline/golang-github-actions@v1.1.0 with: run: staticcheck token: ${{ secrets.GITHUB_TOKEN }} - name: sec uses: grandcolline/golang-github-actions@v1.1.0 with: run: sec token: ${{ secrets.GITHUB_TOKEN }} 実際に計測してみた各構成で 10 回実行してみると 平均時間は以下の通り。 並列構成では 1分30秒/job * 6 job = 9分 直列構成だと 1分50秒/job * 1 job = 1分50秒 実行速度としては 20 秒くらいの差しかなかった。 トータルでは、 7分50秒の差！ どっちにしよう？ GitHub Actions の料金は job のトータル実行時間の従量課金制度なので、直列の方がお金に優しい。 だが、コミット頻度の少ないプロジェクトなら、並列 で時間を大事にでも良さそう。 今回は実行時間の差が 20 秒程度なので、直列でも全然問題ないレベルですが、お金との兼ね合いで 直列・並列の使い分けは変わってきそう、と思った話でした。","link":"/2020/02/19/2020-02-20-github-actions/"},{"title":"Serverless Framework の設定値の上書き・追加 方法","text":"Serverless Framework での上書き設定について詰まった点をまとめました。 基本的には以下公式ドキュメントを参考にしつつ、いくつか実装パターンを試験しました。 環境変数を例にとって設定してみます。 Environment Variables stage 共通で設定123provider: environment: key: 'hello,world' 上記設定の場合、環境変数は以下の様に設定されます。 key: hello,world 1つの値を上書きする場合参考: Overwriting Variables 1234567891011provider: name: openwhisk stage: devcustom: myStage: ${opt:stage, self:provider.stage}functions: trigger: environment: key: ${self:custom.myStage} 上記設定の場合、デプロイ後の環境変数の設定は以下の様になります。 sls deploy でデプロイした場合 key: dev sls deploy --stage hoge でデプロイした場合 key: hoge stage 毎に envorinment を追加・上書きデフォルトの設定を provider.environment で指定し、各 stage 毎の設定は self:custom.${self:provider.stage}.environment で追加・上書きできる様にしています。 12345678910111213141516171819202122provider: environment: project: ${self:custom.${self:provider.stage}.project} slackChannel: ${self:custom.${self:provider.stage}.slack.channel} key: 'hello,world'custom: hoge: project: hoge slack: channel: '#hoge' moge: project: moge slack: channel: '#moge' environment: key: 'moge-key' key2: 'moge-key2'functions: trigger: environment: ${self:custom.${self:provider.stage}.environment, self:provider.environment} 上記設定の場合、デプロイ後の環境変数の設定は以下の様になります。 sls deploy --stage hoge でデプロイした場合 project: hogeproject slackChannel: #hoge key: hello,world sls deploy --stage moge でデプロイした場合 project: hogeproject slackChannel: #hoge key: moge-key &lt;-- 更新 key2: moge-key2 &lt;-- 追加 functions.trigger.environment を上書き設定する理由は、以下の様にした場合、 1234functions: trigger: # environment: ${self:custom.${self:provider.stage}.environment, self:provider.environment} environment: ${self:custom.${self:provider.stage}.environment} self:custom.hoge.environment の設定がなく、 Warning が発生する為です。 123Serverless Warning -------------------------------------- A valid service attribute to satisfy the declaration 'self:custom.hoge.environment' could not be found. 以上参考になれば幸いです。","link":"/2020/03/23/2020-03-24-serverlessfw-environment/"},{"title":"Backlog でコメント追加時に 「お知らせしたいユーザ」に Slack DM する","text":"Backlog でコメント追加時に 「お知らせしたいユーザ」に Slack DM する AWS Serverless Application Model with Golang プロジェクト作りました♪ 使い方は、 Git プロジェクトを見ていただければ！ kenzo0107/backlog-to-slack-dm もしよくわからんぞ！という時は連絡ください♪ 構築するに辺り、検討した点世の中には backlog API 関連の SDK などあるか？griffin-stewie/go-backlog が諸々揃っていて良さそうだったので使ってみました。 利用したい箇所としては、以下です。 コメント追加時のイベント情報を受ける type Activity struct ↑で受け取れる通知したいユーザの ID から Email アドレスを取得する API 実行 但し、このライブラリでは、Activity には通知したい情報を含む Notifications がコメントアウトとなっていたので、直ちに利用できず、 急を要していたこともあり、 fork して kenzo0107/go-backlog で対応しました。 単純にコメントアウトを外して使える様にしただけでは、 json.Unmarshal 実行時にエラーとなっており、他のパラメータも幾分か対応する必要がありました。 Backlog からのアクセス制御はどうする？Backlog からのアクセス制御はBasic 認証について言及があったので Basic 認証にしました。 IP アドレスの変更に影響されない方法であれば、 Webhook の URL に BASIC 認証をつけていただくことで、IP アドレスに依存しない認証できます。 Webhook 送信サーバーの IP アドレスを教えてください IP レンジは予告なく変更される可能性があり、作成者以外ではなかなか気付きにくいかもしれません。その為、 Backlog Webhook に設定する URL は、 https://&lt;user&gt;:&lt;password&gt;@...... と Basic 認証の情報を埋め込む様にしました。 これを API Gateway + Lambda Authorizer (Request Type) で認証させる様にしました。 Backlog API 実行時に許可する ip はどうするか？Backlog API を実行する Lambda を Nat Gateway をルーティングした Private Subnet に置くことで、出口 IP を固定する様にし、その IP を Backlog 側で許可 IP として設定しました。 注意点Backlog Webhook 各プロジェクト毎に設定する必要があり全プロジェクト一括して設定ということができませんでした。 2020-02-27 現在 各プロジェクト管理者に秘密情報として通知する様に対応をしました。 まとめまだテストを書き切れてないところはありますが、問題なく動作していることを確認しています。 Backlog を取り入れている方へ、何かしら参考になれば幸いです。 以上です。","link":"/2020/02/24/2020-02-25-backlog-comment-slack-dm/"},{"title":"vscode で Go Generate Unit Test が便利だった♪","text":"Go でテストを書く際に vscode の Go extension の単体テストのフォーマットを簡単に生成できる機能があったので利用すると非常に便利でした。 以下の様な main.go ファイルがあるとします。 12345678package mainfunc hello(s string) string { if s == \"\" { return \"world\" } return s} 関数名をクリックしてポインタを置いて、 Mac だと Command + Shift + p でコマンドパレットが表示されるので Go: Generate Unit Tests For Function と打ち込んでエンターを押すと 以下 main_test.go が生成されます。 1234567891011121314151617181920212223package mainimport \"testing\"func Test_hello(t *testing.T) { type args struct { s string } tests := []struct { name string args args want string }{ // TODO: Add test cases. } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if got := hello(tt.args.s); got != tt.want { t.Errorf(\"hello() = %v, want %v\", got, tt.want) } }) }} 関数がスネークケース (Test_hello) になるのがやや気になりますが、テスト対象の関数名が小文字で始まる場合にこうなる様です。 テストケースに必要な情報を struct で管理し、 for で順次回すというフォーマットです。 TABLE DRIBEN TESTS func hello(string) の引数が args で指定されています。 123type args struct { s string} もしテスト対象が func(string, int) なら以下の様に変わってくれます。 1234type args struct { s string i int} テストを以下の様に書いて、 123456789101112131415161718192021222324252627282930313233343536package mainimport &quot;testing&quot;func TestHello(t *testing.T) { type args struct { s string } tests := []struct { name string args args want string }{ { name: &quot;return 'hello' if you set 'hello'&quot;, args: args{ &quot;hello&quot;, }, want: &quot;hello&quot;, }, { name: &quot;空文字を指定したら world が返ってくる&quot;, args: args{ &quot;&quot;, }, want: &quot;world&quot;, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if got := hello(tt.args.s); got != tt.want { t.Errorf(&quot;hello() = %v, want %v&quot;, got, tt.want) } }) }} テストを実行すると name 毎にテストが PASS しているか確認できます。 半角スペースが _ に変換されます。 1234567891011$ go test -count 1 -v .$ go test -count 1 -v ./hoge=== RUN TestHello=== RUN TestHello/return_'hello'_if_you_set_'hello'=== RUN TestHello/空文字を指定したら_world_が返ってくる--- PASS: TestHello (0.00s) --- PASS: TestHello/return_'hello'_if_you_set_'hello' (0.00s) --- PASS: TestHello/空文字を指定したら_world_が返ってくる (0.00s)PASSok github.com/kenzo0107/hoge 0.212s 便利です♪ ちなみに、 Go: Generate は他にもあります。 vscode で golang のテストを書く際に参考になれば何よりです。以上です。","link":"/2020/03/06/2020-03-07-vscode-go/"},{"title":"CodePipeline で CodeBuild へ環境変数を渡し、上書きすることで CodeBuild を再利用する","text":"CodeBuild を再利用し、不要に作成しない様にした話です。 terraform で buildspec を管理してみる buildspec.yml 1234567---version: 0.2env: variables: FOO: \"${foo}\" ... codebuild.tf 12345678910111213data &quot;template_file&quot; &quot;buildspec&quot; { template = file(&quot;buildspec.yml&quot;) vars = { foo = &quot;foo&quot; }}resource &quot;aws_codebuild_project&quot; &quot;foo&quot; { source { type = &quot;CODEPIPELINE&quot; buildspec = data.template_file.buildspec.rendered } terraform apply 実行し CodeBuild を作成すると、環境変数 FOO=foo が設定されます。 CodePipeline で CodeBuild の環境変数を上書きCodeBuild の処理内容は同じだが、環境変数だけ変えたい場合に有効です 123456789resource &quot;aws_codepipeline&quot; &quot;moge&quot; { stage { name = &quot;Build&quot; action { configuration = { ProjectName = aws_codebuild_project.foo.name EnvironmentVariables = &quot;[{\\&quot;name\\&quot;:\\&quot;FOO\\&quot;,\\&quot;value\\&quot;:\\&quot;moge\\&quot;,\\&quot;type\\&quot;:\\&quot;PLAINTEXT\\&quot;}]&quot; } これで CodePipeline を実行した場合、 FOO=moge が指定され、見事上書きされてます。 例題${repository_url} に nginx の ECR リポジトリを代入すると Nginx のイメージビルド &amp; ECR プッシュの処理をする CodeBuild が作れます。 1234567891011121314151617181920---version: 0.2env: variables: DOCKER_BUILDKIT: 1 REPOSITORY_URL: \"${repository_url}\"phases: pre_build: commands: - $(aws ecr get-login --region $AWS_DEFAULT_REGION --no-include-email) - IMAGE_TAG=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7) build: commands: - &gt;- docker build -t $REPOSITORY_URL:latest -f Dockerfile . - docker tag $REPOSITORY_URL:latest $REPOSITORY_URL:$IMAGE_TAG - docker push $REPOSITORY_URL:latest - docker push $REPOSITORY_URL:$IMAGE_TAG CodePipeline で環境変数 REPOSITORY_URL=123456789012.dkr.ecr.ap-northeast-1.amazonaws.com/fluentd と指定すると fluentd イメージをビルド &amp; ECR push ができます。 結論無闇に CodeBuild を作成することなかれ 以上参考になれば幸いです。 参考環境変数の指定の仕方は以下参考にしました。 Build specification reference for CodeBuild","link":"/2020/03/30/2020-03-31-buildspec-env/"},{"title":"go.uber.org&#x2F;multierr で error をまとめる","text":"こんなツイートを見つけた！ これはやってみたい！ と早速取り掛かって、これは知って良かった！という内容の一つがコチラ！ 複数のエラーをまとめる処理！ for 内で発生したエラーをまとめて後で表示する、という処理を自前で用意してたけど、体系的にパッケージとして整備していたのがあったとは♪ 知識のアップデート完了 参考Golangのエラー処理とpkg/errors","link":"/2020/03/30/2020-03-31-go-multierr/"},{"title":"Lambda Edge で Basic 認証 や CSP 対応","text":"Basic 認証 や CSP 対応を Lambda Edge で対応した話です。 CSP とは？ XSS, Data Injection の様な特定種類の攻撃を検知し、影響を軽減する為に追加できるセキュリティレイヤー 参考: コンテンツセキュリティポリシー (CSP) どんなことができる？コンテンツを特定のドメインやプロトコルでのみ実行可能な様に制約をかけることができる。これによって、意図しない通信ができなくなる。 どうすれば設定できる？ヘッダーに情報追加HTTP ヘッダーに Content-Security-Policy : policy で設定 要素を用いてポリシー指定1&lt;meta http-equiv=&quot;Content-Security-Policy&quot; content=&quot; policy &quot;&gt; Lambda Edge でやる理由1CloudFront --&gt; ALB --&gt; ECS - Rails 普段、上記の構成でサービスしてることが多く、CloudFront がオリジンとなる Rails からのレスポンス（Origin Response）のヘッダー情報に CSP policy を設定することで、CSP 対策をしようと試みました。 Rails 自体で設定する方法もありますが、後述します。 terraform で設定してみるhttps://github.com/kenzo0107/sample-lambda-edge templates/csp.js CSP 対策 を Original Response に設定 templates/basic_auth.js Basic 認証を Vewer Request に設定 実際設定してみてどう？Rails でフロントに Vue を使用している場合に script-src 'unsafe-eval' の指定が必要だったりで、フロント全然動かない、なんてことありました。 使用しているサードパーティやフロントエンドで CSP の設定で問題が生じることが多々ありました。 これは Rails 側で設定しないと開発環境では動いたけど、Lambda Edge 効いてる環境だと動かない、ということがありそうで Lambda Edge でできるにはできるが、アプリケーション側に任せる方が良いのでは？と思いました。 参考MDN web docs - Content-Security-Policy","link":"/2020/04/15/2020-04-16-cloudfront-csp/"},{"title":"Go で Json 取扱","text":"ToC やりたいこと Json ファイルの読み込み struct に格納する Json ファイル出力 余談 やりたいこと Json ファイルの読み込み・構造体に格納 Json ファイルの書き込み時のエスケープ処理対応 Json ファイル出力 Json ファイルの読み込み hoge.json 123456789{ \"results\": [ { \"id\": 123456, \"title\": \"Hello, Gopher\", \"active\": true } ]} main.go 12345678func main () { fpath := filepath.Join(\"hoge.json\") b, err := ioutil.ReadFile(filepath.Clean(fpath)) if err != nil { log.Fatal(err) } log.Println(string(b))} ioutil.ReadFile(fpath) でなく ioutil.ReadFile(filepath.Clean(fpath)) としている理由は以下参照してください。 gosec で警告される os.Open() 対応gosec で警告される os.Open() に対応した話です。 読み取れました！ でも、この results[] の中の id が使いたいんだけどなぁ…と思っても、このままだと id の値を取得できません。 123456789{ \"results\": [ { \"id\": 123456, \"title\": \"Hello, Gopher\", \"active\": true } ]} struct に格納するJSON-to-Go に json を貼り付けるとサクッと struct が生成されます。 12345678910111213141516171819202122// Hoge : -type Hoge struct { Results []struct { ID int `json:\"id\"` Title string `json:\"title\"` Active bool `json:\"active\"` } `json:\"results\"`}func main() { fpath := filepath.Join(\"test\", \"hoge.json\") b, err := ioutil.ReadFile(filepath.Clean(fpath)) if err != nil { log.Fatal(err) } var hoge Hoge if err := json.Unmarshal(b, &amp;hoge); err != nil { log.Fatal(err) } log.Printf(\"Title: %#v\", hoge.Results[0].Title)} json.Unmarshal(b, &amp;hoge) で json データを hoge に格納しています。 実行すると、ちゃんと取れてます。 12020/05/10 23:48:01 Title: &quot;Hello, Gopher&quot; Json ファイル出力map[int]int 型のデータを hoge.json に書き出してみます。 1234567891011121314func main() { a := make(map[int]int) a[1] = 2 a[10] = 100 b, err := json.MarshalIndent(a, \"\", \" \") if err != nil { log.Fatal(errors.Wrap(err, \"failed json.MarshalIndent\")) } jsonFpath := filepath.Join(filepath.Clean(\"hoge.json\")) if err := ioutil.WriteFile(jsonFpath, b, 0644); err != nil { log.Fatal(err) }} hoge.json が出力されていることが確認できます。 1234{ \"1\": 2, \"10\": 100} 余談json.Unmarshal の error チェックをしない実装をよく見かけます。 実装上、失敗したとて、問題ない場合があるのであれば、良いですが、基本省略すべきでないです。 12var hoge Hogejson.Unmarshal(b, &amp;hoge) その他、実は error 型返す関数をチェックしてないということがあるので、以下 module で対処する様にしています。 12go get -u github.com/kisielk/errcheckerrcheck ./... Go プロジェクトでは、必ず、GitHub Actions でも、ローカルでも実行する様にしています。 以上参考になれば幸いです。","link":"/2020/05/06/2020-05-07-go-json/"},{"title":"Go で時刻を json.Unmarshal する際の注意点","text":"ToC とある 3rd Party の API のお話 フォーマットどこで定義してるんだろう？ フォーマットを自由に扱うにはどうする？ その他、実例 Slack API usergroups.create まとめ 参考 とある 3rd Party の API のお話とある 3rd Party の API を go で取り扱っていた所、以下の様な json を返す API がありました。 12345{ ... \"created\": \"1981-01-07T17:44:13Z\" ...} json を struct に格納する時に json.Unmarshal するとどんなフォーマットで返ってくるでしょう？ 12345678910111213type Hoge struct { Created time.Time `json:\"created\"`}func main() { var hoge Hoge j := []byte(`{\"created\": \"1981-01-07T17:44:13Z\"}`) if err := json.Unmarshal(j, &amp;hoge); err != nil { log.Fatal(err) } t := hoge.Created fmt.Println(t) // 1981-01-07 17:44:13 +0000 UTC} The Go Playground フォーマットは 1981-01-07 17:44:13 +0000 UTC でした。 フォーマットどこで定義してるんだろう？コードを追ってみると、 *time.Time のレシーバ UnmarshalJSON で RFC3339 を決め打ちで time.Parse しています。 参照: golang.org/src/time/time.go 123456789101112// UnmarshalJSON implements the json.Unmarshaler interface.// The time is expected to be a quoted string in RFC 3339 format.func (t *Time) UnmarshalJSON(data []byte) error { // Ignore null, like in the main JSON package. if string(data) == \"null\" { return nil } // Fractional seconds are handled implicitly by Parse. var err error *t, err = Parse(`\"`+RFC3339+`\"`, string(data)) return err} つい encoding/json の方を探してしまった 汗 フォーマットを自由に扱うにはどうする？独自型を定義しました。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package mainimport ( \"encoding/json\" \"fmt\" \"log\" \"time\" \"bytes\")type Hoge struct { Created JSONTime `json:\"created\"`}// JSONTime exists so that we can have a String method converting the datetype JSONTime string// String converts the unix timestamp into a stringfunc (t JSONTime) String() string { tm := t.Time() return fmt.Sprintf(\"\\\"%s\\\"\", tm.Format(time.RFC3339))}// Time returns a `time.Time` representation of this value.func (t JSONTime) Time() time.Time { tt, _ := time.Parse(time.RFC3339, string(t)) return tt}// UnmarshalJSON will unmarshal both string and int JSON valuesfunc (t *JSONTime) UnmarshalJSON(buf []byte) error { s := bytes.Trim(buf, `\"`) *t = JSONTime(string(s)) return nil}func main() { var hoge Hoge j := []byte(`{\"created\": \"1981-01-07T17:44:13Z\"}`) if err := json.Unmarshal(j, &amp;hoge); err != nil { log.Fatal(err) } t := hoge.Created fmt.Println(t) // \"1981-01-07T17:44:13Z\" fmt.Println(t.Time()) // 1981-01-07 17:44:13 +0000 UTC} The Go Playground API のレスポンスを独自型 type JSONTime string で受ける 時刻の計算や比較をしたい場合は t.Time() で time.Time を返して計算 という具合に都合が良いです。 その他、実例 Slack API usergroups.createSlack API usergroups.create では、レスポンスに date_create が 1446746793 と timestamp で返ってくるので、 type JSONTime int64 とすると良い。 1234567891011121314151617181920212223242526// JSONTime exists so that we can have a String method converting the datetype JSONTime int64// String converts the unix timestamp into a stringfunc (t JSONTime) String() string { tm := t.Time() return fmt.Sprintf(\"\\\"%s\\\"\", tm.Format(\"Mon Jan _2\"))}// Time returns a `time.Time` representation of this value.func (t JSONTime) Time() time.Time { return time.Unix(int64(t), 0)}// UnmarshalJSON will unmarshal both string and int JSON valuesfunc (t *JSONTime) UnmarshalJSON(buf []byte) error { s := bytes.Trim(buf, `\"`) v, err := strconv.Atoi(string(s)) if err != nil { return err } *t = JSONTime(int64(v)) return nil} まとめ *time.Time のレシーバ UnmarshalJSON はフォーマット RFC3339 で返す。 任意のフォーマットにするには、独自型を作成しレシーバ UnmarshalJSON を設定し、Parse 時にフォーマット指定する。 以上参考になれば幸いです。 参考slack-go/slack の JSONTime 定義","link":"/2020/05/19/2020-05-20-go-json-time/"},{"title":"Backlog API in Go 作りました","text":"ToC Backlog API in Go 作りました Backlog API を作る経緯 Backlog API in Go 作りましたGitHub - kenzo0107/backlog: Backlog API in GoBacklog API in Go. Contribute to kenzo0107/backlog development by creating an account on GitHub. 只今、サポートする API を増やしております。 Backlog API を作る経緯とあるナレッジマネジメントシステムから Backlog へ移行するお仕事があり migration tool を作ろう！と思い API が必要となりました。 以前、Backlog でコメントに「お知らせしたいユーザ」に Slack DM をする、というのを SAM で作ったときは griffin-stewie/go-backlog で API を扱っていました。 ですが、griffin-stewie/go-backlog は 2018年1月を最後に更新されていませんでした。。 例えば、ファイルをアップロードして Backlog の Wiki に追加する、という API がありません。 それならば！と思い、自作することとしました。 GitHub - kenzo0107/backlog: Backlog API in GoBacklog API in Go. Contribute to kenzo0107/backlog development by creating an account on GitHub. 是非、コミット &amp; スポンサーお待ちしております m(_ _)m","link":"/2020/06/05/2020-06-06-backlog-api-go/"},{"title":"AWS IAM Policy はインラインでなく、管理ポリシーを積極的に使おう","text":"ToC 概要 結論、インラインポリシーを控え、管理ポリシーを積極的に使おう terraform の気持ちになってみる 例) 管理ポリシー 例) インラインポリシー まとめ 概要IAM Policy で、管理ポリシーとインラインポリシーってどういう違いがあって、どっちを使うべきなのだろう？と調べてみました。 結論、インラインポリシーを控え、管理ポリシーを積極的に使おう管理ポリシーとインラインポリシー 上記リンクの AWS ドキュメントの 「管理ポリシーとインラインポリシーの比較」 項目を見ると、管理ポリシーを利用するメリットが明らかです。 インラインポリシーではなく、管理ポリシーを使用することをお勧めします。 管理ポリシーは次の機能を備えています。 再利用可能性 一元化された変更管理 バージョニングとロールバック アクセス許可管理の委任 AWS 管理ポリシーの自動更新 terraform の気持ちになってみるterraform の気持ちになって必要なリソースを考えると以下の様になります。 管理ポリシー aws_iam_policy aws_iam_user_policy_attachment インラインポリシー aws_iam_user_policy 例) 管理ポリシー123456789101112131415161718192021222324# IAM Userresource &quot;aws_iam_user&quot; &quot;hoge&quot; { name = &quot;hoge&quot; path = &quot;/&quot;}# 管理ポリシーresource &quot;aws_iam_policy&quot; &quot;hoge&quot; { name = &quot;hoge&quot; policy = data.aws_iam_policy_document.hoge.json}# ポリシーdata &quot;aws_iam_policy_document&quot; &quot;hoge&quot; { statement { ... }}# ポリシーを IAM User にアタッチresource &quot;aws_iam_user_policy_attachment&quot; &quot;hoge&quot; { user = aws_iam_user.hoge.name policy_arn = aws_iam_policy.hoge.arn} 例) インラインポリシー12345678910111213141516171819# IAM Userresource &quot;aws_iam_user&quot; &quot;hoge&quot; { name = &quot;hoge&quot; path = &quot;/&quot;}# ポリシーdata &quot;aws_iam_policy_document&quot; &quot;hoge&quot; { statement { ... }}# インラインポリシーとして IAM User にポリシーをアタッチresource &quot;aws_iam_user_policy&quot; &quot;hoge&quot; { name = &quot;hoge&quot; user = aws_iam_user.hoge.name policy = data.aws_iam_policy_document.hoge.json} まとめterraform プロジェクトをレビューする機会がある折には、 aws_iam_user_policy を見つけたら是非、目を細めて「管理ポリシーの優位性が…」云々を語り、AWS のドキュメントをリンクを貼ってみてください。 以上参考になれば幸いです。","link":"/2020/07/01/2020-07-02-aws-iam-policy-managed-or-inline/"},{"title":"aws_ssm_parameter の value を ignore_change に指定しない","text":"ToC 概要 対応ステップ まとめ 概要これまで秘密情報は以下の様な取り扱いをしていることが多かったです。 123456789resource \"aws_ssm_parameter\" \"master_password\" { name = \"master_password\" type = \"SecureString\" value = \"dummy\" lifecycle { ignore_changes = [value] }} 123data \"aws_ssm_parameter\" \"master_password\" { name = \"master_password\"} 上記いずれの場合にも、事前に Parameter Store への登録が必須です。つまり、terraform で管理されない作業が発生していることになります。 対応ステップterraform で完結させる方法としては、パラメータストアに登録する value を暗号化して登録することです。 暗号化に必要な KMS Key を作成する 1234resource \"aws_kms_key\" \"a\" { description = \"KMS key 1\" deletion_window_in_days = 10} aws kms encrypt で値を暗号化 123aws kms encrypt --key-id &lt;key-id&gt; --plaintext &lt;value&gt;AQECAHgaPa0J8...3MmDBdqP8dPp28OoAQ== data.aws_kms_secrets で取得した値をパラメータストアに登録する 123456789101112data \"aws_kms_secrets\" \"parameters\" { secret { name = \"master_password\" payload = \"AQECAHgaPa0J8...3MmDBdqP8dPp28OoAQ==\" }}resource \"aws_ssm_parameter\" \"master_password\" { name = \"/${var.environment}/database/password/master\" type = \"SecureString\" value = data.aws_kms_secrets.parameters.plaintext[\"master_password\"]} パラメータストアに登録という前提では resource &quot;aws_ssm_parameter&quot; の処理が必要でしたが、以下の様に直接リソースへの指定も可能です。 12345resource \"aws_rds_cluster\" \"example\" { # ... other configuration ... master_password = data.aws_kms_secrets.parameters.plaintext[\"master_password\"] master_username = data.aws_kms_secrets.parameters.plaintext[\"master_username\"]} まとめ悩んだ時に公式ドキュメントを見ると解決されていることが本当に多いです。 ちなみに、KMS キーで暗号化するのは誰がするか？と考えると、おそらく terraform のコードを書いている方になろうかと思いますが、その施行者には KMS キーで暗号化する権限が必要になります。 kms:Encrypt 以上参考になれば幸いです。","link":"/2020/05/31/2020-06-01-terraform-sensitive-data/"},{"title":"Ansible でサーバ環境構築","text":"目的Ansible でLAMP環境を構築する 経緯Chefでサーバ環境構築していましたが、設定ファイルが多い印象を持っていた為、より敷居が低いという噂のAnsibleでの構築を考えました。 覚える為に徐々に設定していく項目を増やしていく、という方針で進めます。 個人的経験上ですが、gitやQiitaでplaybookを見ていてもどの記述で何を実現しているかを理解するのはとっつきにくく、理解せず実現させてしまう可能性もあるからです。 playbook.yml1234567891011---- hosts: test-servers sudo: yes tasks: # Apache - name: be sure httpd is installed yum: name=httpd state=installed - name: be sure httpd is running and enabled service: name=httpd state=running enabled=yes あとがき以下サイトが大変参考になりました。 http://yteraoka.github.io/ansible-tutorial/ 参考サイトを手順通り実行しても出来ない！ということがよくあるので、その際はサイトURLとできなかったことをまとめておくと良いと思いました。 できないできない、と色々ネットサーフィンしてたらまた同じサイトを見てたりということもあるので。","link":"/2014/09/11/2014-09-12-make-lamp-by-ansible/"},{"title":"さくらVPS CentOS6.5にRedisを導入しphpで動かすまで","text":"Redis - ( レディス ) Remote Dictionary ServerKey Value Storeを構築できるツール 環境 さくらVPS CentOS6.5 Final Redis 2.8.19 (2015/2月時点の最新Stable) PHP 5.4.34 手順 Redisインストール Redis設定 Redisデータ設定・取得テスト chkconfig にRedis登録 phpredisインストール php.iniにredis.so追加 httpd再起動 phpからcallして挙動確認 Redisインストール準備 12$ sudo su# yum -y install gcc make Redisインストール※圧縮ファイルをダウンロード→解凍→コンパイル 123456789101112# cd /usr/local/src# wget http://redis.googlecode.com/files/redis-2.2.12.tar.gz--2014-09-12 14:59:10-- http://redis.googlecode.com/files/redis-2.2.12.tar.gzredis.googlecode.com をDNSに問いあわせています... 74.125.204.82, 2404:6800:4008:c04::52redis.googlecode.com|74.125.204.82|:80 に接続しています... 接続しました。HTTP による接続要求を送信しました、応答を待っています... 200 OK長さ: 455240 (445K) [application/x-gzip]`redis-2.2.12.tar.gz' に保存中100%[==================================================================&gt;] 455,240 555K/s 時間 0.8s2014-09-12 14:59:12 (555 KB/s) - `redis-2.2.12.tar.gz' へ保存完了 [455240/455240] 圧縮ファイルの解凍/ビルド 123# tar xzvf redis-2.2.12.tar.gz# cd redis-2.2.12# make &amp;&amp; make install 設定ファイルバックアップ 1# cp -p redis.conf redis.conf.org redis.conf 編集 1# vi redis.conf redis.conf変更内容 1234567891011# daemon#daemonize nodaemonize yes# logfile#logfile stdoutlogfile /var/log/redis.log#loglevel#loglevel verboseloglevel notice redisサーバ起動 1# redis-server redis.conf クライアント起動 1# src/redis-cli redis設定の簡易テストデータ設定 1# set tanaka test データ取得 12# get tanaka&quot;test&quot; 起動スクリプト作成 （init.dに作成） 1sudo cp /usr/local/src/redis-2.2.12/utils/redis_init_script /etc/init.d/redis 設定ファイルをコピー 12$ sudo mkdir /etc/redis$ sudo cp /usr/local/src/redis-2.2.12/redis.conf /etc/redis/6379.conf 設定ファイル編集 1$ sudo vim /etc/redis/6379.conf 1234567891011#daemonize no デーモン化の設定を有効化。daemonize yes# pidfile /var/run/redis.pid 起動シェルの設定にあわせる。pidfile /var/run/redis_6379.pid# logfile stdout ログファイル出力を、標準出力からファイルに変更logfile /var/log/redis.log# dir ./ working directoryにdumpファイルが生成されるらしいので、変更します。dir /usr/local/redis/ 後々仕様するディレクトリを作成しておく 1sudo mkdir /usr/local/redis/ 起動時に起動する様、chkconfigリストに登録 12# /sbin/chkconfig --add redis# /sbin/chkconfig redis on service redis does not support chkconfig 「#」のすぐ後にスペースが入っていると上記エラーが出力されるので、スペースを削除する。 12345678910111213$ cat /etc/init.d/redis# as it does use of the /proc filesystem.REDISPORT=6379EXEC=/usr/local/bin/redis-serverCLIEXEC=/usr/local/bin/redis-cliPIDFILE=/var/run/redis_${REDISPORT}.pidCONF=\"/etc/redis/${REDISPORT}.conf\"...... 一行目の # as it does use of the /proc filesystem. が原因です。この行を削除しましょう。 上記設定後、再度chkconfigリストに登録設定してください。 ※可能であればrebootして起動確認 1234# reboot# sudo su -# cd redis-2.2.12# src/redis-cli phpredisインストール//gitでソースを取得 ** php.ini編集 12[redis]extension=redis.so redisがphpのモジュールとして追加されているか確認 123$ php -m | grep redisredis php.iniの更新を反映する為、apache 再起動 1service httpd restart 以下例文で表示されるか確認 12345678910&lt;?php$redis = new Redis();$redis-&gt;connect(&quot;127.0.0.1&quot;,6379);$tmp = &quot;redis (^-^)&quot;;$redis-&gt;set(&quot;test_key&quot;,$tmp);$res = $redis-&gt;get(&quot;test_key&quot;);var_dump($res);?&gt;","link":"/2014/09/11/2014-09-12-php-redis-on-centos6.5-sakura-vps/"},{"title":"Python エラー対処：error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory","text":"概要以下コマンドでpythonのバージョン確認しようとしたら掲題のエラーが発生。トラブルシューティングします。 手順 エラーが消えました。 以上","link":"/2015/02/16/2015-02-17-python-error-about-loading-shared-libraries-libpython2.7.so.md/"},{"title":"Python エラー対処： PowmInsecureWarning: Not using mpz_powm_sec.  You should rebuild using libgmp &gt;&#x3D; 5 to avoid timing attack vulnerability.   _warn(&quot;Not using mpz_powm_sec.  You should rebuild using libgmp &gt;&#x3D; 5 to avoid timing attack vulnerability.&quot;, PowmInsec","text":"概要pysftp利用としたらgmp5以上にしてくれと怒られている。 pysftpを利用するのに必要なparamiko、そのparamikoに必要なpycryptoがエラー出力している。 ToDogmp5 インストールpython リビルドpycrypto アンインストール&amp;インストール 手順 以上","link":"/2015/02/16/2015-02-17-python-error-about-powmlnsecurewarning/"},{"title":"CentOS で Python バージョンアップ","text":"経緯pysftpを利用したかったが元々インストール済みの2.4系が古く、動作しなかったため、動作可能なバージョンを2.7系にアップする 環境CentOS5.8(Final) インストールするPython VersionPython2.7.6 インストール手順yumでインストールすると他の不要なモジュールまでインストールしてしまい依存関係を上書いてしまうので、ソースからビルドします。 pip インストール paramiko, pysftpインストール 総評Apacheの再起動なし。既存Pythonバージョンも残しで出来ました。","link":"/2015/02/16/2015-02-17-versionup-python-on-centos/"},{"title":"MAMPにMemcacheをインストールしphpで動かすまで","text":"概要ローカル開発環境にMemcacheをインストールしたい。 環境MacOSX Yosemite 10.10.1MAMP3.0.7.3 注意インストールするのはMemcacheです。Memcachedではないです。 ToDo Memcacheのソースダウンロードしコンパイル memcache.soをphp.iniから呼び出し MAMP再起動 手順 Memcacheのソースダウンロードしコンパイル memcache.soをphp.iniから呼び出し php から利用 サンプル 以上","link":"/2015/02/19/2015-02-20-mamp-with-memcache/"},{"title":"CentOSに phpMyAdminインストール利用するまで","text":"概要CentOSにphpMyAdminを設定 環境 CentOS release 5.11 (Final) PHP 5.3.6 MySQL 5.5.41 phpMyAdmin 5.3.6 手順","link":"/2015/02/23/2015-02-24-install-phpadmin-centos/"},{"title":"MacOSXにHomebrewをインストールする","text":"1ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" 以下MacOSXの初期セットアップで使っているshellです。 https://github.com/kenzo0107/Brewfile","link":"/2015/02/27/2015-02-28-install-homebrew-on-macosx/"},{"title":"DUNSナンバー確認手順","text":"概要thawteにて「SGC Super Certs」の申請する際DUNSナンバーが必要となる為クライアント様がDUNS(ダンズ)ナンバーを所持されているか確認する必要がありました。 http://www.tsr-net.co.jp/service/product/get_a_duns_number/ DUNSナンバー確認手順を以下に記載します。 尚、以下手続きはクライアント様ご自身が実施する必要があります。理由としては、クライアント実行した場合、無料で問い合わせできるからです。他社からは3000円になります。 2015/02/23 現在 DUNSナンバー確認手順 へアクセスします。中央の赤いボタン「DUNS Numberを検索する」をクリックします。 クライアントの企業名や住所で検索します。WHOIS(フーイズ)で検索したドメイン所有者で検索する。 例として、アメブロ「ameblo.jp」の所有者情報を使用します。 検索結果にて該当項目の「DUNS」画像ボタンをクリックします 使用許諾契約書ページが表示されます ページフッター付近にて「同意する」ボタンをクリックします 申し込みフォームが表示されるので必要情報を入力し「確認」ボタンをクリックします 「送信」ボタンをクリックします 上記入力のメール宛にDUNSナンバー通知を確認します。 以下クライアント様より報告いただいたDUNSナンバー情報フォーマットになります。 1234[受付番号] ***[対象企業] ***[ DUNS# ] ***[ 自／他 ] 自社 上記を元にthawteではSSL証明書発行手続きをします。 クライアント様登録の電話番号が誤っている、もしくは現在使われていない場合は、SSL申請手続きでリジェクトされるという事態が起きます。 実際起きました汗 クライアント様へ確認依頼する際はお電話番号が後使用可能であるかを不躾ではありますが、確認しておくのが良いかと思います。 以上","link":"/2015/02/27/2015-02-28-show-duns-number/"},{"title":"SVN から Git へ過去コミットログを担保した上で移行する","text":"概要リポジトリ管理を SVN から Git 移行する必要がありその際に利用したコマンドをまとめます。 手順まずリモートに移行先のリポジトリを作成しておきます。 以下コマンドを実行しようとしたら以下エラーが出た場合1234567$ git pull origin masterPermission denied (publickey).fatal: Could not read from remote repository.Please make sure you have the correct access rightsand the repository exists. 要約すると、公開鍵のパーミッション拒否されています、ということです。 remote repositoryにどのようにアクセスしようとしているかの確認をします。 12345678910111213141516[core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true ignorecase = true precomposeunicode = true[svn-remote &quot;svn&quot;] url = http://www.svn.rubygroupe.jp/svn/hogehoge fetch = trunk:refs/remotes/svn/trunk branches = branches/*:refs/remotes/svn/* tags = tags/*:refs/remotes/svn/tags/*[remote &quot;origin&quot;]# url = git@github.com:xxxxxxxxx.git &lt;span style=&quot;color: #ff0000&quot;&gt;url = https://github.com:xxxxxxxxx.git&lt;/span&gt; fetch = +refs/heads/*:refs/remotes/origin/* 再度 pull してみてください。 1$ git pull origin master あとがきGit側がSVNを引き取ることを想定して用意しているという大人な対応に感謝 そもそも何故移行？という方はまずGitを触ってみてください。","link":"/2015/02/27/2015-02-28-transit-svn-to-git/"},{"title":"さくらVPS fluentd + elasticsearch + kibana3","text":"ElasticSearch インストール公式サイト: http://www.elasticsearch.org/blog/apt-and-yum-repositories/ yumリポジトリ追加1rpm --import http://packages.elasticsearch.org/GPG-KEY-elasticsearch elasticsearchリポジトリ設定ファイル追加12345678cat &gt;&gt; /etc/yum.repos.d/elasticsearch.repo &lt;&lt;'EOF'[elasticsearch-1.0]name=Elasticsearch repository for 1.0.x packagesbaseurl=http://packages.elasticsearch.org/elasticsearch/1.0/centosgpgcheck=1gpgkey=http://packages.elasticsearch.org/GPG-KEY-elasticsearchenabled=1EOF javaとelasticsearchインストール1yum install elasticsearch java-1.7.0-openjdk サーバ起動時モジュール自動起動設定1chkconfig elasticsearch on elasticsearch起動1service elasticsearch start 動作テスト123456789101112131415curl -X GET http://localhost:9200/// response{ \"status\" : 200, \"name\" : \"Hydron\", \"version\" : { \"number\" : \"1.0.3\", \"build_hash\" : \"61bfb72d845a59a58cd9910e47515665f6478a5c\", \"build_timestamp\" : \"2014-04-16T14:43:11Z\", \"build_snapshot\" : false, \"lucene_version\" : \"4.6\" }, \"tagline\" : \"You Know, for Search\"} Kibana インストールkibana ユーザ追加1useradd kibana パスワード設定123456passwd kibanaユーザー kibana のパスワードを変更。新しいパスワード: [パスワード入力]よくないパスワード: 辞書の単語に基づいています新しいパスワードを再入力してください: [もう一度パスワード入力]passwd: 全ての認証トークンが正しく更新できました。 kibanaのパーミッション設定1chmod +x /home/kibana kibana1su - kibana kibana3ダウンロード1curl -LO https://download.elasticsearch.org/kibana/kibana/kibana-3.0.0milestone5.tar.gz モジュール解凍1tar zxvf kibana-3.0.0milestone5.tar.gz シンボリックリンク設定1ln -s /home/kibana/kibana-3.0.0milestone5 ./kibana kibana config編集 /home/kibana/kibana/config.js 12// 以下のように設定elasticsearch: &quot;http://(ドメイン)/es/&quot;, kibanaユーザ解除1exit Elasticsearchへの接続用に/es/をリバースプロキシ構成1234htdigest -c /etc/httpd/conf/htdigest &quot;Required authentication&quot; (Basic認証の設定したいID)Adding password for okochang in realm Required authentication.New password: [パスワード入力] (Basic認証の設定したいPW)Re-type new password: [パスワード入力] (Basic認証の設定したいPW) vim /etc/httpd/conf.d/vhosts.conf 設定ファイルのシンタックスチェック1httpd -t ▼実行結果 1Syntax OK httpd再起動1service httpd restart kibana管理画面1http://(ドメイン)/#/dashboard/file/default.json 以下のように表示されたら成功 fluent-plugin-elasticsearch gcc, gcc-c インストール1yum install gcc gcc-c++ libcurl-devel fluent-plugin-elasticsearch インストール1/usr/lib64/fluent/ruby/bin/fluent-gem install fluent-plugin-elasticsearch --no-ri --no-rdoc 1vim /etc/td-agent/td-agent.conf 12345678910111213141516171819202122232425262728293031323334353637383940## Input&lt;source&gt; type tail path /var/log/httpd/access_log format /^(?&lt;date&gt;\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} \\w{3}) (?&lt;processing_time&gt;[^ ]*) (?&lt;remote&gt;[^ ]*) (?&lt;user&gt;[^ ]*) \\[(?&lt;method&gt;.*)\\] (?&lt;status&gt;[^ ]*) (?&lt;size&gt;[^ ]*) \\[(?&lt;referer&gt;[^ ]*)\\] \\[(?&lt;agent&gt;.*)\\]/ pos_file /var/log/td-agent/tmp/apache.access.log.pos tag apache.access&lt;/source&gt;## Output&lt;match apache.access&gt; type copy &lt;store&gt; type file path /var/log/td-agent/apache.access time_slice_format %Y%m%d time_format %Y%m%dT%H%M%S%z &lt;/store&gt; &lt;store&gt; type forward send_timeout 60s recover_wait 10s heartbeat_interval 1s &lt;server&gt; name (fluentdサーバ) host (fluentdサーバIP) port (Port) &lt;/server&gt; &lt;/store&gt; &lt;store&gt; type elasticsearch host (elasticsearchサーバIP) port (Port) type_name access_log logstash_format true logstash_prefix apache_access logstash_dateformat %Y%m flush_interval 10s &lt;/store&gt;&lt;/match&gt; fluentd インストール事前準備 ユーザ毎のリソース制限ファイル修正 /etc/security/limits.conf 以下追記 12root soft nofile 65536root hard nofile 65536 ▼リソース属性 noproc最大プロセス数 nofileオープンできる最大ファイル数 maxlogin最大ログイン数 data最大データサイズ fsize最大ファイルサイズ as最大メモリ空間サイズ priorityユーザ実行の優先度 stackユーザ実行の最大スタック rssユーザ実行プロセスのメモリサイズ coreコアファイルの最大値 カーネルパラメータ設定 /etc/sysctl.conf 1234// 以下追記net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_tw_reuse = 1net.ipv4.ip_local_port_range = 10240 65535 再起動1reboot Apatch設定テスト用のログファイルとしてApacheのアクセスログを使用1grep &quot;custom&quot; /etc/httpd/conf/httpd.conf ▼実行結果 12LogFormat &quot;%{%Y-%m-%d %T %Z}t %D %a %u [%r] %s %b [%{Referer}i] [%{User-Agent}i]&quot; customCustomLog logs/access_log custom td-agent にアクセス出来る様にログディレクトリ権限修正1chmod 755 /var/log/httpd td-agentのインストール1curl -L http://toolbelt.treasuredata.com/sh/install-redhat.sh | sh td-agent 設定 /etc/td-agent/td-agent.conf 1234567891011121314151617181920212223242526272829303132&lt;match log.**&gt; # fluentd-plugin-elasticsearch利用 type elasticsearch # Kibanaで利用するためにindexの形式整備 logstash_format true # index の prefix 指定 logstash_prefix demo-log # 転送先の Elasticsearch hosts localhost:9200 # Elasticsearchに書き込む際、ドキュメントtype指定 type_name application-log # buffer設定 - メモリバッファ利用 buffer_type memory # チャンクサイズ 1MB buffer_chunk_limit 1m # チャンクキュー最大サイズ 128 buffer_queue_limit 128 # 指定秒毎にバッファをflush - 指定秒数毎にElasticsearchへの書き込みリクエスト発行 flush_interval 2s # flush失敗時、最大リトライ回数 retry_limit 17&lt;/match&gt; 12345678// tmpディレクトリ作成mkdir /var/log/td-agent/tmp// 所有者修正chown td-agent.td-agent /var/log/td-agent/tmp// サーバ起動時設定chkconfig td-agent on// 起動service td-agent start 参考サイト http://okochang.hatenablog.jp/entry/2014/03/17/223805 http://fluentular.herokuapp.com/ http://okochang.hatenablog.jp/entry/2014/03/21/191523","link":"/2015/02/28/2015-03-01-fluentd-elasticsearch-kibana-sakura-vps/"},{"title":"ドメインの所有者を確認する方法","text":"概要クライアント様所有のドメインにSSL証明書をインストールして欲しいという依頼についてthawteよりSGC SuperCertsから申請しSSL証明書発行する経緯となった。 その申請に必要なDUNSナンバー取得の為、ドメインの所有者がクライアント様になっているかを確認する必要があった。 DUNSナンバーの確認方法は以下 DUNSナンバー確認手順概要thawteにて「SGC Super Certs」の申請する際DUNSナンバーが必要となる為クライアント様がDUNS(ダンズ)ナンバーを所持されているか確認する必要がありました。 http:&amp;#x2F;&amp;#x2F;www.tsr-net.co.jp&amp;#x2F;service&amp;… 手順WHOISというドメインの検索サービスで検索可能です。 http://whois.jprs.jp/ 上記リンクからWHOISにアクセスし、検索したいドメイン名を入力する。「検索」ボタンをクリックすると所有者情報が表示されます。 thawteのSSL発行時には以上の情報をもとにCSR発行手続きします。 以上","link":"/2015/02/28/2015-03-01-show-domains-owner/"},{"title":"EC-CUBEベリトランス設定","text":"前提ベリトランス契約を完了していただいていることが条件となります。 テスト用と本番用モジュール設定があるので注意してください。注意点を以下にまとめます。 注意点本番環境用のベリトランス決済モジュールはベリトランス開通した時点から料金発生となります。 流れとしては、以下を検討する必要があります テスト環境用テスト用モジュール情報をオペよりいただき設定する。 本番用にベリトランスへサービスのローンチ1〜2週間前程度で開通依頼し、本番用を設定 手順管理画面＞オナーズストア＞購入商品一覧 リンク押下します。 「購入商品一覧を取得する」ボタンを押下します。 モジュール未ダウンロードの場合、ベリトランス行のダウンロードリンクを押下します。 ダウンロードに失敗する場合、ログチェック ログ管理＞詳細 クリック ログ状況を確認してください。大抵書き込み権限がない場合なので、書き込み権限付与してください。 1$ chmod -R 0777 (EC-CUBEパス)/data/downloads 再度ダウンロードを実行してください。 ベリトランス行の設定リンクを押下します。 以下のような内容が別ウィンドウで表示されるので情報を入力していきます。 ベリトランス3G MDK決済モジュールの各項目を設定します。マーチャントCCID, 認証鍵を設定します。取引IDプレフィックスは決済情報に付加される文字列です。以下のようにテスト用では、設定した日付などを入れておいてください。 取引IDプレフィックスおおよそ以下で設定で問題なし。 テスト用 : (サイトエイリアス)df(設定日)_ 本番用 : (サイトエイリアス)df テスト用DBに本番DBを同期することがあるかと思います。 一度決済済みの情報が流入すると決済が通らなくなるので本番→テストDB同期した際は、テスト用取引IDプレフィックス変更する様にします。 クレジットカード設定以下基本セットです。クライアントより特別指示がない限りは以下で問題ありません。 その他決済設定についてクライアントより指示がない限り「有効にする」チェックボックスを外してください。 コンビニ決済設定 電子マネー(Edy)決済設定 電子マネー決済(Suica)設定 銀行・郵貯(Pay-easy)決済設定 銀聯ネット決済設定 PayPal決済設定 上書きファイル一覧設定ファイルを自動上書きしない(カスタマイズ利用者向け)にラジオボタンを入れてください。 設定登録「この内容で登録する」ボタンをクリックする 設定反映されると以下のようなポップアップが確認されます。 引き続き支払い設定等記載して参ります。 参照 https://www.ec-cube.net/document/sbi/sbi_211.pdf https://www.ec-cube.net/product/veritrans.php 以上","link":"/2015/03/03/2015-03-04-setting-veritrans-on-eccube/"},{"title":"CentOS にmuninインストール 監視しアラートメール受信","text":"概要 CentOSにmuninをインストールし死活監視します。 warning, criticalを検知した際にアラートメール送信をします。 MySQL, Redisの監視設定も記載しました。 環境CentOS 5.11(Final)CentOS 6.5(Final)munin 2.0.21 手順 参考 http://castor.s26.xrea.com/blog/2007/10/19","link":"/2015/03/04/2015-03-05-install-munin-and-notify-alart-mail-on-centos/"},{"title":"MacOSX パスワード付きzipファイル作成法","text":"概要クライアント様へ資料等を送付する場合、パスワード付きの圧縮ファイルでメール2回に分けて送付すると思います。 1回目: 圧縮ファイル添付2回目: 圧縮ファイルのパスワード そんなときよく使っているTerminalでのパスワード付きzipファイル作成方法を以下にまとめました。 手順パスワード生成 生成したパスワードを保存します。 パスワード付きzipファイル生成 以上","link":"/2015/03/10/2015-03-11-how-to-make-zipfile-with-password-on-macosx/"},{"title":"MacOSX ipv6 OFF","text":"概要Yosemite のネット回線遅い？と思うことが多く初期設定でipv6設定されている為世の中のipv6化しきっていないのもあり一旦ipv6をOFF設定しようと思います。 ps↓ こんな記事も http://minoru.jetsets.jp/%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF/os-x-yosemite-%E3%82%A4%E3%83%B3%E3%82%BF%E3%83%BC%E3%83%8D%E3%83%83%E3%83%88%E3%81%8C%E3%82%84%E3%81%A3%E3%81%B1%E3%82%8A%E9%81%85%E3%81%84/","link":"/2015/03/10/2015-03-11-off-ipv6-macosx/"},{"title":"httpd: Could not reliably determine the server&#39;s fully qualified domain name, using 127.0.0.1 for ServerName","text":"概要Apache再起動時に以下のようなエラー文が表示される。※Apache自体は問題なく再起動できています。 12345service httpd restarthttpd を停止中: [ OK ]httpd を起動中: httpd: Could not reliably determine the server's fully qualified domain name, using 127.0.0.1 for ServerName [ OK ] ServerNameに127.0.0.1を使用しているFQDNを確実に判断できません。 原因/etc/hosts で「127.0.0.1」で設定されているhost名がApache設定ファイルで定義されていない hogehostについてApache定義ファイルで設定されていない。 1127.0.0.1 hogehost localhost.localdomain localhost /etc/httpd/conf/httpd.conf 12#ServerName www.example.com:80ServerName hogehost:80","link":"/2015/03/15/2015-03-16-httpd-error-could-not-reliably-determine-the-servers-fully-qualified-domain-name/"},{"title":"logrotate ログファイル名に日毎に日付追加 保存期間 過去ファイル圧縮 設定","text":"概要/etc/logrotate.conf 初期設定では、以下の不便さがあります。 error_log.1とファイル名に連番が振られていて、具体的な日付がわかりにくい。 基本、logを残すのは4week となっており、1ヶ月分程度しか情報がなく、過去に遡ってそもそも調査できないケースがある ファイルが圧縮されず容量を食ってしまう。 調査時に非常に不便です。 httpdをyumでインストールした場合、初期logrotate.confの設定は以下のようになっています。 日付をログファイル名に付与する/etc/logrotate.conf に以下一文を追記してください 1dateext ログの保存期間を半年(53/2 週≒27)にする12# keep 4 weeks worth of backlogsrotate 27 ※ディスク容量の問題もあるかと思いますので設定前に月どの程度のサイズになるか設定を検討してから決定してください。 日付過去ファイルを圧縮する 12# uncomment this if you want your log files compressedcompress apacheログの日付毎のrotate 設定後翌日以下のように出力されていることがわかります。 12error_log-20150315.gzerror_log logrotate.confに全角文字が存在する場合、正しくrotateされない可能性があるので注意してください。","link":"/2015/03/16/2015-03-17-logrotate-daily/"},{"title":"EC-CUBE管理画面でパスワード不用でログインさせる方法","text":"概要EC-CUBEパッケージで管理画面にログイン情報なしにログインする方法をまとめました。 管理画面パスワードが誰も知らず、アクセスできないなんてときに以下実施しました。 上記修正後、パスワードなしでログインできます。 ログイン後 システム設定 &gt; メンバー管理 からパスワードを再設定してください。 ソースを元に戻してください。","link":"/2015/03/17/2015-03-18-login-without-password-to-admin/"},{"title":"Google Analytics Tagの設定をチェックする","text":"手順Google Chrome ExtensionでTag Assistantをインストールする。ツール表示箇所に以下のように表示される。 設定箇所該当ページアクセスしTag Assistantマークをクリック Remaketing タグ を設定している場合 Remarketing, 購入タグ設定している場合 以上","link":"/2015/03/23/2015-03-24-check-google-analytics-tag/"},{"title":"robots.txtを配置してsitemap.xmlを読み込ませSEO効率を上げる","text":"概要SEO対応としてリリース時にやっておきたいことの1つです。検索エンジンのクローラーに読み込ませたいファイルを指定することでSEO効率を上げます。 前提サイトリリース可能な状態にある、または、リリース可能にほぼほぼ近い状態にあること。 リリース直前・直後くらいに実施する作業という認識です。 各ファイルの概要robots.txtrobots.txt とは、goo、Google、Lycos などのロボット型検索エンジンに対する命令を記述するためのファイルです。 sitemap.xmlsitemap.xmlはXML形式でURLやタイトルなどの各ページの情報を記載したファイルです。 手順sitemap.xml作成以下sitemap generatorで作成可能です。 http://tafcue.com/xml_sitemap-convenient_tools/xml_sitemap_tools001/ ※不必要なURLがある場合は削除したり、追加したいURLは自分で編集する必要があります。 例) 詳細ページが様々なURLで表示している場合 検索エンジンにとって１つのコンテンツに対してユニークなURLとなることが望ましいです。 以下いずれも同一ページ内容となる場合、1つのページが様々なURL表記となり、よろしくないです。 http://hogehoge.com/products/detail.php?product_id=100 http://hogehoge.com/products/detail.php?product_id=100&amp;name=ProductName http://hogehoge.com/products/detail.php?product_id=100&amp;category_id=10 上記のような場合があるときは1つに絞ってください。 ドキュメントルートにsitemap.xmlを配置します。 /path/to/DocumentRoot/sitemap.xml ドキュメントルートに以下のように記載したrobots.txtを配置します。 /path/to/DocumentRoot/robots.txt 以上で検索エンジンのクローラーにsitemap.xmlを読み込ませるように設定ができました。 確認方法GoogleWebMaster Toolで設定状況を確認することができます。 https://www.google.com/webmasters/tools/ ※登録無料です。 以上","link":"/2015/04/05/2015-04-06-robot-txt-for-seo/"},{"title":"MySQL vs postgreSQL コマンド対比","text":"MySQL vs PostgreSQLコマンド対比表 |*項目|*MySQL|*PostgreSQL||DB接続|mysql -h hostname -u user -ppasswd |psql -h hostname -U user ||DB一覧表示|SHOW DATABASES;|\\l||DB変更|use db_name|psql db_name||DB作成|CREATE DATABASE db_name;|CREATE DATABASE db_name;||ユーザ一覧表示|SELECT * FROM mysql.user;|SELECT * FROM pg_shadow;||ユーザ追加|CREATE USER username IDENTIFIED BY [PASSWORD] ‘password’|CREATE ROLE username WITH LOGIN PASSWORD ‘password’||ユーザパスワード変更|SET PASSWORD FOR user = PASSWORD(‘password’);|ALTER USER username password ‘password’ALTER USER username with encrypted password ‘password’;||全権限設定|GRANT ALL PRIVILEGES ON DATABASE database TO username;|GRANT ALL PRIVILEGES ON DATABASE database TO username;||TABLE一覧表示|SHOW TABLES|\\d||FIELD一覧表示|SHOW CLOMNS FROM tbl_nameSHOW COLOMNS FROM tbl_name FOM db_name|\\d tbl_name||SQL実行| SELECT * FROM tbl_name;UPDATE tbl_name SET column=’‘;DELETE FROM tbl_name; |SELECT * FROM tbl_name;UPDATE tbl_name SET column=’‘;DELETE FROM tbl_name;||DB接続切断|exit;|\\q|","link":"/2015/04/06/2015-04-07-mysql-vs-postgresql/"},{"title":"CentOS7にredisインストール","text":"概要CentOS7でのredisインストールと起動方法まとめ 以上","link":"/2015/04/07/2015-04-08-install-redis-centos7/"},{"title":"ターミナルからファイル指定しSublime Text 2で開く","text":"環境 MacOSX 10.10.2 SublimeText2 2.0.2 シンボリックリンク1$ sudo ln -s /Applications/Sublime\\ Text\\ 2.app/Contents/SharedSupport/bin/subl /usr/bin/subl 指定ファイルをsublimetextで開く1$ subl filename","link":"/2015/04/09/2015-04-10-open-sublimetext2-via-termianl/"},{"title":"MacOSX+Vagrant (CentOS7)にSenrtyをインストールして動作確認するまで","text":"概要https://sentry.readthedocs.org/en/latest/quickstart/ 環境 MacOS 10.10.2 Yosemite Vagrant 1.6.5 Virtual Box 4.3.20 r96996 CentOS 7.1.1503 (Core) Python 2.7.5 (pip 6.0.8) Redis 3.0.0 MySQL 5.6 NginX 1.6.2 事前準備Sentry公式サイトには、以下セットアップをしておくようにと書いてあります。 Python2.7 python-setuptools, python-pip, python-dev, libxslt1-dev, libxml2-dev, libz-dev, libffi-dev, libssl-dev DB (PostgreSQL:推奨 or MySQL) =&gt; MySQL採用します Redis NginX Sentry公式サイトでは、OSはUbuntuで試験しておりますが上記環境にてCentOSでも動作確認が取れております。 VagrantにCentOS7 boxイメージを追加/起動/SSH接続※IPをvagrantfileのデフォルト「192.168.33.10」と設定します。 pip インストール ※ pipで各種モジュールインストール時のエラー対処 [command ‘gcc’ failed with exit status 1] Redis インストール MySQLインストール Sentry用にMySQL初期設定 NginXインストール/firewall http通信許可設定 NginX /etc/nginx/conf.d/default.conf Sentry用設定 手順Sentry インストール/起動 celery起動 celeryインストールされていなければ以下でインストール実行 1pip install celery URLアクセスhttp://192.168.33.10 へアクセス ログインページが表示されます。 以下、ロギングに必要なことを実施していきます。 ログインアカウント チーム作成 プロジェクト作成 ロギング ログインしてください。※アカウントがなければ「新しいアカウントを作成」リンクから作成してください チーム作成 プロジェクト作成 新規プロジェクトが作成されました。 「Go It !」ボタンをクリックすると作成したプロジェクトページへ遷移します。 プロジェクトページの「設定」タブをクリック 実際にログを送信してみます。APIキーを確認します。左メニューの「APIキー」リンクをクリック Defaultキーをコピー テスト送信MacOSX Terminalから以下実行ravenで非同期送信します。 1raven test (コピーしたDefault APIキー) ravenをインストールしてなければ以下のようにbrewでインストールしてください。 ストリームタブにてイベントが追加されたことがわかります。 PHPからログを投げてみる。実施方法が記載されているので確認します。 設定タブをクリックした後、「Setup &amp; Installation」をクリック PHPアイコンをクリック MacOSローカルにてraven-phpをgitから落としセットアップします。 12git clone https://github.com/getsentry/raven-phpcd raven-php/ 新規ファイル「t.php」を作成 t.php t.php実行 1php t.php 以下のようにPHPからもSentryへ投げたログを確認することが可能です。 Python, Goからもログを投げることができます。是非試してみてください。 以上","link":"/2015/04/10/2015-04-11-install-sentry-on-centos7/"},{"title":"MacOSXにgoをインストール","text":"概要golangのとっかかりとして簡単に Googleが開発した言語 動的型付け (Like Python) 分散処理が得意などなど Wiki参照 http://ja.wikipedia.org/wiki/Go_%28%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0%E8%A8%80%E8%AA%9E%29 手順brew をインストールしていない場合は以下参照してください。 MacOSXにHomebrewをインストールする1ruby -e &amp;quot;$(curl -fsSL https:&amp;#x2F;&amp;#x2F;raw.githubusercontent.com&amp;#x2F;Homebrew&amp;#x2F;install&amp;#x2F;master&amp;#x2F;install)&amp;quot; 以下MacOSX… brewからinstall hello.go ビルド1go build hello.go 実行1./hello Hello, Worldとして表示されればOK !!","link":"/2015/04/11/2015-04-12-install-go-on-macosx/"},{"title":"Linux 32bitか64bitか確認する","text":"システム情報表示1$ uname -a i686, i686, i386–&gt; 32 bit X86_64, amd64–&gt; 64 bit","link":"/2015/04/12/2015-04-13-show-32-or-64-bit-on-linux/"},{"title":"git rev-parse でできること","text":"","link":"/2015/04/16/2015-04-17-git-rev-parse/"},{"title":"独自pre-commit設定","text":"手順 pre-commitは以下からどうぞ https://github.com/kenzo0107/git-hooks php構文チェック (php -l ) masterでのcommit禁止 php-cs-fixerでPSR準拠でコード変換処理はコメントアウト中","link":"/2015/04/16/2015-04-17-original-pre-commit/"},{"title":"MacOSX  zsh を使おう！","text":"メリット ターミナル上で補間機能をつけられる コマンドやファイルパス、大文字・小文字も変換して補間 手順zshがあるか確認1234$ cat /etc/shells.../etc/zsh なければ、brewでinstall 12$ brew install zsh$ sudo sh -c 'echo $(which zsh) &gt;&gt; /etc/shells' shellをzshに切り替え12$ chpass -s /bin/zshPassword: (パスワード入力) デフォルトのshell設定 1$ chsh -s /bin/zsh 個人の.zshrc設定は以下のようにしてます。 https://github.com/kenzo0107/dotfiles 上記をダウンロードしホームディレクトリ(~/)に.zshrcを配置します。 gitで管理しているので以下のようにリンク貼るのもよしです。 1$ ln .zshrc ~/.zshrc zsh設定ファイル読み込み 1$ source .zshrc ターミナルを再起動すると以下のように設定が反映されたことが確認できます。","link":"/2015/04/23/2015-04-24-lets-use-zsh/"},{"title":"Mac - caps lock を Control に","text":"概要CapsLockなんていらん！という人向けにメモ代わり。 手順メニュー マーク &gt; システム環境設定クリックキーボードを選択 修飾キー選択 Caps Lock キーをControlにアサイン 以上","link":"/2015/04/26/2015-04-27-capslock-to-control-on-macosx/"},{"title":"git コマンドまとめ","text":"123456789101112131415161718192021222324252627282930313233343536// リモートのorigin/developをlocalのdevelopブランチへチェックアウトするgit checkout -b develop origin/develop// 特定のファイルの編集内容を取り消す。Commitされているファイルの状態に戻す。git checkout &lt;filepath&gt;// commit一覧表示git loggit log --pretty=oneline// 直前のcommitを取り消すgit commit --amend// commitをなかったことにする。git reset --hard HEAD// indexに加えたファイルを元に戻すgit reset (file_name)// 過去n個のcommitをなかったことにする。git reset --hard HEAD~{n}// pushをなかったことにする。(remoteのコミット情報を削除)git push -f origin HEAD// タグ一覧確認git tag -n// タグ付けgit tag -am \"&lt;message&gt;\" &lt;tag_name&gt;// タグ削除git tag -d &lt;tag_name&gt;// 作業中ファイルの退避git stash// 作業中ファイルの吐き出しgit stash pop","link":"/2015/04/26/2015-04-27-git-command-list/"},{"title":"SSH autologin でサーバログイン 〜毎回sshコマンドいちいち打つのはやめよう〜","text":"環境MacOSX 10.10.3 Yosemite 概要サーバへSSHでログインして、と伝えるとExcelやらテキストを開いてサーバ情報をコピペしてはっつけて…と頑張っている人をみてこれはしんどそうだなぁと思って作ったシェルです。 手順以下gitにまとめました。使い方はREADMEに書いておきました。 サーバ情報はprojects.yamlに追記していきます。 https://github.com/kenzo0107/SSHAutoLogin 以下のような具合にシェルコマンド実行すればログインできます。 hoge : プロジェクトの省略名prd : ログインしたい環境 1$ sh al.sh hoge prd 以上","link":"/2015/04/30/2015-05-01-ssh-autologin/"},{"title":"独自git-hooksをglobal設定","text":"概要ローカルの全git管理の開発環境に適用する。 やったこと今回はpre-commitの設定 PHPのシンタックスチェック masterブランチでのcommit禁止→ masterはmergeしてpushのみ。 ※PHPのシンタックスでphpmdを利用しているので phpmdインストールしておく 1$ brew install phpmd 手順以下、git-hookのREADMEに手順記載しました。 https://github.com/kenzo0107/git-hooks 備考その他、pre-pushで今いるbranchから同名のリモートbranchにのみpushを許可するなど諸々まとめたらまた記載します。 以上","link":"/2015/05/01/2015-05-02-git-hooks-global-template/"},{"title":"簡単♪ Vagrant で Ansible 設定 -  pingで疎通確認までを3分で実施","text":"Vagrantでローカル開発環境構築を簡易的にAnsibleで実施した手順を以下githubにまとめました。 GitHub - kenzo0107/Vagrant-Ansible: Default Ansible Setting on VagrantDefault Ansible Setting on Vagrant. Contribute to kenzo0107/Vagrant-Ansible development by creating an account on GitHub. Ansibleの初期設定が面倒だったのでそこを簡易化することに努めました。","link":"/2015/05/07/2015-05-08-ansible-on-vagrant/"},{"title":"AnsibleでSentry導入！起動確認まで10分以内♪","text":"SentryとはSentryはアプリからイベントログを送信し集計するツールです。エラーログの管理にも利用できます。 そのエラーが解決したかしていないか、どの程度の頻度で発生するか、などグラフ化してくれます。 他言語対応していたり非エンジニアのプロジェクトメンバーにもエラーログを意識させることができるのでその辺りも有用です。 概要以前、 MacOSX+Vagrant (CentOS7)にSenrtyをインストールして動作確認するまでという記事を書きましたが今回はそれをAnsibleでまとめました。 環境 MacOSX 10.10.3 Yosemite Virtual Box 4.3.26 Vagrant 1.7.2 ansible 1.9.1 CentOS7 手順GithubのREADMEを参照してください。手順通りで10分で起動確認できます。 https://github.com/kenzo0107/vagrant-ansible-sentry-centos7","link":"/2015/05/09/2015-05-10-install-sentry-by-ansible/"},{"title":"net-tools vs. iproute2 対応表","text":"RHEL7/CentOS7ではnet-toolsを廃止予定としています。 ipコマンド推奨の理由は以下。 net-toolsでは、 ネットワークトラフィック制御対応できない。 ルーティングテーブルを複数保持するような複雑なルーティングを実現できない。 ifconfigでは/procからの情報を出力しており、ipコマンドのnetlinkAPIを利用した方が数ミリsec速い。 http://linuxjm.osdn.jp/html/LDP_man-pages/man7/netlink.7.html なので、徐々にipコマンドに慣れていきたい。 net-tools vs. iproute2 |*net-tools |*iproute2 || ifconfig | ip l (ip link) || ifconfig -a | ip a show (ip addr show) || ifconfig eth0 up | ip link set eth0 up || netstat | ss || netstat -i | ip -s link || netstat -l | ss -l || netstat -r | ip r (ip route) || route [add or del] | ip route [add or del] || route -n | ip route show || arp -n | ip n (ip neighbor) |","link":"/2015/05/10/2015-05-11-net-tools-vs-iproute2/"},{"title":"CentOSにPHPUnitをComposer.phar経由でインストールする。","text":"概要yum でインストール場合、以下コマンド実行します。 1$ sudo yum install php-xml php-pear php-phpunit-PHPUnit --enablerepo=epel yumの場合、既存phpモジュールを上がいて既存環境に影響を与える可能性があります。その為、composer経由で影響なくインストールする手順を以下にまとめました。 手順","link":"/2015/05/13/2015-05-14-install-phpunit-via-composer.phar-on-centos/"},{"title":"AWS EC2(CentOS) SSHログイン設定ToDo","text":"","link":"/2015/05/14/2015-05-15-ssh-login-to-ec2/"},{"title":"MacOSX環境にgoインストールして実行確認まで","text":"環境 MacOSX10.10.3 Yosemite インストール手順brewでインストール1$ brew install go goバージョン確認1$ go version .bash_profile に追記※zshの場合、~/.zshrcに追記 123export GOROOT=/usr/local/opt/go/libexecexport GOPATH=$HOMEexport PATH=$PATH:$GOROOT/bin:$GOPATH/bin サンプルファイル(hello.go)作成vim hello.go 1234567package mainimport &quot;fmt&quot;func main() { fmt.Printf(&quot;Hello, World\\n&quot;)} ファイル実行1go run hello.go 「Hello, World」と表示されれば成功！Go言語のインストール完了です。 以上","link":"/2015/05/17/2015-05-18-install-go-macosx/"},{"title":"nslookup で レコードタイプ確認","text":"概要DNSのクラウド管理サービス(お名前.com等)でレコード設定すると「設定反映まで72時間掛かる場合があります。」とメッセージが表示されることがあります。 設定確認法としてnslookupコマンドで設定を確認してみたのでメモ 手順","link":"/2015/05/17/2015-05-18-nslookup/"},{"title":"Vagrant+Ansibleでhttpd, MySQL, PHPをソースからインストールし起動確認するまで","text":"開発環境 MacOSX Yosemite 10.10.3 VirtualBox 4.3.26 r98988 Vagrant 1.7.2 Ansible 1.9.1 手順1. Vagrant Box追加1$ vagrant box add centos6.5 https://github.com/2creatives/vagrant-centos/releases/download/v6.5.1/centos65-x86_64-20131205.box 2. gitリポジトリをclone1$ git clone https://github.com/kenzo0107/Vagrant-Ansible 3. vagrantからVM起動12$ cd Vagrant-Ansible/centos6$ vagrant up 4. ssh-config設定をssh.configへ転記1$ vagrant ssh-config &gt; ssh.config 5. VM疎通テスト1$ ansible default -m ping 1234default | success &gt;&gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;} 上記のようにsuccessと出力されれば成功 6. Ansibleで環境構築実行1$ ansible-playbook lamp.yml 時折、Timeoutが確認されましたが、再度実行いただくと問題なく構築されたことを確認してます。 7. Apache起動確認http://192.168.33.10にアクセスしWorking!と表示されれば成功 8. phpからMySQLへの接続確認http://192.168.33.10/dbtest.phpにアクセスしConnect Success: Localhost via UNIX socketと表示されれば成功 以上です。","link":"/2015/05/19/2015-05-20-install-php-httpd-mysql-on-vagrant/"},{"title":"Vagrant + Ansible で go実行環境構築し、フレームワーク(goji)で簡易的なCRUD処理実行","text":"環境 MacOSX Yosemite 10.10.3 Virtual Box 4.3.28 Vagrant 1.7.2 Ansible 1.9.1 go実行構築環境VagrantへAnsibleで以下環境構築します。 Linux(Centos6.5) + Nginx + MySQL + Go 12345$ git clone https://github.com/kenzo0107/Vagrant-Ansible$ cd Vagrant-Ansible/centos6/lnmg$ vagrant up$ vagrant ssh-config &gt; ssh.config$ ansible-playbook lnmg.yml sshログインしてgoのバージョン確認します。123$ vagrant ssh[vagrant@vagrant-centos65 ~]$ go versiongo version go1.4.1 linux/amd64 DB: testdb作成123[vagrant@vagrant-centos65 ~]$ mysql -u rootmysql&gt; create database testdbmysql&gt; quit Go SampleProject ダウンロードプロジェクトtree1234567891011121314151617181920212223go_project├── db│ └── migrate.go├── go-sql-driver│ └── mysql├── jinzhu│ └── gorm├── lib│ └── pq├── models│ └── user.go├── route├── route.go├── user_controller.go├── views│ └── user│ ├── exit.html│ ├── index.html│ └── new.html├── wcl48│ └── valval└── zenazn └── goji vagrant にsshログインしgo getでgo_projectインストール 12$ vagrant ssh[vagrant@vagrant-centos65 ~]$ go get github.com/kenzo0107/go_project project内のgoのbinaryファイルのrouteを実行 123[vagrant@vagrant-centos65 ~]$ cd ~/go/src/github.com/kenzo0107/go_project[vagrant@vagrant-centos65 ~]$ ./route&lt;span style=&quot;color: #0000cc&quot;&gt;2015/06/14 14:27:07.749545 Starting Goji on [::]:8000&lt;/span&gt; ユーザ入力ページへアクセス [http://192.168.33.11:8000/user/index] 12~/go/src/github.com/kenzo0107/go_project/routeはgo build route.go user_controller.goにより生成されたbinaryファイルです。 参考 http://qiita.com/masahikoofjoyto/items/b2e6c2cad447e48f91ee","link":"/2015/06/14/2015-06-15-go-framework-goji-on-vagrant/"},{"title":"コマンドラインでURLの存在チェック","text":"以下コマンドをMacローカルでterminalから実行 1curl -v &lt;URL&gt; 2&gt;&amp;1 1&gt;/dev/null | awk '{if($2~&quot;HTTP&quot;) print}' OK Pattern.1&lt; HTTP/1.1 200 OK NG Pattern1&lt; HTTP/1.1 404 Not Found","link":"/2015/06/15/2015-06-16-check-whether-the-url-exists/"},{"title":"find検索で「Permission denided」を除いて表示したい","text":"1find / -name 'hogehoge' 2&gt;/dev/null","link":"/2015/06/15/2015-06-16-find-command-results-exclude-permission-denied/"},{"title":"CentOSにmysqldiffインストールし実行確認","text":"rootユーザに切り替え1$ sudo su - mysqldiff インストール123# cd /usr/local/src# wget http://search.cpan.org/CPAN/authors/id/A/AS/ASPIERS/MySQL-Diff-0.43.tar.gz# tar zxvf MySQL-Diff-0.43.tar.gz mysqldiffにlibパスを設定1# vi MySQL-Diff-0.43/bin/mysqldiff 12345#!/usr/bin/perl -wuse lib '/usr/local/src/MySQL-Diff-0.43/lib'; ←追加=head1 NAME Slurpインストール1# yum -y install perl-File-Slurp シンボリックリンク 1# ln -s /usr/local/src/MySQL-Diff-0.43/bin/mysqldiff /usr/local/bin/mysqldiff mysqldiff実行確認1$ mysqldiff db1, db2 を比較 1$ mysqldiff db1 db2","link":"/2015/06/15/2015-06-16-install-mysqldiff-on-centos/"},{"title":"クライアント受注のWebサイト構築時のSSL証明書インストールする為にしたこと","text":"概要クライアントより受注したWebサイト構築時にSSL証明書をインストールする必要が生じ、その際に行った手続きをまとめます。 前提無料SSLや自作SSLではセキュリティ面で不安な為、シェアの高いSSL証明書発行しているthawteより取得することとしました。費用面についてもクライアントに了承を得ています。 thawte 世界第2位のシェアを誇る認証機関であり、各種サーバー認証及びコードさイニング認証を世界各国のお客様へ提供している。 ToDo WHOIS(フーイズ)でドメインの所有者情報確認 クライアント所持のDUNS(ダンズ)ナンバー確認 CSR発行 SGC Super Certs申請 SSL証明書インストール 手順1. WHOIS(フーイズ)でドメインの所有者情報確認ドメインの所有者を確認する方法概要クライアント様所有のドメインにSSL証明書をインストールして欲しいという依頼についてthawteよりSGC SuperCertsから申請しSSL証明書発行する経緯となった。 その申請に必要なDUNSナンバー取得の為、ドメインの所有者がクライアント様になっているかを確認する必要… WHOISで検索したドメインの所有者情報が表示されます。この情報を保管します。後にCSR発行時に利用します。 2. クライアント所持のDUNS(ダンズ)ナンバー確認DUNSナンバー確認手順 - 長生村本郷Engineers'Blog以下に移行しました。kenzo0107.github.io 3. CSR発行Apache + OpenSSL でSHA256対応CSR生成 - 長生村本郷Engineers'Blog以下に移行しました。kenzo0107.github.io 4. SGC Super Certs申請SGC SuperCerts 購入手順概要クライアントより指定ドメインにSSL証明書をインストールして欲しいとの依頼があり、比較的安価なthawteによりSSL証明書発行するよう手配しました。その手順を以下に記載します。 ※2015年4月時点で値上げした模様です。SSL比較サイト等で改めて検討してください。 SSL料… 5. SSL証明書インストールSSL証明書インストール対象サーバにアクセスし以下SSL証明書インストール手順に沿って進めます。 手順 rootユーザに変更12sudo su -Password: (rootユーザのパスワード入力) 秘密鍵のパスフレーズ解除apache再起動してもパスフレーズを問われないようにするためです。 …","link":"/2015/06/15/2015-06-16-install-ssl-certificate-for-customers/"},{"title":"SSL証明書インストール","text":"対象サーバにアクセスし以下SSL証明書インストール手順に沿って進めます。 手順 rootユーザに変更12sudo su -Password: (rootユーザのパスワード入力) 秘密鍵のパスフレーズ解除apache再起動してもパスフレーズを問われないようにするためです。 123openssl rsa -in server.key -out server.keyEnter pass phrase for server.key: (秘密鍵用パスフレーズ入力)writing RSA key SSL証明書作成thawteから届いたメールより発行された「証明書本体」をpublickey.crtに貼り付けして保存 12# publickey.crtファイル作成・編集vim /etc/httpd/conf/ssl.crt/publickey.crt 中間証明書作成thawteから届いたメールより発行された「中間証明書」をintermediate.crtに貼り付けして保存 12# intermediate.crtに貼り付けして保存vim /etc/httpd/conf/ssl.crt/intermediate.crt ssl.conf修正1vim /etc/httpd/conf.d/ssl.conf 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240# This is the Apache server configuration file providing SSL support.# It contains the configuration directives to instruct the server how to# serve pages over an https connection. For detailing information about these# directives see &lt;URL:http://httpd.apache.org/docs/2.2/mod/mod_ssl.html&gt;## Do NOT simply read the instructions in here without understanding# what they do. They're here only as hints or reminders. If you are unsure# consult the online docs. You have been warned.#LoadModule ssl_module modules/mod_ssl.so## When we also provide SSL we have to listen to the# the HTTPS port in addition.#Listen 443#### SSL Global Context#### All SSL configuration in this context applies both to## the main server and all SSL-enabled virtual hosts.#### Some MIME-types for downloading Certificates and CRLs#AddType application/x-x509-ca-cert .crtAddType application/x-pkcs7-crl .crl# Pass Phrase Dialog:# Configure the pass phrase gathering process.# The filtering dialog program (`builtin' is a internal# terminal dialog) has to provide the pass phrase on stdout.SSLPassPhraseDialog builtin# Inter-Process Session Cache:# Configure the SSL Session Cache: First the mechanism# to use and second the expiring timeout (in seconds).#SSLSessionCache dc:UNIX:/var/cache/mod_ssl/distcacheSSLSessionCache shmcb:/var/cache/mod_ssl/scache(512000)SSLSessionCacheTimeout 300# Semaphore:# Configure the path to the mutual exclusion semaphore the# SSL engine uses internally for inter-process synchronization.SSLMutex default# Pseudo Random Number Generator (PRNG):# Configure one or more sources to seed the PRNG of the# SSL library. The seed data should be of good random quality.# WARNING! On some platforms /dev/random blocks if not enough entropy# is available. This means you then cannot use the /dev/random device# because it would lead to very long connection times (as long as# it requires to make more entropy available). But usually those# platforms additionally provide a /dev/urandom device which doesn't# block. So, if available, use this one instead. Read the mod_ssl User# Manual for more details.SSLRandomSeed startup file:/dev/urandom 256SSLRandomSeed connect builtin#SSLRandomSeed startup file:/dev/random 512#SSLRandomSeed connect file:/dev/random 512#SSLRandomSeed connect file:/dev/urandom 512## Use &quot;SSLCryptoDevice&quot; to enable any supported hardware# accelerators. Use &quot;openssl engine -v&quot; to list supported# engine names. NOTE: If you enable an accelerator and the# server does not start, consult the error logs and ensure# your accelerator is functioning properly.#SSLCryptoDevice builtin#SSLCryptoDevice ubsec#### SSL Virtual Host Context##&lt;VirtualHost _default_:443&gt;# General setup for the virtual host, inherited from global configuration#DocumentRoot &quot;/var/www/html&quot;#ServerName www.example.com:443DocumentRoot &quot;/var/www/html&quot; (←本番ドメイン用のドキュメントルート設定)ServerName (本番ドメイン):443 (← ServerNameを 「(本番ドメイン):443」に設定)# Use separate log files for the SSL virtual host; note that LogLevel# is not inherited from httpd.conf.ErrorLog /var/log/ssl_error_log (←エラーログ設定)TransferLog /var/log/ssl_access_log (←アクセスログ設定)LogLevel warn# SSL Engine Switch:# Enable/Disable SSL for this virtual host.SSLEngine on# SSL Protocol support:# List the enable protocol levels with which clients will be able to# connect. Disable SSLv2 access by default:SSLProtocol all -SSLv2 -SSLv3# SSL Cipher Suite:# List the ciphers that the client is permitted to negotiate.# See the mod_ssl documentation for a complete list.SSLCipherSuite ALL:!ADH:!EXPORT:!SSLv2:RC4+RSA:+HIGH:+MEDIUM:+LOW# Server Certificate:# Point SSLCertificateFile at a PEM encoded certificate. If# the certificate is encrypted, then you will be prompted for a# pass phrase. Note that a kill -HUP will prompt again. A new# certificate can be generated using the genkey(1) command.#SSLCertificateFile /etc/pki/tls/certs/localhost.crtSSLCertificateFile /etc/httpd/conf/ssl.csr/publickey.crt (←証明書本体 絶対パス指定)# Server Private Key:# If the key is not combined with the certificate, use this# directive to point at the key file. Keep in mind that if# you've both a RSA and a DSA private key you can configure# both in parallel (to also allow the use of DSA ciphers, etc.)#SSLCertificateKeyFile /etc/pki/tls/private/localhost.keySSLCertificateKeyFile /etc/httpd/conf/ssl.csr/server.key (←秘密鍵 絶対パス指定)# Server Certificate Chain:# Point SSLCertificateChainFile at a file containing the# concatenation of PEM encoded CA certificates which form the# certificate chain for the server certificate. Alternatively# the referenced file can be the same as SSLCertificateFile# when the CA certificates are directly appended to the server# certificate for convinience.#SSLCertificateChainFile /etc/pki/tls/certs/server-chain.crtSSLCertificateChainFile /etc/httpd/conf/ssl.csr/intermediate.crt (←中間証明書 絶対パス指定)# Certificate Authority (CA):# Set the CA certificate verification path where to find CA# certificates for client authentication or alternatively one# huge file containing all of them (file must be PEM encoded)#SSLCACertificateFile /etc/pki/tls/certs/ca-bundle.crt# Client Authentication (Type):# Client certificate verification type and depth. Types are# none, optional, require and optional_no_ca. Depth is a# number which specifies how deeply to verify the certificate# issuer chain before deciding the certificate is not valid.#SSLVerifyClient require#SSLVerifyDepth 10# Access Control:# With SSLRequire you can do per-directory access control based# on arbitrary complex boolean expressions containing server# variable checks and other lookup directives. The syntax is a# mixture between C and Perl. See the mod_ssl documentation# for more details.#&lt;Location /&gt;#SSLRequire ( %{SSL_CIPHER} !~ m/^(EXP|NULL)/ \\# and %{SSL_CLIENT_S_DN_O} eq &quot;Snake Oil, Ltd.&quot; \\# and %{SSL_CLIENT_S_DN_OU} in {&quot;Staff&quot;, &quot;CA&quot;, &quot;Dev&quot;} \\# and %{TIME_WDAY} &gt;= 1 and %{TIME_WDAY} &lt;= 5 \\# and %{TIME_HOUR} &gt;= 8 and %{TIME_HOUR} &lt;= 20 ) \\# or %{REMOTE_ADDR} =~ m/^192\\.76\\.162\\.[0-9]+$/#&lt;/Location&gt;# SSL Engine Options:# Set various options for the SSL engine.# o FakeBasicAuth:# Translate the client X.509 into a Basic Authorisation. This means that# the standard Auth/DBMAuth methods can be used for access control. The# user name is the `one line' version of the client's X.509 certificate.# Note that no password is obtained from the user. Every entry in the user# file needs this password: `xxj31ZMTZzkVA'.# o ExportCertData:# This exports two additional environment variables: SSL_CLIENT_CERT and# SSL_SERVER_CERT. These contain the PEM-encoded certificates of the# server (always existing) and the client (only existing when client# authentication is used). This can be used to import the certificates# into CGI scripts.# o StdEnvVars:# This exports the standard SSL/TLS related `SSL_*' environment variables.# Per default this exportation is switched off for performance reasons,# because the extraction step is an expensive operation and is usually# useless for serving static content. So one usually enables the# exportation for CGI and SSI requests only.# o StrictRequire:# This denies access when &quot;SSLRequireSSL&quot; or &quot;SSLRequire&quot; applied even# under a &quot;Satisfy any&quot; situation, i.e. when it applies access is denied# and no other module can change it.# o OptRenegotiate:# This enables optimized SSL connection renegotiation handling when SSL# directives are used in per-directory context.#SSLOptions +FakeBasicAuth +ExportCertData +StrictRequire&lt;Files ~ &quot;\\.(cgi|shtml|phtml|php3?)$&quot;&gt; SSLOptions +StdEnvVars&lt;/Files&gt;&lt;Directory &quot;/var/www/cgi-bin&quot;&gt; SSLOptions +StdEnvVars&lt;/Directory&gt;# SSL Protocol Adjustments:# The safe and default but still SSL/TLS standard compliant shutdown# approach is that mod_ssl sends the close notify alert but doesn't wait for# the close notify alert from client. When you need a different shutdown# approach you can use one of the following variables:# o ssl-unclean-shutdown:# This forces an unclean shutdown when the connection is closed, i.e. no# SSL close notify alert is send or allowed to received. This violates# the SSL/TLS standard but is needed for some brain-dead browsers. Use# this when you receive I/O errors because of the standard approach where# mod_ssl sends the close notify alert.# o ssl-accurate-shutdown:# This forces an accurate shutdown when the connection is closed, i.e. a# SSL close notify alert is send and mod_ssl waits for the close notify# alert of the client. This is 100% SSL/TLS standard compliant, but in# practice often causes hanging connections with brain-dead browsers. Use# this only for browsers where you know that their SSL implementation# works correctly.# Notice: Most problems of broken clients are also related to the HTTP# keep-alive facility, so you usually additionally want to disable# keep-alive for those clients, too. Use variable &quot;nokeepalive&quot; for this.# Similarly, one has to force some clients to use HTTP/1.0 to workaround# their broken HTTP/1.1 implementation. Use variables &quot;downgrade-1.0&quot; and# &quot;force-response-1.0&quot; for this.SetEnvIf User-Agent &quot;.*MSIE.*&quot; \\ nokeepalive ssl-unclean-shutdown \\ downgrade-1.0 force-response-1.0# Per-Server Logging:# The home of a custom SSL log file. Use this when you want a# compact non-error SSL logfile on a virtual host basis.CustomLog logs/ssl_request_log \\ &quot;%t %h %{SSL_PROTOCOL}x %{SSL_CIPHER}x \\&quot;%r\\&quot; %b&quot;↓ディレクトリへのアクセス設定 ---start-----&lt;Directory &quot;/var/www/html&quot;&gt; (←ドキュメントルート指定) AllowOverride All Options -Indexes FollowSymLinks Includes ExecCGI Order allow,deny Allow from all&lt;/Directory&gt;↑ディレクトリへのアクセス設定 ---end-----&lt;/VirtualHost&gt; Apache再起動1service httpd restart https://(ドメイン)にアクセス可能か確認する","link":"/2015/06/15/2015-06-16-install-ssl-certificate/"},{"title":"UnityでAndroidアプリ「マメコラン！」公開♪","text":"アプリ公開しました♪Google Play 『マメコラン！』 アンドロイダー紹介VTR種ちゃんに紹介してもらいました♪ 新感覚走り泳ぎ飛びゲー！ マメコラン！「マメコラン！」kenzo0107https://androider.jp/official/app/0e098734256fe037/?from=apptane「マメコラン！」は、おとぎ話“ウサギとカメ”のその後を綴った、感動のストーリーが魅力のランニングアクションゲームなので… アンドロイダー詳細ページアンドロイドアプリ情報サイト[アンドロイダー]アンドロイドアプリとスマホの情報サイト、アンドロイダー(Androider)。おすすめのゲームアプリやアプリランキングなど、セキュリティ審査済みの公認アプリで安心スマホライフ！ 種ちゃんhttp://idol.lecre.jp/ アプカジュさんにご紹介いただきました！ ポイント基本走りゲーですが、世の中には面白い走りゲーはいっぱいあるのでさらに泳いだり、飛んだり、また、ストーリー重視で作りました♪ 進めば進むほどストーリーが明らかになっていく、実はあいつが…なんて感じに作りました。 笑いあり、涙ありです。ぜひプレイしてみてください(^-^) 環境 Unity 4.6 MacOSX Yosemite 10.10.2 さくらレンタルサーバ (¥1,000/月) - AnsibleでLAMP構築。 5rocks (現tapjoy) - クライアントサイド Unity用SDKあり リアルタイムでKPI測定可能。リモートプッシュ付き。月のActiveUser数で有料or無料に。 2015年5月28日時点 TapJoyになって若干Documentに誤記等あり、ちょっと心配。 ** fluentd + ElasticSearch + kibana でログ解析 - サーバサイド アイテムの利用率等を知る為導入しました。 工夫点マップはオブジェクトプールしてプレイヤーが進む毎に生成する、カメラから外れたら再度プールに戻す、という風に効率化しシーン間移動を素早くするようにしました。 アンドロイダー公認アプリになりました♪アンドロイドアプリ情報サイト[アンドロイダー]アンドロイドアプリとスマホの情報サイト、アンドロイダー(Androider)。おすすめのゲームアプリやアプリランキングなど、セキュリティ審査済みの公認アプリで安心スマホライフ！","link":"/2015/06/15/2015-06-16-mamekorun-android-application-by-unity/"},{"title":"SGC SuperCerts 購入手順","text":"概要クライアントより指定ドメインにSSL証明書をインストールして欲しいとの依頼があり、比較的安価なthawteによりSSL証明書発行するよう手配しました。その手順を以下に記載します。 ※2015年4月時点で値上げした模様です。SSL比較サイト等で改めて検討してください。 SSL料金比較サイト http://serverkurabe.com/ssl-matome/ http://www.ssl-concier.com/products/ 前提対象ドメインに対して以下が完了していること。 CSR生成済みであること。 DUNSナンバー取得済みであること。 クライアント担当者の氏名・役職・連絡先・会社住所を把握していること。 手順https://products.thawte.com/orders/thawte_sgc.do?ref=5480204MED80655&amp;contract=REF98555&amp;language_id=9&amp;change_lang=9:title 必要項目を選択し「次へ」ボタン押下する。 CSRの入力 補足サーバ名入力「Common Name」項目が対象のドメインになっていることを確認する。 サーバURLの確認 組織情報確認 赤枠に入力していきます。 連絡先情報入力こちらで登録したメールアドレスへ、購入完了メールが送信されます。クライアント窓口を確認し、記入する情報を貰い登録すること。 クレジット情報入力※クレジットのみの購入になります。 注文確認改めて全て再確認してください。問題なければ「注文を送信」で購入完了です。 数日すると問題なければSSL証明書がメールに添付されて届きます。 以上","link":"/2015/06/15/2015-06-16-sgc-supercerts/"},{"title":".htaccessでmod_rewriteを利用可能にする","text":"概要あまり意識せず利用しているmod_rewriteですがhttpd設定により使用できない場合があります。 以下設定手順をまとめました。 設定手順/etc/httpd/conf/httpd.conf 修正123456789101112131415LoadModule deflate_module modules/mod_deflate.so # ← 有効化LoadModule rewrite_module modules/mod_rewrite.so # ← 有効化AccessFileName .htaccess # ← 追加DocumentRoot &quot;/var/www/html&quot;&lt;Directory &quot;/var/www/html&quot;&gt; .... # Options Indexes FollowSymLinks Options Indexes FollowSymLinks ExecCGI # ← ExecCGI追加 # AllowOverride None AllowOverride All # ← Allに修正 ....&lt;/Directory&gt; ##上記修正後 httpd 再起動 1# service httpd restart","link":"/2015/06/23/2015-06-24-htaccess_mod_rewrite/"},{"title":"さくらVPS iptables設定 ~ある日警告文が届いた「ご利用中のサーバから、外部へ向けてUDP FloodによるDoSと思わしきトラフィックを確認いたしました。」~","text":"iptables (FireWall) Settingさくらレンタルサーバから以下のような警告文が届きました。 12345678ご利用中のサーバから、外部へ向けてUDP FloodによるDoSと思わしきトラフィックを確認いたしました。また、お客様のサーバを含めた複数のサーバにおいて同時に同じトラフィック波形のパケットを多数送信している事から、同じBot Netに属していると推測いたします。お心当たりがない場合、サーバを第三者に不正利用されている可能性がございます。現在、被害拡大防止の為の緊急措置として、当該サーバに対し通信停止措置を実施しております。予め、ご了承下さいますよう、お願いいたします。 要約すると サーバを踏み台にして外部に攻撃しているみたい、だから対策しないと止めるよ？ とのこと。 対策としてOS再インストールした後、以下iptablesの設定をしました。 設定手順以下root権限で実行 1su - 接続済み通信許可1iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT ローカルループバックアドレス許可1iptables -A INPUT -i lo -j ACCEPT ICMP許可1iptables -A INPUT -p icmp -j ACCEPT プライベートIPアドレス拒否123456iptables -A INPUT -s 10.0.0.0/8 -j DROPiptables -A INPUT -d 10.0.0.0/8 -j DROPiptables -A INPUT -s 172.16.0.0/12 -j DROPiptables -A INPUT -d 172.16.0.0/12 -j DROPiptables -A INPUT -s 192.168.0.0/16 -j DROPiptables -A INPUT -d 192.168.0.0/16 -j DROP ブロードキャストアドレス拒否12iptables -A INPUT -d 0.0.0.0/8 -j DROPiptables -A INPUT -d 255.255.255.255 -j DROP フラグメントパケット攻撃対策1iptables -A INPUT -f -j DROP ステルススキャン禁止1iptables -A INPUT -p tcp -m state --state NEW ! --syn -j DROP IDENT port probe対策1iptables -A INPUT -p tcp --dport 113 -j REJECT --reject-with tcp-reset PING Flood 対策1iptables -A INPUT -p icmp --icmp-type echo-request -m hashlimit --hashlimit 1/s --hashlimit-burst 5 --hashlimit-mode srcip --hashlimit-name input_icmp --hashlimit-htable-expire 300000 -j DROP 以下一般的なポート許可 不要な場合は設定しないで良いです。 ポートを変えている場合はそのポートで許可してください。 SSH ポート許可 (22)1iptables -A INPUT -p tcp -m tcp --dport 22 -j ACCEPT HTTP ポート許可 (80)1iptables -A INPUT -p tcp -m tcp --dport 80 -j ACCEPT デフォルトポリシーの設定123iptables -P INPUT DROPiptables -P OUTPUT ACCEPTiptables -P FORWARD DROP 設定確認1iptables -L --line-numbers -n 設定保存 反映12service iptables saveservice iptables restart まとめ現状この設定をしてからは特に被害にはあっていないです。 友人のさくらVPSでも同様の攻撃を受けたので教えてあげたら被害はなくなったとのことで 一定の効果はあるかと存じます。 ##コマンドまとめ 123456789101112131415iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPTiptables -A INPUT -i lo -j ACCEPTiptables -A INPUT -p icmp -j ACCEPTiptables -A INPUT -s 10.0.0.0/8 -j DROPiptables -A INPUT -d 10.0.0.0/8 -j DROPiptables -A INPUT -s 172.16.0.0/12 -j DROPiptables -A INPUT -d 172.16.0.0/12 -j DROPiptables -A INPUT -s 192.168.0.0/16 -j DROPiptables -A INPUT -d 192.168.0.0/16 -j DROPiptables -A INPUT -d 0.0.0.0/8 -j DROPiptables -A INPUT -d 255.255.255.255 -j DROPiptables -A INPUT -f -j DROPiptables -A INPUT -p tcp -m state --state NEW ! --syn -j DROPiptables -A INPUT -p tcp --dport 113 -j REJECT --reject-with tcp-resetiptables -A INPUT -p icmp --icmp-type echo-request -m hashlimit --hashlimit 1/s --hashlimit-burst 5 --hashlimit-mode srcip --hashlimit-name input_icmp --hashlimit-htable-expire 300000 -j DROP 123456iptables -A INPUT -p tcp -m tcp --dport 22 -j ACCEPTiptables -A INPUT -p tcp -m tcp --dport 80 -j ACCEPTiptables -P INPUT DROPiptables -P OUTPUT ACCEPTiptables -P FORWARD DROP 12service iptables saveservice iptables restart","link":"/2015/06/23/2015-06-24-sakua-vps-iptables/"},{"title":"Boot2DockerでMacOSXローカル環境に開発環境構築","text":"DockerEngine環境構築開発環境 MacOSX Yosemite 10.3.3 VirtualBox 4.3.28 Vagrant 1.7.2 Dockerを利用するには以下が必要 Docker Engine Dockerクライアント docker コマンド実行 Dockerサーバ Dockerコンテナ実行 Docker Engine 構成 DockerサーバとクライアントはRESTfulなHTTPSで通信する 123456789 +---------+ +----------+ |Docker | |Docker | |Container| |Container | +---------+ +----------+ ↑ ↑+-----------------+ +---------------+| Docker Client | ----&gt; | Docker Server ||(docker Command) | +---------------++-----------------+ 上記を構築するために以下をインストールする Boot2Docker DockerクライアントとDockerサーバをまとめてセットアップできるソフトウェア Boot2Docker構成 Docker Client Linux VM Docker Server 12345| | Docker Server || | Linux VM || Docker Client | VirtualBox |+-------------------------------+| Mac OSX or Windows | 導入手順Boot2Docker 公式サイト : より 「MacOSXボタン」クリック Boot2Docker パッケージインストール インストーラを実行 Terminal等で以下実行 Linux VM作成 1$ boot2docker init Linux VM起動 12345678910111213$ boot2docker startWaiting for VM and Docker daemon to start..............................oooooooooooooooooStarted.Writing /Users/kenzo/.boot2docker/certs/boot2docker-vm/ca.pemWriting /Users/kenzo/.boot2docker/certs/boot2docker-vm/cert.pemWriting /Users/kenzo/.boot2docker/certs/boot2docker-vm/key.pemTo connect the Docker client to the Docker daemon, please set: export DOCKER_HOST=tcp://192.168.59.103:2376 export DOCKER_CERT_PATH=/Users/kenzo/.boot2docker/certs/boot2docker-vm export DOCKER_TLS_VERIFY=1 Linux VMステータス確認 12$ boot2docker statusrunning 環境変数の設定 boot2docker start にて起動時にexport設定を実行する 123$ export DOCKER_HOST=tcp://192.168.59.103:2376$ export DOCKER_CERT_PATH=/Users/kenzo/.boot2docker/certs/boot2docker-vm$ export DOCKER_TLS_VERIFY=1 Docker Engine全体の環境設定確認 12345678910111213141516171819202122232425$ docker infoContainers: 0Images: 0Storage Driver: aufs Root Dir: /mnt/sda1/var/lib/docker/aufs Backing Filesystem: extfs Dirs: 0 Dirperm1 Supported: trueExecution Driver: native-0.2Logging Driver: json-fileKernel Version: 4.0.5-boot2dockerOperating System: Boot2Docker 1.7.0 (TCL 6.3); master : 7960f90 - Thu Jun 18 18:31:45 UTC 2015CPUs: 4Total Memory: 1.955 GiBName: boot2dockerID: G776:YBRC:OUGN:T7KF:TM43:6BTU:2PVW:HGWW:3CXO:YLCF:23ON:EJVEDebug mode (server): trueFile Descriptors: 9Goroutines: 15System Time: 2015-06-28T12:07:33.59750188ZEventsListeners: 0Init SHA1:Init Path: /usr/local/bin/dockerDocker Root Dir: /mnt/sda1/var/lib/docker 注意点docker infoで確認できる Storage Driverでaufsとなっている場合 Dockerコンテナhttpdインストールができない。 利用するStorage Driverを事前に確認する必要がある。 DockerFileベストプラクティスまとめhttps://docs.docker.com/articles/dockerfile_best-practices/","link":"/2015/06/27/2015-06-28-boot2docker-on-macosx/"},{"title":"Google Cloud SDKインストール For MacOSX","text":"概要2015/06/18(木) Google Cloud Platform Next に参加し無料クーポンも頂き早速利用したくなり登録！ お目当はKubanetes！ Google Cloud SDKをインストールする際、zsh利用していた為、ややてこずったのでまとめました。 手順","link":"/2015/06/27/2015-06-28-install-google-cloud-sdk-on-macosx/"},{"title":"CentOS7 に PHP5.6インストール","text":"概要CentOS7 で Laravel5.1を利用するべく、PHP5.6をインストールする運びとなった為、以下メモ。","link":"/2015/07/15/2015-07-16-install-php56-on-centos7/"},{"title":"Go言語 開発整備 on MacOSX","text":"環境 MacOSX 10.10.4 Yosemite Go 1.4 Eclipse Mars Release (4.5.0) 概要2015-08-01時点 IntelliJでgo開発を検討していましたがGo言語Ver 1.4 をサポートしていなかったのでEclipseに GoClipse を入れて補間機能等を整備します。 Go インストール1$ brew install go brewをインストールしていない場合は以下参照https://kenzo0107.github.io/2015/02/27/2015-02-28-install-homebrew-on-macosx/ Go バージョン確認12$ go versiongo version go1.4.2 darwin/amd64 環境変数設定個人的には ~/.zshrc 利用していますが、ない場合は ~/.bash_profile などに以下を追記してください。 12345# goif [ -x &quot;`which go`&quot; ]; then export GOPATH=$HOME/go export PATH=$PATH:$GOPATH/binfi ちなみに -x は実行可能か判定しています。すなわち「if [ -x “which go“]」 は which go というコマンドが実行できるかを判定しています。 現状ローカル環境で which go と入力すると以下のようになります。 12$ which go/usr/local/bin/go GOPATH Workspaceになります。プロジェクトはこちらに作っていくことになります。 go install, go get した際の保存場所になります。 go 環境情報確認1234567891011121314151617$ go envGOARCH=&quot;amd64&quot;GOBIN=&quot;&quot;GOCHAR=&quot;6&quot;GOEXE=&quot;&quot;GOHOSTARCH=&quot;amd64&quot;GOHOSTOS=&quot;darwin&quot;GOOS=&quot;darwin&quot;GOPATH=&quot;/Users/kenzo/go&quot;GORACE=&quot;&quot;GOROOT=&quot;/usr/local/Cellar/go/1.4.2/libexec&quot;GOTOOLDIR=&quot;/usr/local/Cellar/go/1.4.2/libexec/pkg/tool/darwin_amd64&quot;CC=&quot;clang&quot;GOGCCFLAGS=&quot;-fPIC -m64 -pthread -fno-caret-diagnostics -Qunused-arguments -fmessage-length=0 -fno-common&quot;CXX=&quot;clang++&quot;CGO_ENABLED=&quot;1&quot; ※ brewでgoをインストールした場合、GOROOTは /usr/local/Cellar/ 配下となります。 環境変数設定反映.zshrc の場合1$ source .zshrc .bash_profile の場合1$ source .bash_profile GoClipseインストールEclipse の上部メニューの Help &gt; Install New Software... クリック GoClipse Software Location設定 http://goclipse.github.io/releases/ をLocationに入力しOK GoClipseにチェックを入れ Next &gt;ボタンクリックでインストールを進めてください。 GoClipse を選択しインストール GoClipseにチェックを入れ Next &gt;ボタンクリックでインストールを進めてください。 Perspective に Go が表示されるようになります。 GoClipse 各パス設定 以上","link":"/2015/08/01/2015-08-02-golang-development-on-macosx/"},{"title":"Eclipse でGoコードデバッグツールgdb設定  on MacOSX","text":"概要goのデバックモジュール GDBをインストールします。ただMacのセキュリティ上の理由からGDBを利用するには証明書を作成する必要があります。 環境 MacOSX 10.10.4 Yosemite Go 1.4 Eclipse Mars Release (4.5.0) gdb 7.9 GDB インストール1$ brew install homebrew/dupes/gdb GDB バージョン確認123456789101112131415$ gdb --versionGNU gdb (GDB) 7.9Copyright (C) 2015 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type &quot;show copying&quot;and &quot;show warranty&quot; for details.This GDB was configured as &quot;x86_64-apple-darwin14.1.0&quot;.Type &quot;show configuration&quot; for configuration details.For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.Find the GDB manual and other documentation resources online at:&lt;http://www.gnu.org/software/gdb/documentation/&gt;.For help, type &quot;help&quot;.Type &quot;apropos word&quot; to search for commands related to &quot;word&quot;. 証明書 作成 gdb に gdb-cert の署名を適用12$ codesign -s gdb-cert /usr/local/Cellar/gdb/7.9/bin/gdb [master]gdb-cert: ambiguous (matches &quot;gdb-cert&quot; and &quot;gdb-cert&quot; in /Library/Keychains/System.keychain) taskgated プロセスをkill1$ sudo killall taskgated MacOSX 再起動後以下確認 以下のように C/C++ に GDB が表示される 以上","link":"/2015/08/01/2015-08-02-setting-for-gdb/"},{"title":"CentOS7 に Nginx + Go + Revel インストール・動作確認","text":"環境 EC2 t2.micro CentOS Linux release 7.1.1503 (Core) Go version go1.4.2 linux/amd64 以下手順です。 事前準備EC2 インスタンスへSSHログイン1$ ssh -i aws.pem centos@xxx.xxx.xxx.xx root権限へ変更1$ sudo su - yumパッケージ インストール12# yum update# yum install -y vim wget '*mercurial*' tree 最新のGitインストールGit が1.8以前の場合 go get が正しく動作しない事象が確認されている為、Gitを最新バージョンにします。 ※ 2015-08-03 時点 git version 2.5.0 https://kenzo0107.github.io/2016/02/22/2016-02-23-install-latest-git-on-centos/ Goインストールインストール123# cd /usr/local/src# wget https://golang.org/dl/go1.4.2.linux-amd64.tar.gz# tar -C /usr/local/ -xzf go1.4.2.linux-amd64.tar.gz Go用 WorkSpace 作成 このディレクトリでソースを管理します。 go get や go installした際はこのディレクトリに追加されます。 1# mkdir -p /var/golang rootユーザにてGo実行パス設定 /root/.bashrc に以下追記 12345export GOROOT=/usr/local/goexport GOBIN=$GOROOT/binexport GOPATH=/var/golangexport PATH=$PATH:$GOBIN 設定反映 1# source /root/.bashrc 反映されたか確認 12345# which go/usr/local/go/bin/go# go versiongo version go1.4.2 linux/amd64 上記のようにコマンドを実行し表示されれば問題ありません。 centosユーザにも同様にGo実行パス設定1# su - centos /home/centos/.bashrc も同様に追記 12345export GOROOT=/usr/local/goexport GOBIN=$GOROOT/binexport GOPATH=/var/golangexport PATH=$PATH:$GOBIN 設定反映 1$ source /home/centos/.bashrc 反映されたか確認 12345$ which go/usr/local/go/bin/go$ go versiongo version go1.4.2 linux/amd64 上記のようにコマンドを実行し表示されれば問題ありません。 Revel インストールRevelフレームワーク と Revelコマンド を go get でインストールします。 12$ go get github.com/revel/revel$ go get github.com/revel/cmd/revel Nginxインストール Nginx用リポジトリ作成 1# vim /etc/yum.repo.d/nginx.repo 以下追記 12345[nginx]name=nginx repobaseurl=http://nginx.org/packages/rhel/$releasever/$basearch/gpgcheck=0enabled=1 Nginx モジュールインストール1# yum install -y nginx 起動時設定サーバ起動時・再起動時にNginxが立ち上がるようにします。 12345678910111213141516171819202122# systemctl disable httpd# systemctl enable nginx# systemctl start nginx# systemctl status nginxnginx.service - nginx - high performance web server Loaded: loaded (/usr/lib/systemd/system/nginx.service; enabled) Active: active (running) since Mon 2015-08-03 06:07:44 UTC; 2s ago Docs: http://nginx.org/en/docs/ Process: 12642 ExecStart=/usr/sbin/nginx -c /etc/nginx/nginx.conf (code=exited, status=0/SUCCESS) Process: 12641 ExecStartPre=/usr/sbin/nginx -t -c /etc/nginx/nginx.conf (code=exited, status=0/SUCCESS) Main PID: 12645 (nginx) CGroup: /system.slice/nginx.service ├─12645 nginx: master process /usr/sbin/nginx -c /etc/nginx/nginx.... └─12646 nginx: worker processAug 03 06:07:44 ip-172-31-19-253 systemd[1]: Starting nginx - high performan....Aug 03 06:07:44 ip-172-31-19-253 nginx[12641]: nginx: the configuration file...kAug 03 06:07:44 ip-172-31-19-253 nginx[12641]: nginx: configuration file /et...lAug 03 06:07:44 ip-172-31-19-253 systemd[1]: Failed to read PID from file /r...tAug 03 06:07:44 ip-172-31-19-253 systemd[1]: Started nginx - high performanc....Hint: Some lines were ellipsized, use -l to show in full. Nginx 設定ファイル修正1# vim /etc/nginx/conf.d/default.conf 1234567server { listen 80; server_name ec2-xx-xx-xx-xxx.ap-northeast-1.compute.amazonaws.com; location / { proxy_pass http://127.0.0.1:9000; } Revel でアプリケーション作成・実行 「myapp」というプロジェクトを revelコマンドで作成 1# revel new myapp 実行 1# revel run myapp 実行結果 ※セキュリティグループでhttpでアクセスできるように設定してください。","link":"/2015/08/03/2015-08-04-install-nginx-go-revel-on-centos7/"},{"title":"CentOS7 に MySQL 5.6インストール","text":"参照: http://www.kakiro-web.com/linux/mysql-yum-repository-install.html MySQLのリポジトリ設定パッケージをダウンロード1$ wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm MySQLのリポジトリのインストール1234567# rpm -Uvh mysql-community-release-el7-5.noarch.rpm# MySQL 5.6 のリポジトリ利用# yum --enablerepo=mysql56-community list | grep mysql# MySQL 5.6 インストール# yum --enablerepo=mysql56-community install -y mysql-community-server rpm(Redhat Package Management)コマンドについて以下参照 http://itpro.nikkeibp.co.jp/article/COLUMN/20060227/230875/ MySQL 起動 / 登録12# systemctl start mysqld.service# systemctl enable mysqld.service DBアクセス123456789101112131415$ mysql -u rootWelcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 3Server version: 5.6.25 MySQL Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; 上記のようにアクセスできれば成功です。 以上","link":"/2015/08/04/2015-08-05-install-mysql5.6-on-centos7/"},{"title":"MySQL コマンドまとめ","text":"Dump 不要なテーブルは「–ignore-table=(テーブル名)」で排除1mysqldump -u &lt;user&gt; -p&lt;password&gt; dbname --ignore-table=dbname.table &gt; dump.sql DDL(Data Definition Language)取得1mysqldump -u &lt;user&gt; -p&lt;password&gt; --no-data dbname &gt; ddl.sql データ（INSERTクエリ）取得1mysqldump -u &lt;user&gt; -p&lt;password&gt; --no-create-info dbname &gt; data.sql DBインポート1mysql -u &lt;user&gt; -p&lt;password&gt; dbname &lt; data.sql ちなみにインポート時に以下のようなエラーが出た場合は、 1ERROR 2006 (HY000) at line ***: MySQL server has gone away 以下記事参照してください。[http://kenzo0107.hatenablog.com/entries/2015/12/17] DDLなし + CSV はきだし[f:id:kenzo0107:20160119111938p:plain] 123mkdir ./csvchmod o+x ./csvmysqldump -u &lt;user&gt; -p&lt;password&gt; --tab=./csv --fields-terminated-by=, --fields-optionally-enclosed-by=\\\" --lines-terminated-by=\"\\r\\n\" dbname SELECT文からCSVデータで出力 SELECT ~ INTO OUTFILE output.csv の場合、DBサーバに /tmp/hoge.csvは出力される。コマンド実行するサーバとDBサーバが異なる場合は注意が必要です。 12use dbnameSELECT * INTO OUTFILE'/tmp/hoge.csv' FIELDS TERMINATED BY ';' OPTIONALLY ENCLOSED BY '&quot;' ESCAPED BY '' LINES STARTING BY '' TERMINATED BY '\\r\\n' FROM table; コマンドラインSQLファイルをロードし実行結果をCSVに保存 1mysql -h &lt;host&gt; -u &lt;user&gt; -p&lt;password&gt; &lt;db_name&gt; -e \"`cat query.sql`\" | sed -e 's/\\t/,/g' &gt;/tmp/result.csv INPUT OUTFILEを利用せずにCSVファイル生成https://kenzo0107.github.io/2015/12/16/2015-12-17-mysql-error-2006-hy000-mysql-server-has-gone-away/ テーブル指定し出力1mysqldump -u &lt;user&gt; -p&lt;password&gt; -t dbname table1 tabl2 &gt; no_data.sql テーブル指定しWHERE句ありで出力1mysqldump -u &lt;user&gt; -p&lt;password&gt; -t dbname table1 \"-w created_at &lt; '2016-10-27' \" &gt; no_data.sql output by CSV format123mysql -u user -ppassword&gt; use dbname&gt; LOAD DATA INFILE \"&lt;CSVFile&gt;\" INTO TABLE table FIELDS TERMINATED BY ',' ENCLOSED BY '\"'; 全テーブルTRUNCATE1mysql -u root dbname -N -e 'show tables' | while read table; do mysql -u root -e \"truncate table $table\" dbname; done AUTO_INCREMENT値確認1SELECT auto_increment FROM information_schema.tables WHERE table_name = '&lt;table&gt;'; AUTO_INCREMENT値設定1ALTER TABLE &lt;table&gt; auto_increment=&lt;int val&gt;; テーブル名変更1ALTER TABLE &lt;old table&gt; rename &lt;new table&gt;; テーブルにカラム追加1ALTER TABLE &lt;table&gt; ADD &lt;column&gt; TINYINT(3) NOT NULL DEFAULT &lt;deafult value&gt; COMMENT '&lt;comment&gt;' AFTER &lt;previous column&gt;; 例) テーブル user の email カラムの次にカラム名: mailmagazine_status をtinyint(3) 符号なし(unsigned)、デフォルト0 の追加 1ALTER TABLE user ADD mailmagazine_status TINYINT(3) UNSIGNED NOT NULL DEFAULT 0 COMMENT 'メルマガステータス' AFTER email; テーブルのカラム削除1ALTER TABLE &lt;table&gt; DROP COLUMN &lt;column&gt;; テーブルのカラム編集1ALTER TABLE &lt;table&gt; CHANGE &lt;old column&gt; &lt;new column&gt; &lt;column 定義&gt;; 例) product テーブル の カラム名「no」を 「id」に、 unsigned(符号なし)、NULL禁止、デフォルト: 0、カラムコメント 「商品ID」に修正 1ALTER TABLE product CHANGE `no` `id` int(11) unsigned NOT NULL DEFAULT '0' COMMENT '商品ID'; インデックス一覧表示1SHOW INDEXES FROM &lt;table&gt;; インデックス追加123ALTER TABLE &lt;table&gt; ADD INDEX &lt;index name&gt;(&lt;column&gt;);ALTER TABLE &lt;table&gt; ADD INDEX &lt;index name&gt;(&lt;column1&gt;,&lt;column2&gt;); インデックス削除1ALTER TABLE &lt;table&gt; DROP INDEX &lt;index name&gt;; ユニーク追加1ALTER TABLE &lt;table&gt; ADD UNIQUE(&lt;column&gt;); ユニーク削除DBの文字コード確認1SHOW CREATE DATABASE dbname gz形式で圧縮状態のファイルを特定DBスキーマへ実行1zcat dump.sql.gz | mysql -u &lt;user&gt; -p&lt;password&gt; dbname 全テーブルの統計情報をサイズ順に一覧表示1SELECT table_name, engine, table_rows AS tbl_rows, avg_row_length AS rlen, floor((data_length + index_length) / 1024 / 1024) AS allMB, floor((data_length) / 1024 / 1024) AS dMB, floor((index_length) / 1024 / 1024) AS iMB FROM information_schema.tables WHERE table_schema = database() ORDER BY (data_length + index_length) DESC; テーブルの文字コード等確認1234567SELECT * FROM information_schema.schemata WHERE schema_name = 'database_name';+--------------+---------------+----------------------------+------------------------+----------+| CATALOG_NAME | SCHEMA_NAME | DEFAULT_CHARACTER_SET_NAME | DEFAULT_COLLATION_NAME | SQL_PATH |+--------------+---------------+----------------------------+------------------------+----------+| def | database_name | utf8 | utf8_general_ci | NULL |+--------------+---------------+----------------------------+------------------------+----------+ DB/Table 作成DB作成 (CHARACTER=utf8)1CREATE DATABASE `database_name` CHARACTER SET utf8; 指定DB/ホストに対してユーザ・パスワード・権限設定以下設定する必要あり database_name user_name host_name password 1GRANT ALL PRIVILEGES ON `database_name`.* TO `user_name`@'host_name' IDENTIFIED BY 'password' WITH GRANT OPTION; 指定ユーザにmysql スロークエリログ参照権限付与1GRANT select ON mysql.slow_log TO user_name; 指定ユーザにRDS(AWS)のmsyql スロークエリ参照権限付与1GRANT EXECUTE ON PROCEDURE mysql.rds_rotate_slow_log TO user_name; 指定ユーザの権限表示1SHOW GRANTS for 'user_name'@'%'; 設定反映上記権限設定した後に設定反映 1FLUSH PRIVILEGES; GRANT での権限付与の場合は特に FLUSH PRIVILEGES は不要ですが念の為。INSERT、UPDATE、DELETE 等で権限付与した場合は FLUSH PRIVILEGES が必要になります。 テーブル毎の容量確認[http://kenzo0107.hatenablog.com/entry/2016/06/27/121920:embed:cite] ちょうど1年前に1SELECT NOW() - INTERVAL 1 YEAR; 昨日のことのように1SELECT NOW() - INTERVAL 1 DAY; 1日前の00:00:001SELECT CURDATE() - INTERVAL 1 DAY; 1日前の11:00:001SELECT DATE_FORMAT(CURDATE() - INTERVAL 1 DAY, '%Y-%m-%d 11:00:00'); MySQL バージョン確認1234567mysql -u &lt;user&gt; -p&lt;pass&gt; -e\"SELECT version();\"+------------+| version() |+------------+| 5.5.42-log |+------------+ 各種メトリクスSELECT / INSERT / UPDATE / DELETE / REPLACE コマンドの実行回数取得1mysql -u root -NBe \"SHOW GLOBAL STATUS\" | grep \"Com_\" | grep -E \"select|insert|update|delete|replace\" [f:id:kenzo0107:20160927140251p:plain] Item Explain Com_delete 削除 (DELETE) 実行回数 Com_delete_multi 複数行 (DELETE) 実行回数 Com_insert 登録 (INSERT) 実行回数 Com_insert_select コピー作成 (INSERT SELECT) 実行回数 Com_replace 再作成 (REPLACE) 実行回数 Com_replace_select 再作成 (REPLACE SELECT) 実行回数 Com_select 選択 (SELECT) 実行回数 Com_update 更新 (UPDATE) 実行回数 Com_update_multi 複数行更新 (UPDATE) 実行回数 接続中 のコネクション数取得1mysql -u root -BNe \"SHOW STATUS LIKE 'Threads_connected';\"","link":"/2015/08/04/2015-08-05-mysql-command-list/"},{"title":"Safariで証明書情報確認","text":"鍵マークをクリック 「証明書を表示」ボタンクリック 「証明書情報」","link":"/2015/08/04/2015-08-05-show-infomation-about-certificate-on-safari/"},{"title":"NginxにオレオレSSL証明書インストール","text":"環境 AWS EC2 : t2.micro OS : CentOS Linux release 7.1.1503 (Core) Nginx: 1.8.0 OpenSSL: 1.0.1e-fips 11 Feb 2013 前提 Nginxがインストール済みである。 事前にパスワード作成12$ cat /dev/urandom | LC_CTYPE=C tr -dc '[:alnum:]' | head -c 40v6biM9MMByBO0SWFitcbnyF0VUsJLbZsizpP7K15 40文字のランダムな半角英数字が生成されます。 証明書作成時に必要となるパスワードです。絶対忘れないようにしてください。 以下ec2インスタンスのパブリック DNSを ec2-xx-xx-xx-xx.ap-northeast-1.compute.amazonaws.com として話を進めます。 KEY ファイル作成sha256(略してsha2)で作成します。 1234# mkdir -p /etc/nginx/conf# cd /etc/nginx/conf# openssl genrsa -des3 -out server.key 2048 -sha256Enter pass phrase for server.key: v6biM9MMByBO0SWFitcbnyF0VUsJLbZsizpP7K15 CSR ファイル作成12345678910111213141516171819202122# openssl req -new -sha256 -key server.key -out server.csrEnter pass phrase for server.key: v6biM9MMByBO0SWFitcbnyF0VUsJLbZsizpP7K15You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [XX]:JPState or Province Name (full name) []:TokyoLocality Name (eg, city) [Default City]:Setagaya-kuOrganization Name (eg, company) [Default Company Ltd]:UmiyamaShouji inc.Organizational Unit Name (eg, section) []:ProductionCommon Name (eg, your name or your server's hostname) []:ec2-xx-xx-xx-xx.ap-northeast-1.compute.amazonaws.comEmail Address []: (空白のままEnter)Please enter the following 'extra' attributesto be sent with your certificate requestA challenge password []: (空白のままEnter)An optional company name []: (空白のままEnter) 確認123456$ ls -altotal 8drwxr-xr-x. 2 root root 40 Aug 5 13:43 .drwxr-xr-x. 3 root root 17 Aug 5 13:32 ..-rw-r--r--. 1 root root 729 Aug 5 13:43 server.csr-rw-r--r--. 1 root root 963 Aug 5 13:37 server.key RSA key作成12# openssl rsa -in server.key -out server.keyEnter pass phrase for server.key: v6biM9MMByBO0SWFitcbnyF0VUsJLbZsizpP7K15 CRT作成12345# openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crtSignature oksubject=/C=JP/ST=Tokyo/L=Setagaya-ku/O=UmiyamaShouji inc./OU=Production/CN=ec2-xx-xx-xx-xx.ap-northeast-1.compute.amazonaws.comGetting Private key ssl.conf作成example_ssl.conf をコピーし ssl.conf 作成12# cd /etc/nginx/conf.d# cp example_ssl.conf ssl.conf ssl.conf編集1# vi ssl.conf 123456789101112131415161718192021# HTTPS server#server { listen 443 ssl; server_name ec2-xx-xx-xx-xx.ap-northeast-1.compute.amazonaws.com; ssl on; ssl_certificate conf/server.crt; ssl_certificate_key conf/server.key;# 以下随時設定# ssl_session_cache shared:SSL:1m;# ssl_session_timeout 5m;# ssl_ciphers HIGH:!aNULL:!MD5;# ssl_prefer_server_ciphers on; location / { root /usr/share/nginx/html; index index.html index.htm; }} Nginx再起動1# systemctl restart nginx もしこんなエラーが出たら1nginx[2246]: nginx: [emerg] SSL_CTX_use_PrivateKey_file(&quot;/etc/nginx/conf/server.key&quot;) failed (SSL: error:0906406D:PEM routines:PEM_def_callback:problems getting password error:0906A068:PEM routines:PEM_do_header:bad password read error:140B0009:SSL routines:SSL_CTX_use_PrivateKey_file:PEM lib) server.keyのパスフレーズにより読み込みができないというエラーです。バックアップを作成してパスフレーズを解除してください。完了したら再起動して確認してください。 123$ sudo cp server.key server.key.bk$ sudo openssl rsa -in server.key -out server.key$ systemctl restart nginx 動作確認 Safari等で https://ec2-xx-xx-xx-xx.ap-northeast-1.compute.amazonaws.com へアクセス 作成した証明書情報確認 以下参考までに[http://kenzo0107.hatenablog.com/entry/2015/08/05/144733:embed:cite]","link":"/2015/08/04/2015-08-05-use-my-certificate-on-nginx/"},{"title":"ElasticSearchインストール on CentOS7","text":"環境 CentOS Linux release 7.0.1406 (Core) ElasticSearch 1.7.1 必要モジュールダウンロード1$ yum install -y curl-devel java-1.8.0-openjdk java バージョン確認123# java version \"1.7.0_85\"OpenJDK Runtime Environment (rhel-2.6.1.2.el7_1-x86_64 u85-b01)OpenJDK 64-Bit Server VM (build 24.85-b03, mixed mode) public signing keyをダウンロード・インストール1$ rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch yum repositoryに elasticsearch追加/etc/yum.repos.d/elasticsearch.repo 作成1$ vi /etc/yum.repos.d/elasticsearch.repo 以下追記123456[elasticsearch-1.7]name=Elasticsearch repository for 1.7.x packagesbaseurl=http://packages.elastic.co/elasticsearch/1.7/centosgpgcheck=1gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearchenabled=1 インストール1$ yum install -y elasticsearch config設定設定ファイル作成12$ cp /etc/elasticsearch/elasticsearch.yml /usr/share/elasticsearch/config$ vim /usr/share/elasticsearch/config/elasticsearch.yml 9200 port設定 12- #http.port: 9200+ http.port: 9200 kuromojiプラグインをインストール形態素解析器を使って日本語検索が可能になる。 ElasticSearchのバージョンによって elasticsearch-analysis-kuromojiのバージョンを変更する必要があります。以下参考にお願いします。 https://github.com/elastic/elasticsearch-analysis-kuromoji ここかなりハマりました汗 1$ /usr/share/elasticsearch/bin/plugin --install elasticsearch/elasticsearch-analysis-kuromoji/2.7.0 elasticsearch サーバ起動時、自動起動登録123$ systemctl start elasticsearch.service$ systemctl daemon-reload$ systemctl enable elasticsearch.service 実行確認1$ curl http://localhost:9200 実行結果 12345678910111213{ &quot;status&quot; : 200, &quot;name&quot; : &quot;Angelica Jones&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;version&quot; : { &quot;number&quot; : &quot;1.7.1&quot;, &quot;build_hash&quot; : &quot;b88f43fc40b0bcd7f173a1f9ee2e97816de80b19&quot;, &quot;build_timestamp&quot; : &quot;2015-07-29T09:54:16Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;4.10.4&quot; }, &quot;tagline&quot; : &quot;You Know, for Search&quot;} 以上","link":"/2015/08/05/2015-08-06-install-elasticsearch-on-centos7/"},{"title":"ある日 Nginx で HTTP&#x2F;2 and SPDY indicator が青く輝いてくれなくなった件","text":"概要Chrome Extention の HTTP/2 and SPDY indicator はHTTP/2 で通信していることを確認する Chrome の拡張 Pluginです。 これがある日 not enable となっていたのでその対応をまとめます。 結果的に何が原因だったか ALPN をサポートしていなかった為です。 ALPN Wiki 以下 HTTP/2 通信テストサービスでたどり着きました。 HTTP/2 Test - Verify HTTP/2 Support | KeyCDN ToolsVerify if a URL is delivered through the HTTP/2 network protocol. では何を対応するかALPN を サポートしている openssl を 1.0.2 系にしNginx を openssl 1.0.2 以上 で rebuild します。 ※ 2016/08/10 現時点の最新 1.0.2h OpenSSL 1.0.2h インストール12345678# cd /usr/local/src# wget https://www.openssl.org/source/openssl-1.0.2h.tar.gz# tar xvf openssl-1.0.2h.tar.gz# cd openssl-1.0.2h# ./config# make# make test# make install 現状の OpenSSL と置き換え12345678# which openssl/usr/bin/openssl# mv /usr/bin/openssl /root/# ln -s /usr/local/ssl/bin/openssl /usr/bin/openssl# openssl versionOpenSSL 1.0.2h 3 May 2016 Nginx バージョン確認12# nginx -vnginx version: nginx/1.11.3 現状の configure 状況確認1234567# nginx -Vnginx version: nginx/1.11.3built by gcc 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC)built with OpenSSL 1.0.2h 3 May 2016TLS SNI support enabledconfigure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_random_index_module --with-http_secure_link_module --with-http_stub_status_module --with-http_auth_request_module --with-http_xslt_module=dynamic --with-http_image_filter_module=dynamic --with-http_geoip_module=dynamic --with-http_perl_module=dynamic --add-dynamic-module=njs-1c50334fbea6/nginx --with-threads --with-stream --with-stream_ssl_module --with-stream_geoip_module=dynamic --with-http_slice_module --with-mail --with-mail_ssl_module --with-file-aio --with-ipv6 --with-http_v2_module --with-cc-opt='-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic' Configure パラメータに --add-dynamic-module=njs-1c50334fbea6/nginx のような njs が含まれている場合は削除します。含まれている場合、 configure がこけます。 (前回記事参照) Nginx 1.9.6 → 1.11.1 へバージョンアップ 脆弱性対応脆弱性CVE-2016-4450 に対応した Nginx 1.11.1 が 2016-05-31 リリースされたということで早速バージョンアップを試みました。 SIOS Tech. Lab - エンジニアのためになる技術トピックス ダウンタイムゼロで実行できました。 現状の Ng… 明示的に openssl を指定する為、以下パラメータを追加 1--with-openssl=/usr/local/src/openssl-1.0.2h 現状の Nginx バージョンを変更しない前提で 1.11.3 の ソースをダウンロード123# cd /usr/local/src# wget http://nginx.org/download/nginx-1.11.3.tar.gz# tar xvf nginx-1.11.3.tar.gz Nginx リビルド123456# cd /usr/local/src/nginx-1.11.3# ./configure --prefix=/etc/nginx --with-openssl=/usr/local/src/openssl-1.0.2h --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_random_index_module --with-http_secure_link_module --with-http_stub_status_module --with-http_auth_request_module --with-http_xslt_module=dynamic --with-http_image_filter_module=dynamic --with-http_geoip_module=dynamic --with-http_perl_module=dynamic --with-threads --with-stream --with-stream_ssl_module --with-stream_geoip_module=dynamic --with-http_slice_module --with-mail --with-mail_ssl_module --with-file-aio --with-ipv6 --with-http_v2_module --with-cc-opt='-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic'# make# make test# make install version 確認 built with OpenSSL が 1.0.2h となっていることが確認できます。 123456# nginx -Vnginx version: nginx/1.11.3built by gcc 4.8.5 20150623 (Red Hat 4.8.5-4) (GCC)built with OpenSSL 1.0.2h 3 May 2016TLS SNI support enabledconfigure arguments: --prefix=/etc/nginx --with-openssl=/usr/local/src/openssl-1.0.2h --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-http_ssl_module --with-http_realip_module --with-http_addition_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_random_index_module --with-http_secure_link_module --with-http_stub_status_module --with-http_auth_request_module --with-http_xslt_module=dynamic --with-http_image_filter_module=dynamic --with-http_geoip_module=dynamic --with-http_perl_module=dynamic --with-threads --with-stream --with-stream_ssl_module --with-stream_geoip_module=dynamic --with-http_slice_module --with-mail --with-mail_ssl_module --with-file-aio --with-ipv6 --with-http_v2_module --with-cc-opt='-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic' Nginx シンタックスチェック123# nginx -tnginx: the configuration file /etc/nginx/nginx.conf syntax is oknginx: configuration file /etc/nginx/nginx.conf test is successful Nginx 再起動1# systemctl restart nginx HTTP/2 and SPDY indicator 確認青く輝きました！ HTTP/2 テストでも ALPN がサポートされていることが確認できました。","link":"/2016/08/09/2016-08-10-nginx-http2/"},{"title":"jenkins が停止していますが PID ファイルが残っています","text":"プロセスIDの管理ファイルは通常停止すれば削除されます。但し、Jenkins 再起動時に サーバそのものを再起動させたとか、野暮なことをすると Jenkins おじさんが怒ります。 対策 1PIDが素直に残っているのであればJenkins PIDファイルを削除 1$ sudo rm /var/run/jenkins.pid 対策2PIDが存在しない、という場合には以下ディレクトリにアクセス権限がないパターン /var/log/jenkins /var/cache/jenkins chown / chmod で所有者・パーミッション変更してアクセス可能な状態にしましょう。","link":"/2016/08/14/2016-08-15-jenkins-stopped-due-to-pid-file/"},{"title":"du でデバイスの使用率が残り少ない、何か消さなきゃいけないというときに","text":"あっ！使い切ってる… という事象が発生しました。あるモジュールを導入しようとした際に一気に使い切ってしまった。。 123456ファイルシス サイズ 使用 残り 使用% マウント位置/dev/xvda1 8.0G 8.0G 0M 100% /devtmpfs 3.9G 0 3.9G 0% /devtmpfs 3.7G 0 3.7G 0% /dev/shmtmpfs 3.7G 17M 3.7G 1% /runtmpfs 3.7G 0 3.7G 0% /sys/fs/cgroup ディレクトリ内のファイル容量チェック 対象デバイスに移動し、ファイル容量チェック そこでファイルのサイズが大きい箇所をチェック 上記 1,2 を繰り返していくと原因が特定がしやすい。 12345678910111213141516171819202122232425対象デバイスに移動し、ファイル容量チェック$ cd /$ sudo du -sh * | sort -nr385M root267M opt117M home79M boot22M etc17M run3.1G usr2.7M tmp4.3G var ← /var がサイズが大きい0 sys0 srv0 sbin0 proc0 mnt0 media0 lib640 lib0 dev0 bin$ sudo find /var -size +100M -exec ls -lh {} \\; -name ‘*.tar.gz’ など解凍後不要になったアーカイブファイルを検索するもよし。 以前の記事もご参照ください。yum cache は知らず知らずのうちに大容量になっていることもあります。 意外と容量食ってた yum cacheyum cache 容量12# du -sh &amp;#x2F;var&amp;#x2F;cache&amp;#x2F;yum155M &amp;#x2F;var&amp;#x2F;cache&amp;#x2F;yum 155MByteある汗 yum cache 削除12345# yum clean all読み込んだ… 諸々削除して30 % くらいは削れた汗 監視をしっかりせねば","link":"/2016/08/14/2016-08-15-reduce-disk-usage/"},{"title":"Jenkins 死亡時の対策","text":"スレッドが死亡するとこんな表示に… 原因は 1No space left on device デバイスの容量不足でした。。 対策以下 cleanBuild.sh を作成しビルド掃除をするようにしました。 過去ビルド履歴が消去されてしまいますが低コスト運用の為、背に腹は変えられない！ ビルド履歴はむしろ、Slackに通知してあげてるのでSlackで担保されている、ということにしよう。","link":"/2016/08/14/2016-08-15-why-jenkins-died/"},{"title":"Digest 認証設定","text":"Basic 認証 と Digest 認証の違いBasic 認証 user と password を平文でサーバに送っている。 通信内容を傍受されると user / password がわかってしまう。 Digest 認証 user / password を MD5 で暗号化して通信してから送っている。 以上から Digest 認証の方がセキュリティ面で良いです。 Digest 認証設定方法 (Apache) Digest 認証ファイル設定 1$ htdigest -c &quot;/var/www/.htpasswd&quot; &quot;Digest_Auth&quot; &lt;user_name&gt; /etc/httpd/conf.d/vhost.conf 12345678910111213141516171819&lt;VirtualHost *:80&gt; ServerName jugem.jugem.jp DocumentRoot /var/www/html/jugem ErrorLog logs/error.log TransferLog logs/access.log &lt;Directory &quot;/var/www/html/jugem&quot;&gt; Options Indexes FollowSymLinks Includes ExecCGI AllowOverride All Order allow,deny Allow from all AuthType Digest AuthName &quot;Digest_Auth&quot; AuthDigestProvider file AuthUserFile /var/www/.htpasswd # ここで先ほど作成した Digest認証ファイルを指定 AuthGroupFile /dev/null Require valid-user &lt;/Directory&gt;&lt;/VirtualHost&gt; シンタックスチェックし問題なければ再起動1234# httpd -tSyntax OK# service httpd graceful 確認実際に 指定した ServerName にアクセスしてDigest 認証 が設定されているか確認してください。 以上です。","link":"/2016/08/15/2016-08-16-digest-authentication/"},{"title":"5分でできる♪ AWS Lambda で EC2 Event を Slack 通知","text":"以前 AWS EC2 メンテ通知のイベントチェックスクリプトを作成しました。合わせて、対象インスタンスを停止・起動する様にしました。 AWS [Retirement Notification] 対応概要とある日、AWS よりこんなメール通知が来ました。 要約するとホストしている基盤のハードウェアで回復不可能な障害が検知されたので指定期限までに対応しないとインスタンスが停止する、とのこと。 今回こちらの対応をまとめました。 12345678910111213141516171… これを AWS Lamda で Slack 通知させる様にし毎朝メンテの必要なイベントがわかる様にしました。 事前準備12macOS%$ pip install lambda-uploader awsclimacOS%$ aws configure --profile &lt;profile&gt; プロジェクト clone12345678macOS%$ git clone https://github.com/kenzo0107/AWSEC2Events2SlackmacOS%$ tree AWSEC2Events2Slack.├── README.md├── event.json├── lambda.json├── lambda_function.py└── requirements.txt 各種環境に合わせて情報編集 lambda.json 12345678910111213141516{ &quot;name&quot;: &quot;AWSEvent2Slack&quot;, &quot;description&quot;: &quot;Notificate AWS events to Slack&quot;, &quot;region&quot;: &quot;ap-northeast-1&quot;, &quot;handler&quot;: &quot;lambda_function.lambda_handler&quot;, &quot;role&quot;: &quot;arn:aws:iam::xxxxxxxxxxxx:role/lambda-check-events-to-slack&quot;, &quot;timeout&quot;: 60, &quot;memory&quot;: 128, &quot;variables&quot;: { &quot;SLACK_INCOMING_WEBHOOK&quot;:&quot;https://hooks.slack.com/services/XXXXXXXXX/XXXXXXXXX/XXXXXXXXXXXXXXXXXXXXXXXX&quot;, &quot;SLACK_CHANNEL&quot;:&quot;#channel&quot;, &quot;SLACK_USERNAME&quot;:&quot;AWSEvent2Slack&quot;, &quot;SLACK_ICON_URL&quot;:&quot;http://i.imgur.com/6RCTdfi.png&quot; }} Item Explain role EC2リソースをdescribeする権限を所持したポリシーをアタッチ variables 通知先Slack情報 AWS Lambda へソースアップロード12345macOS%$ lambda-uploader --profile &lt;profile&gt;Î» Building PackageÎ» Uploading PackageÎ» Fin AWSコンソールより Lambda 確認登録されていることがわかります。 テスト実行イベントを取得しSlackに通知させる様にすることができました。 トリガー設定CloudWatch スケジュール式で cron 設定し 毎朝届ける様に指定しました。 総評lambda-uploader でのアップロードによりローカルで開発→テスト→デプロイとバージョン管理が明確になって良いです。 但し、一点気になる点はアップロード後、ソースがコンソール上で見えません。 具体的には Lambda 関数 「AWSEvent2Slack」のデプロイパッケージが大きすぎて、インラインコード編集を有効にできません。ただし、関数を今すぐ呼び出すことはできます。 とコンソール上に表示されます。 前まで zip にまとめてアップロードするシェルを書いていたけどその時はソースは見ることができました。 ローカルで挙動確認しておりコンソール上では見えなくても今のところ支障なしです。 以上参考になれば何よりです。","link":"/2017/07/17/2017-07-18-notify-to-slack-about-ec2-events-by-lambda/"},{"title":"Hubot で Git の Pull Request や Issue のコメントのメンション相手に Slack DM で通知","text":"概要Git での Pull Request や Issue コメントのメンションがメール通知で気づけず困った！という声を多く聞き、メンション相手に Slack DM を通知する様な仕組みを作りました。 システム概要今回は AWS 上に構築しました。 Git は GHE on EC2 github.com の場合だと、IP 定まらない問題があるかと思うので、動的に IP を取得して解放させる様な仕組みを入れる必要がありそう。 hubot は t2.nano と最小 当初、IBM Bluemix で構築してみましたが、サポートから IP 制限はまだできていない、とのことなので on AWS にしました。 GHE からの hubot の受け口は ELB で EIP のみ許可させてます。 今後、受け口を色々作る目的で ELB 立てました。 元々は JIRA のメンションを Slack DM に送るだけの目的だったので 同一 Private Subnet に置いてました。 スクリプト getSlackUsernameByGitUsername 基本 git name と slack name は命名規則が統一されていたので正規表現で変換させる様に解決しています。 git name: kenzo-tanaka だったら slack name: kenzo.tanaka に変換 命名規則に即していないユーザは以下の users リストに変換を任せます。 kimika.himura は DM 送られたくないと言う人を想定してます。 依存ライブラリ “hubot-slack”: “^4.4.0” “hubot-slack-attachement”: “^1.0.1” 1234users = { &quot;kenzo-tanaka&quot;: &quot;kenzo0107&quot;, &quot;kimika.himura&quot;: &quot;no_send&quot;} ソース全容は以下になります。 Git 設定設定したい Organization or Owner &gt; Settings &gt; Hooks で hubot への URL を設定します。((Organization 跨いで一気に全部のリポジトリにHookかけるのは別途スクリプト組むなりしないと難しそう。GitHub社も Organization は 1つとすることを推奨とのことなので今回はこれで！)) その他設定 Content type: application/json Let me select individual events: Issues Issue comment Pull request Pull request review Pull request review comment ※ よりセキュアにする際には Secret 設定してください。 通知が来た！早速 Pull Request でメンションしてみたら通知が来ました！絵文字もしっかり！URL も自動でリンクされている！ 以上、参考になれば幸いです♪","link":"/2017/11/22/2017-11-23-slack-dm-pullrequest-or-issue-comment-by-Hubot/"},{"title":"AWS S3 Bucket の特定フォルダ以下を社外に共有する際のポイント","text":"ToC 概要 共有方法案 案1. A 社の所有するストレージのアップロード権限をいただく 案2. ROM に入れてお渡し 案3. A 社から弊社 S3 Bucket へのアクセスを許可する 結論: 共有するオブジェクトが既知かどうかで付与する権限が変わる。 A社と共有するファイルが既知である場合 A社と共有するファイルが不特定の場合 上記実装に至った経緯 まとめ 概要AWS S3 Bucket の特定パスを社外に共有する際のポイントをまとめました。 共有方法案共有方法案として以下 3 案を考えました。 案1. A 社の所有するストレージのアップロード権限をいただくこちらの S3 Bucket hoge.share へのアクセス許可設定を追加することなく、弊社からオブジェクトをアップロードする。 メリット こちらの S3 Bucket へのアクセス許可設定を追加することなくセキュア。 共有したくないオブジェクトをこちら都合で精査できる。 デメリット オブジェクトアップロードする作業コストが発生。 オブジェクト数が多いと時間が掛かる。 A社のストレージが S3 なら Bucket コピーが使えるが、そうでない場合、一度ダウンロードする作業コストが掛かる。 案2. ROM に入れてお渡し一度ダウンロードして CD-ROM に入れて A 社にお渡し。 （監査で ROM に入れてお渡し、と指示されることが過去ありました） メリット 案1 と近しいが、インターネットを経由しないという点でよりセキュア デメリット オブジェクト数が多いと時間が掛かる 共有頻度が高くなると発送手続きが多くなり、その作業コストが掛かる。 案3. A 社から弊社 S3 Bucket へのアクセスを許可する メリット 弊社作業コストが少なく済む。こちらはアクセス権限を付与するのみ デメリット A 社の作業工数が高くなる。 案1 がセキュアで良さそうです。 自分が実際のタスクで対応したのは 案3 でした。その際は、期日が非常に短く、こちらの作業コストがかけられないというビジネス的な事情からでした。 案3について、検討したことを結論からまとめていきます。 結論: 共有するオブジェクトが既知かどうかで付与する権限が変わる。ユースケースとして、弊社の S3 Bucket hoge.share の特定パス aaa/bbb 以下のオブジェクトを A 社に共有するとします。 A社と共有するファイルが既知である場合A 社と共有するファイルが毎回 hoge.share の aaa/bbb/c.gz と取り決めがある場合、以下の様なポリシーで A 社の ip 制限ができます。 123456789101112131415161718{ \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Principal\": { \"AWS\": [\"*\"] }, \"Action\": [\"s3:GetObject\"], \"Resource\": [\"arn:aws:s3:::hoge.share/aaa/bbb/c.gz\"], \"Condition\": { \"IpAddress\": { \"aws:SourceIp\": [\"&lt;A社IP&gt;/32\"] } } } ]} A社IP から以下コマンドでダウンロードが可能です。 1curl https://s3-ap-northeast-1.amazonaws.com/hoge.share/aaa/bbb/c.gz --output c.gz A社と共有するファイルが不特定の場合 A社用の IAM User (無権限)で作成 S3 Bucket ポリシーで A社IP で制限し、 A社用 IAM User に対し s3:GetObject s3:ListBucket を許可 上記実装に至った経緯aaa/bbb/ 以下のオブジェクト全てを共有する、という様な場合に以下の様な curl でワイルドカードを指定して一括ダウンロードはできません。 1curl https://s3-ap-northeast-1.amazonaws.com/hoge.share/aaa/bbb/* 一括ダウンロードする際には s3:ListBucket 権限が必要で、それをサポートする方法が IP 許可だけでは実現できませんでした。 例) 一括ダウンロードするコマンド 1aws s3 cp --recusive s3://hoge.share/aaa/bbb/ . A社用の IAM User の作成時には無権限とし、 S3 Bucket Policy で明示的な A社用 IAM User のアクセス許可を定義しています。 12345678910111213141516171819202122232425262728293031323334{ \"Version\":\"2012-10-17\", \"Statement\":[ { \"Effect\":\"Allow\", \"Principal\": { \"AWS\": [\"&lt;A 社用 IAM User ARN&gt;\"] }, \"Action\": [\"s3:GetObject\"], \"Resource\": [\"arn:aws:s3:::hoge.share/aaa/bbb/*\"], \"Condition\": { \"IpAddress\": { \"aws:SourceIp\": [\"&lt;A社IP&gt;/32\"] } } }, { \"Effect\":\"Allow\", \"Principal\": { \"AWS\": [\"&lt;A 社用 IAM User ARN&gt;\"] }, \"Action\": [\"s3:ListBucket\"], \"Resource\": [\"arn:aws:s3:::hoge.share\"], \"Condition\": { \"IpAddress\": { \"aws:SourceIp\": [\"&lt;A社IP&gt;/32\"] }, \"StringLike\": { \"s3:prefix\": [\"aaa/bbb/*\"] } } } ]} また、考慮すべき点として、hoge.share へのアクセスログを別途担保する必要があります。 それも含め、対応をまとめたものを Terraform でまとめました。 まとめS3 Bucket の特定フォルダ以下を共有する方法を改めて検討してみていろいろな方法があるなと思いました。 共有先との関係性によってもメリット・デメリットがあろうかと思います。 こんな良い方法があるよ！という連絡お待ちしています m(_ _)m","link":"/2020/08/12/2020-08-13-external-s3-bucket/"},{"title":"aws-cli KMS で暗号化・復号","text":"ToC 暗号化 復号 備忘録です。 暗号化aws cli ver.1 1aws kms encrypt --key-id alias/&lt;kms鍵&gt; --plaintext &quot;&lt;暗号化したい文字列&gt;&quot; --output text --query CiphertextBlob aws cli ver.2 1aws kms encrypt --key-id alias/&lt;kms鍵&gt; --plaintext &quot;$(echo -n '&lt;暗号化したい文字列&gt;' | base64)&quot; --output text --query CiphertextBlob 復号1aws kms decrypt --ciphertext-blob fileb://&lt;(echo '文字列'|base64 -d) | jq .Plaintext --raw-output |base64 -d","link":"/2020/08/13/2020-08-14-aws-kms-encrypt-decrypt/"},{"title":"You are not subscribed to this service","text":"ToC 概要 結論 概要「AWS アカウント開設完了しました！」と言われ、terraform 流してみたら以下の様なエラーが発生。その時対処した内容を備忘録としてまとめます。 1Error: Error fetching Availability Zones: OptInRequired: You are not subscribed to this service. Please go to http://aws.amazon.com to subscribe. 結論AWS のサインアップが完了していなかったことが原因でした。 メニューから マイ Service Quotas にアクセスし、本人確認をしたら解決できました。","link":"/2020/10/05/2020-10-06-aws-startup/"},{"title":"SendGrid メール送信できるまで &amp; なりすまし対策","text":"ToC 概要 メール送信設定手続き SubUser 作成 Username Email I.P. ADDRESSES Domain Authentication 設定 Domain Authenticationとは？ 設定手順 API Key 作成 Rails で SendGrid を利用しメール送信する設定 なりすまし対策 DMARC 設定 集計先メールアドレスを外部委託する場合 p タグの扱い注意 SPF レコード登録 SPF の仕組み SendGrid での SPF レコード設定 指針 スパムメール扱いされていないかテストする方法 まとめ 概要SendGrid ダッシュボードで設定する手続きを運用上のフィードバックを入れつつまとめました。 メール送信設定手続き サブユーザは、メール送信やAPIの処理を分けて管理ができる。 Subuser Stats によって、Subuser 毎の集計が SendGrid ダッシュボードで確認できる。 サブユーザの最大作成数は15で、それ以上作成したい場合はサポート問い合わせ必要。 基本以下設定し、「Create Subuser」ボタンをクリックする。（他は設定しなくても良い。） Username 環境毎(stg, prd 等)に作成すると Subuser 毎に SendGrid ダッシュボードの Stats の確認ができるなどのメリットがあり、管理しやすいので以下例の様に指定しています。 例: &lt;env&gt;-&lt;service name&gt; stg-hogehoge prd-hogehoge EmailGmail を利用している場合は、メールアドレスのエイリアスを設定すると送信先が1つにまとまり管理しやすいです。 例: sample+&lt;subuser name&gt;@&lt;your domain&gt; I.P. ADDRESSESチェックを入れる。 10 以上の subuser 間で共通の IP を指定して運用していますが、現状特段問題はない。 各独自ドメイン毎に IP を指定したい所。追加時は 3,700円/月/1ドメイン。到達率がどこまで上がるかは未検証です。 コスト的に問題なければ、ドメイン毎に IP 指定しておくのが良さそう。 参考: 固定IPアドレスを利用するメリットは何でしょうか？ Domain Authenticationとは？ SendGridがユーザの許可を得てメール送信していることを証明する機能。 設定しておかないと「なりすまし」として扱われ、迷惑メールに振り分けられる可能性が高くなる。 設定手順使用している DNS を選択します。 設定項目 From Domain に メールのドメインパート（送信者のメールアドレスの@以降）を指定する。 Advanced Settings &gt; Use automated security にチェック（デフォルトでチェック） Use automated security … SPF/DKIMに関するレコードの制御をSendGridに任せるかどうかを指定する機能です。 Assign to a subuser stg, prd で共通のドメインパートを指定する場合は、チェックを外す。 stg, prd 各 subuser で Domain Authentication を指定する必要がある。 stg, prd で異なるドメインパートを指定する場合は subuser 紐付け。 発行された DNS レコードを DNS に登録する。 AWS なら Route53 にレコードを登録します。 「Verify」をボタンをクリック レコード登録しただけでは自動で認証せず、「Verify」ボタンをクリックする必要があるので注意！ API Key 作成先ほど作成した Subuser に切り替えます。 「Create API Key」クリック どの権限を持った API Key を発行するか決定します。メールを送信するだけであれば、基本的に Restricted Access で Mail Send の Full Access のみあれば良い。 「Create &amp; View」ボタンクリックで API Key が発行されます。 Rails で SendGrid を利用しメール送信する設定gem sendgrid-actionmailer をインストールして利用しています。 Gemfile development, test では利用しない為、 staging, production のみにしています。こちらは適宜変更してください。 123group :staging, :production do gem 'sendgrid-actionmailer'end production.credentials.enc, staging.credentials.enc prd, stg 用にそれぞれ別に発行した API Key を credentials に設定します。 sconfig/environments/production.rb 1234config.action_mailer.delivery_method = :sendgrid_actionmailerconfig.action_mailer.sendgrid_actionmailer_settings = { api_key: Rails.application.credentials[:sendgrid_api_key]} 以上でメール送信ができる状態にはなりました。 以降は送信者のドメイン認証によるメールが届かない問題への対応になります。 なりすまし対策DMARC (Domain-based Message Authentication, Reporting, and Conformance) は送信ドメイン認証技術の1つで、SPF, DKIM 等の送信者ドメイン認証に失敗したメールの振る舞いを送信者が指定できるものです。 主に以下3つの役割があります。 メール送信者が認証失敗したメールの取り扱いを指定できる 認証失敗したら迷惑メールボックスに入れる or そもそも受信拒否させることができる。 メール送信者が認証結果をメール受信者から受け取れる なりすましたメール送信者を把握できる。 第三者署名（代理署名）を許容しない なりすましと判断されたメールを受信できない様にする。 つまり、DMARC 設定していないと？、第三者がなりすましたメールをユーザは疑いなく受信できてしまい、また、送信者はそのことを知る術がありません。 DMARC 設定として、以下レコードを登録します。 1_dmarc.example.com CNAME v=DMARC1;p=quarantine;rua=mailto:dmarc.rua@example.com Item Value Explain v=DMARC1 プロトコルバージョン バージョン 1 のみ存在する為、 DMARC1 を指定する。 p=quarantine DMARCレコードのポリシー p=quarantine 指定で迷惑メールに振り分ける。 pct=100 DMARCを適用する割合 pct=100 or pct タグを削除しないとなりすまし攻撃に晒される危険性がある為、pct タグを削除しとく。 rua=mailto:… 集計レポートの報告先URI 集計レポートの通知先。事前に通知先をアプリケーション担当者に確認しておく。 集計先メールアドレスを外部委託する場合DMARC レコードの設定ドメイン (example.com) と 集計先の mailto で指定しているドメイン (hogehoge.jp) が異なる場合、 1_dmarc.example.com CNAME v=DMARC1;p=quarantine;rua=mailto:dmarc.rua@hogehoge.jp 以下のレコードを集計先ドメイン (hogehoge.jp) で登録し、ドメインの関連性を示す必要があります。 1example.com._report._dmarc.hogehoge.jp IN TXT &quot;v=DMARC1&quot; p タグの扱い注意p=quarantine のみで pct タグは削除が良さそう。 p=reject p=reject にしてしまうと未認証メールは受信サーバは拒否する。 p=quarantine 且つ、 pct=0~99 pct=100 でない場合は、なりすましの危険性に晒される可能性がある。 参考: https://www.valimail.com/blog/what-you-need-to-know-about-the-pct-tag-in-dmarc-records/ However, this can still leave your domain open to impersonation attacks until you set pct=100 or remove the pct tag entirely. pct はデフォルト 100。pct を削除することでデフォルトの 100 が適用される。 SPF レコード登録SPF の仕組みSPF（Sender Policy Framework）は、DMARC 同様、送信ドメイン認証技術の1つです。 送信者のメールアドレスのドメイン情報のある DNS に SPF レコードを登録し、受信者が正しい送信元からの配信かをその SPF レコードを元に確認できる仕組みです。 送信者は上記の仕組みで、受信者に「なりすましでないこと」を証明する一連の仕組みを SPF と呼びます。 SendGrid での SPF レコード設定 指針SendGrid で独自ドメイン hogehoge.jp を SendGrid ドメイン認証しメール送信するとGmail のソース等で Return-Path を見ると em1234.hogehoge.jp の様にサブドメインが付与されていることがわかります。 From で設定しているドメイン hogehoge.jp と エンベロープFrom のem1234.hogehoge.jp が異なる為、 Sender ID や docomo の独自認証がパスしない場合があります。 その対応として、以下 SPF レコードで以下を登録します。 1&quot;v=spf1 include:em1234.hogehoge.jp ~all&quot; 参考 SendGrid - エンベロープFromを変更したいのですが？ SendGridで独自ドメインからdocomo宛に送信するときの注意点 スパムメール扱いされていないかテストする方法以下、送信元メールアカウントの評価サービスを利用できます。 https://www.mail-tester.com/ 上記の場合、 test-00a9mdjzu@svr1.mail-tester.com にメールを送ると、送信者を評価してくれます。 以下結果を元に、対処する項目を定めます。 まとめSendGrid には SPF/DKIM の独自ドメイン化を支援する Domain Authentication 機能等、迷惑メール扱い防止対策があります。 これらを適切に設定しないと迷惑メールに振り分けられたり、そもそもメールが拒否されたりということが起こりえる為、注意が必要です。 以下の送信ドメイン認証の仕組みを把握しつつ設定を進めると、より理解を深めることができました。 SPF（Sender Policy Framework） DKIM (DomainKeys Identified Mail) DMARC (Domain-based Message Authentication, Reporting, and Conformance) さらに到達率を上げる為に、気づいた対策があれば追記して参ります。また、ご指摘いただけますと幸いです。 以上ご参考になれば幸いです。","link":"/2020/10/07/2020-10-08-sendgrid-settings/"},{"title":"Terraform CodePipeline 認証エラー対応","text":"ToC 概要 エラーが発生する経緯 回避策 概要Terraform で管理する CodePipeline で GitHub 認証エラーが発生した為、その回避方法をまとめます。 terraform-provider-aws v3.0.0 で以下対応がされましたが、別の問題が発生している様です。 resource/aws_codepipeline: Removes GITHUB_TOKEN environment variable (#14175) エラーが発生する経緯terraform のコードは以下の様になっています。 12345678910111213141516171819202122resource &quot;aws_codepipeline&quot; &quot;deploy&quot; { ... stage { name = &quot;Source&quot; action { name = &quot;Source&quot; category = &quot;Source&quot; owner = &quot;ThirdParty&quot; provider = &quot;GitHub&quot; version = &quot;1&quot; output_artifacts = [local.prefix] configuration = { OAuthToken = var.github_token Owner = &quot;xxx&quot; Repo = &quot;yyy&quot; Branch = &quot;master&quot; PollForSourceChanges = &quot;false&quot; } } } こちらで configuration = {...} の設定で GitHub 認証をしています。 初回 terraform apply 実行時に OAuthToken に設定した値は tfstate ファイルに hash 化されて保存されます。その後、何かしらリソースを更新すると、その tfstate にある hash 化された token が UpdatePipeline に渡され GitHub 認証エラーが発生する、というものです。 なので、初回は CodePipeline で問題なく GitHub 認証されますが、その後、 terraform apply 実行しリソース更新後に CodePipeline で GitHub 認証エラーが発生します。 上記については以下 issue にて言及されていました。 aws_codepipeline with Github OAuth still breaking auth · Issue #15200 · hashicorp/terraform-provider-awsCommunity Note Please vote on this issue by adding a 👍 reaction to the original issue to help the community and maintainers prioritize this… 回避策12345678910111213141516171819202122232425resource &quot;aws_codepipeline&quot; &quot;deploy&quot; { ... stage { name = &quot;Source&quot; action { ... configuration = { OAuthToken = var.github_token Owner = &quot;xxx&quot; Repo = &quot;yyy&quot; Branch = &quot;master&quot; PollForSourceChanges = &quot;false&quot; } } } ... # NOTE: GitHub 認証エラーの暫定対応として GitHub の設定変更を無視します。 # see: https://github.com/hashicorp/terraform-provider-aws/issues/15200#issuecomment-700808677 lifecycle { ignore_changes = [stage[0].action[0].configuration] }} GitHub 認証を変更無視する様にし、 CodePipeline の認証を更新させない様にすることでGitHub 認証エラーを回避できました。 123lifecycle { ignore_changes = [stage[0].action[0].configuration]} 以前の provider バージョンで terraform plan で OAuthToken が毎回差分に出てしまう問題で対応していたコードが舞い戻ってきました。 他の対応方法があればご教示いただければ幸いです。 以上参考になれば幸いです。","link":"/2020/11/19/2020-11-20-terraform-codepipeline-github-oauth-error/"},{"title":"Go logrus でログ出力レベルを変更する","text":"ToC 概要 実際に動かすとわかりやすい。 概要Go の sirupsen/logrus が便利でよかったので、備忘録として残しておきます。 Go でツール作成時にデバッグしたい時にレベルをさくっと変更したい時があり、その際に利用しています。 実際に動かすとわかりやすい。Go Playground 1234567891011121314151617package mainimport ( log \"github.com/sirupsen/logrus\")func main() { log.SetLevel(log.FatalLevel) log.Trace(\"1\") log.Debug(\"2\") log.Info(\"3\") log.Warn(\"4\") log.Error(\"5\") log.Fatal(\"6\") log.Panic(\"7\")} 以上です。参考になれば幸いです。","link":"/2020/11/26/2020-11-27-go-logging-level/"},{"title":"ruby で ElastiCache Redis （クラスターモード有効） 利用でハマった所","text":"ToC 概要 まず結論 接続設定 Redis Cluster の接続でハマった。 クラスターモード使用時の注意 スケーリング中のアクセスは本当に処理され続けるのか？ sidekiq で利用するには 参考 概要ruby で ElastiCache Redis （クラスターモード有効）を利用した際にハマったことをまとめます。 まず結論検証終わり、以下のコードにまとめました。 1234567891011require 'redis' # redis/redis-rb を利用# 接続先は Configuration Endpoint のみ指定すれば、 cluster でよしなに、 node にアクセスしてくれた♪# 通信時(in transit), 保管時(in rest) に暗号化する様にすると`rediss://｀を指定する必要がある。 (s が1つ多い)redis = Redis.new(cluster: [\"rediss://&lt;elasticache configuration endpoint&gt;:6379\"])redis.set('key1', 'hogehoge')redis.set('key2', 'mogemoge')p redis.get('key1') # =&gt; hogehogep redis.mget('key1', 'key2') # =&gt; Redis::CommandError (CROSSSLOT Keys in request don't hash to the same slot) Redis Client Library として redis/redis-rb を使用しています。 接続設定最終的に ElastiCache Redis クラスターモード有効化時に発行される Configuration Endpoint を cluster: で指定することで解決しました。 1redis = Redis.new(cluster: [\"rediss://&lt;elasticache configuration endpoint&gt;:6379\"]) ちなみに rediss:// はミスでなく、ElastiCache Redis の設定にある通信時(in transit), 保管時(in rest) に暗号化した際に redis-rb の SSL/TLS Support を利用すべく設定しています。 参考: https://github.com/redis/redis-rb#ssltls-support ElastiCache &gt; Redis の暗号化設定 redis-rb を利用していて、暗号化したことで利用しにくいということはなかったので、セキュリティ向上の恩恵を受けることを推奨します。 Redis Cluster の接続でハマった。https://github.com/redis/redis-rb#cluster-support を参照すると以下の様に cluster: に &lt;node endpoint&gt; を複数指定することで接続実装の例を示しています。 12345nodes = [ \"rediss://&lt;node endpoint 1&gt;:6379\", \"rediss://&lt;node endpoint 2&gt;:6379\",]redis = Redis.new(cluster: nodes) ですが、この場合、node を増やして slot を再構成したら、増やした node にあるデータが取れなくなってしまいます。 上記の nodes に増やした node endpoint を追加しないといけなくなります。動的でありません。 以下 AWS ドキュメントにある、「スケーリングプロセス中でもリクエストを処理し続ける」恩恵を受けられなくなってしまうことになります。https://docs.aws.amazon.com/ja_jp/AmazonElastiCache/latest/red-ug/scaling-redis-cluster-mode-enabled.html クラスターの需要の変化に応じて Redis (クラスターモードが有効) のクラスター内のシャード数を変更することで、パフォーマンスを向上させたりコストを削減したりできます。そのために、スケーリングプロセス中でもクラスターがリクエストを処理し続けることができる、オンライン水平スケーリングの使用をお勧めします そこで、ElastiCache Redis クラスターモード有効時に発行される Configuration Endpoint を利用したい。でも以下はダメだった。。 1Redis.New(url: \"rediss://&lt;configuration endpoint&gt;:6379\") そして、以下試した所いけた！嬉しかった！ 1Redis.New(cluster: [\"rediss://&lt;configuration endpoint&gt;:6379\"]) クラスターモード使用時の注意クラスターモード有効時に redis.mget を使用した際に CROSSSLOT Keys in request don't hash to the same slot のエラーが確認されました。 1234567redis = Redis.new(cluster: [\"rediss://&lt;elasticache configuration endpoint&gt;:6379\"])redis.set('key1', 'hogehoge')redis.set('key2', 'mogemoge')p redis.get('key1') # =&gt; hogehogep redis.mget('key1', 'key2') # =&gt; Redis::CommandError (CROSSSLOT Keys in request don't hash to the same slot) 以下にサンプルもあり、呼び出しコマンドで クロススロットコマンドは避けてね、とあります。https://github.com/redis/redis-rb#cluster-support The calling code is responsible for avoiding cross slot commands. スケーリング中のアクセスは本当に処理され続けるのか？実際に以下実施しながら、 set, get を実行しましたが、特に問題なくリクエストは処理され続けられました。 水平スケール シャード追加 Add shards シャード削除 Delete shards レプリカ追加 Add Replicas レプリカ削除 Delete Replicas 垂直スケール インスタンスのスペックアップ・ダウン メンテに強い、Redis クラスターモード！ sidekiq で利用するにはsidekiq は Cluster に適していない、Sentinel or failover サポートの Redis SaaS がおすすめです、とドキュメントにあります。 https://github.com/mperham/sidekiq/wiki/Using-Redis#architecture Cluster is designed for large-scale datasets, like caches, that can spread evenly across machines. Cluster is NOT appropriate for Sidekiq as Sidekiq has a few very hot keys which are constantly changing (aka queues). I recommend using Sentinel or use a Redis SaaS which has built-in support for failover. redis-rb + ElastiCache Redis クラスターモード有効 だけの対処では、対応できないので、別途準備が必要です。 現状、 sidekiq 利用する際には、 クラスターモードは有効にせず使用しています。 参考https://made.livesense.co.jp/entry/2018/10/17/135245","link":"/2020/12/10/2020-12-11-ruby-to-elasticache-redis-cluster-mode/"},{"title":"Terraform Elasticache Redis 6.x 構築時の注意","text":"ToC 概要 redis engine_version = 6.x 指定時の問題点 概要terraform で ElastiCache redis 6 系を利用時に通る儀式があったので、備忘録 redis engine_version = 6.x 指定時の問題点ElastiCache &gt; Redis を 6 系で構築するには以下の様に engine_version = &quot;6.x&quot; と指定する必要があります。 1234resource &quot;aws_elasticache_replication_group&quot; &quot;cache_store&quot; { engine = &quot;redis&quot; engine_version = &quot;6.x&quot; ... そして、構築後に terraform plan をすると、以下の様な変更が生じます。 123~ resource &quot;aws_elasticache_replication_group&quot; &quot;cache_store&quot; { engine = &quot;redis&quot; ~ engine_version = &quot;6.0.5&quot; -&gt; &quot;6.x&quot; この事象について issue が上がっていました。 https://github.com/hashicorp/terraform-provider-aws/issues/15625 対応の一手として ignore_changes = [engine_version] を加えるのは極力避けたい。https://github.com/hashicorp/terraform-provider-aws/issues/15625#issuecomment-727759811 現状は、作成後に以下の様に engine_version = &quot;6.0.5&quot; を指定することで対応しています。 123resource &quot;aws_elasticache_replication_group&quot; &quot;cache_store&quot; { engine = &quot;redis&quot; engine_version = &quot;6.0.5&quot; 構築時に engine_version = &quot;6.0.5&quot; って指定したらいいじゃないか！と思いましたが、それだとエラーになります。 なので、構築時にはコメントを入れておくとレビュワーに優しい。 123456resource &quot;aws_elasticache_replication_group&quot; &quot;cache_store&quot; { engine = &quot;redis&quot; # TODO: 構築後に採用されたバージョンに変更する。 # 構築後に terraform plan で engine_version の差分が生じる為です。 # see: https://github.com/hashicorp/terraform-provider-aws/issues/15625 engine_version = &quot;6.x&quot; 以上参考になれば幸いです。","link":"/2020/12/13/2020-12-14-terraform-elasticache-redis-6.x/"},{"title":"2020年 RapberryPI で作ったおもちゃ達","text":"ロトム図鑑 Login • Instagram Welcome back to Instagram. Sign in to check out what your friends, family &amp; interests have been capturing &amp; sharing around the world. 息子にポケモンのスマホロトム作って！と言われて作りました♪「ロトム、ピカチュウ」でピカチュウの情報を喋ってくれます。 ポケモンの API でもあればなぁ〜と思って探したらこの記事に出会いました。 GoogleHomeでポケモン図鑑作ってみた 「GoogleHomeでポケモン図鑑」を改良してヒカリちゃんの声にしてみた 同僚の素敵記事です♪ ピカチュウラジコン ラジコンにピカチュウを載せたものです。 ウェブページにコントローラーを配置し、 Web 経由でラジコンを操作します。 以下本を読み進めると自然に GET できます！ Raspberry PI 工作が体系的に学べるので初学者にうってつけだと思いました。 プログラミングはダウンロードしそのまま利用できるので、プログラミングの中身を見ずともできてしまいます。プログラミングの説明もしっかりされていて、 Web ブラウザから処理を受けて、モーターを制御するピンに信号を送る仕組みを知るのに持ってこいでした。 主な部品は以下です。 作ってみて「こんなことできるのか！？」とうなることが多いのも然る事乍ら子供に喜んでもらえることがとても嬉しかったです。","link":"/2021/01/21/2021-01-22-pokemon/"},{"title":"既存 ECS Service の Fargate Spot への切り替え方法","text":"ToC 概要 terraform で切り替える aws-cli で切り替える CodeDeploy のデプロイコントローラーで管理されている場合 ダウンタイムなしで切り替え &amp; tfstate 更新手順 余談 ちなみに 余談2 概要既存 ECS Service の Farate Spot への切り替え方法は 2 つあります。 terraform aws-cli terraform で切り替えるcapacity_provider_strategy を追加し 100% FargateSpot で起動させるとします。 123456789101112resource &quot;aws_ecs_service&quot; &quot;app&quot; {- launch_type = &quot;FARGATE&quot;+ capacity_provider_strategy {+ capacity_provider = &quot;FARGATE_SPOT&quot;+ weight = 100+ }+ capacity_provider_strategy {+ capacity_provider = &quot;FARGATE&quot;+ weight = 0+ } terraform では再作成されます。 1234$ terraform plan# aws_ecs_service.app must be replaced-/+ resource &quot;aws_ecs_service&quot; &quot;app&quot; { aws-cli で切り替えるaws-cli では再作成することなく更新が可能です。 12345aws ecs update-service \\ --capacity-provider-strategy capacityProvider=FARGATE,weight=0 capacityProvider=FARGATE_SPOT,weight=100 \\ --cluster &lt;ecs cluster name&gt; \\ --service &lt;ecs service name&gt; \\ --force-new-deployment 直ちに強制デプロイが実行され、ダウンタイムなく CapacityProviderStrategy が適用されます。 CodeDeploy のデプロイコントローラーで管理されている場合以下エラーが発生します。 1An error occurred (InvalidParameterException) when calling the UpdateService operation: Cannot force a new deployment on services with a CODE_DEPLOY deployment controller. Use AWS CodeDeploy to trigger a new deployment. そもそも CodeDeploy でデプロイ管理している場合、 FargateSpot を利用できない。 利用できる様になりました。 (2021-10-14 追記)https://docs.aws.amazon.com/ja_jp/AmazonECS/latest/developerguide/cluster-capacity-providers.html サービスで Blue/Green デプロイタイプを使用している場合、キャパシティープロバイダーの使用はサポートされません。 aws-cli では CodeDeploy 経由でデプロイする様に警告されてしまう為、terraform で再作成する以外方法がなさそうです。CodeDeploy の appspec の設定で以下の様に指定することで再作成せず管理できることを確認しました。 12345678910111213version: 0.0Resources: - TargetService: Type: AWS::ECS::Service Properties: ... CapacityProviderStrategy: - CapacityProvider: 'FARGATE_SPOT' Weight: 100 Base: 0 - CapacityProvider: 'FARGATE' Weight: 0 Base: 0 ※ Base や Weight の設定は適宜変更してください。 ダウンタイムなしで切り替え &amp; tfstate 更新手順terraform でリソース管理をしている場合、以下の手続きでダウンタイムなしで切り替えできました。 aws-cli で更新 terraform apply で tfstate 更新 terraform での管理もでき、且つ、ダウンタイムなしに更新ができます。 余談terraform で ECS Service 新規作成時だと CodeDeploy と CapacityProviderStrategy を両方指定できてしまいます。 以下画像だけ見るとできるの？と思ってしまいます。 これっぽい！ CodeDeploy のデプロイコントローラー管理を残し CapacityProviderStrategy を外そうとすると再作成するしか今の所、対処法がなさそうです。 aws-cli でlaunch_type → capacity_provider_strategy はできるけど、逆はできない。 ちなみにAWS Console 上で ECS Service 構築時に capacity provider strategy を指定しているとDeployment type は Rolling update しか選択できない様になっています。 余談2今回の実装例にある FargateSpot weight 100% だと起動できなくなります。非商用環境ではコスト削減の為、Spot化したくなりますが注意が必要です。 以下 issue で Fargate Spot 枯渇時に Fargate へ切り替え、最低限起動できる様にする、という対処が進んでいる様です。 https://github.com/aws/containers-roadmap/issues/773https://github.com/aws/containers-roadmap/issues/852 以上ご参考になれば幸いです。","link":"/2021/01/28/2021-01-29-apply-fargate-spot-for-existed-fargate/"},{"title":"Go で init() 内の os.Exit(1) を go test で回避する方法","text":"ToC 概要 検証 main_test.go で main.go の処理を上書きできるか検証する テスト実行時のみ環境変数で制御する まとめ 追伸 概要AWS Lambda Go プロジェクトを SAM で構築していた際、パラメータストアから取得する処理を init() に記述しました。 理由はパラメータストアから取得する処理をキャッシュし、再利用することで連続 Lambda の実行コストを節約する為です。 詳細は AWS Lambda ベストプラクティス参照 https://docs.aws.amazon.com/ja_jp/lambda/latest/dg/best-practices.html 実行環境の再利用を活用して関数のパフォーマンスを向上させます。 関数ハンドラー外で SDK クライアントとデータベース接続を初期化し、静的なアセットを /tmp ディレクトリにローカルにキャッシュします。関数の同じインスタンスで処理された後続の呼び出しは、これらのリソースを再利用できます。これにより、実行時間とコストが節約されます。 以下の様に init() でパラメータストアから秘匿情報を取得する処理をキャッシュし、コスト節約したいと考えました。 123456789101112131415func init() { sess := session.Must(session.NewSession(&amp;aws.Config{ Region: aws.String(&quot;ap-northeast-1&quot;), })) // NOTE: パラメータストアから秘匿情報を取得する // see: https://gist.github.com/kenzo0107/10654b09fb7b0ca889e807d27b646d09 ssmClient := awsapi.NewSSMClient(ssm.New(sess)) s, err := ssmClient.GetSSMParameters([]string{ &quot;secret&quot;, }) if err != nil { log.Fatal(err) } 上記コードについてGitHub Actions で go test を実行しテストをしていますが、log.Fatal で os.Exit(1) 発生し処理が停止します。 credentials が設定されてないというエラーです。 12NoCredentialProviders: no valid providers in chain. Deprecated. For verbose messaging see aws.Config.CredentialsChainVerboseErrors GitHub Actions でダミーの credentials を設定しても失敗します。 1234run: go test -v -count=1 -race -cover -coverprofile=coverage ./...env: AWS_ACCESS_KEY_ID: ADUMMYDUMMYDUMMYDUMD AWS_SECRET_ACCESS_KEY: DummyDummyDummyDummyDummyDummyDummyDummy init() でのキャッシュを諦めて、 handler で処理するとハンドリングは簡単です。 ですが、「コスト節約」が頭から離れません。自分は弱い人間です。 go test 実行する時だけでも、この os.Exit(1) を回避できないものか？ということで検証してみました。 検証以下の様な main.go ファイルがあるとします。go test 実行時に init() の log.Fatal をどう回避するか、検証します。 12345678910111213141516171819202122var n = 0func init() { if err := doSomething(n); err != nil { log.Fatal(&quot;error&quot;) }}func doSomething() error { if n == 0 { return errors.New(&quot;error&quot;) } return nil}func handler() int { return 0}func main() { os.Exit(handler())} 特に何も意識せず go test すると init() 内の log.Fatal で os.Exit(1) が発生し強制終了される。テストが完結しない。 テスト実行時にだけ var n = 1 とできたら良さそうだが、、、 main_test.go で main.go の処理を上書きできるか検証するそもそも処理順序はどうなっているのか、まず検証してみた。 参考: https://github.com/kenzo0107/go-sample-order 12345678910111213package pkg// SampleVar : sample variable in pkgvar SampleVar = defaultVar()func defaultVar() int { println(&quot;pkg.var&quot;) return 1}func init() { println(&quot;pkg.init&quot;)} 123456789101112131415161718import &quot;github.com/kenzo0107/go-sample-order/pkg&quot;var someVar = defaultVar()func init() { println(pkg.SampleVar) println(&quot;main.init&quot;)}func main() { println(&quot;main.main&quot;)}func defaultVar() int { println(&quot;main.var&quot;) return 2} 123456789101112func init() { println(&quot;test.init&quot;)}func setup() { println(&quot;test.setup&quot;)}func TestMain(m *testing.M) { setup() m.Run()} go test を実行してみます。処理順序は以下の通りでした。 pkg.var pkg.init main.var main.init test.init test.setup 以上から main_test.go での如何なる処理も main.init より先に実行できません。 main_test.go で main.go の変数 var n を上書きする処理は難しそうです。 テスト実行時のみ環境変数で制御する1234567891011121314func init() { ... logFatal(&quot;error&quot;)}func logFatal(err error) { log.Println(err) if os.Getenv(&quot;TEST&quot;) != &quot;&quot; { return } os.Exit(1)}... log.Fatal を logFatal という関数に置換し、以下処理を実行する様にします。 環境変数 TEST が存在する → log.Println でログを残し、 os.Exit(1) を実行しない 環境変数 TEST が存在しない → os.Exit(1) 実施し強制停止 main.go を実行します。 123456＄ go run main.gomain.varmain.init2021/01/28 00:10:00 errorexit status 1 環境変数 TEST=1 を設定し go test を実行してみる。 1234567891011$ TEST=1 go test -v .main.varmain.init2021/01/28 00:20:00 errortest.inittest.setuptesting: warning: no tests to runPASScoverage: 83.3% of statementsok github.com/kenzo0107/sample 0.345s coverage: 83.3% of statements [no tests to run] 無事 init() 内の os.Exit(1) を回避し処理が継続して実行されました。 GitHub Actions の設定も簡易的な設定です。 12345- name: Test run: go test -v -count=1 -race -cover -coverprofile=coverage ./... env: # NOTE: テストのみ init() で os.Exit 実行回避する為に設定している。 TEST: true 環境変数でなく os.Args をゴニョゴニョして go test の実行を判断もできそうですが、実装的にはシンプルで用途としても他で使えるので LGTM かなと。 まとめ本来 init() でエラーハンドリングをすべきではないのかもしれません。 以下にも言及されていましたが、 init() の処理で失敗したら以降の main() の処理を実行しない、というのは悪い処理でないように思います。 https://stackoverflow.com/questions/33885235/should-a-go-package-ever-use-log-fatal-and-when?answertab=votes#tab-top とはいえ、今回の環境変数で処理を操作、というのはややエレガントさに欠ける気持ちはあります。 やんごとなき事情がある場合にこの様な処理がある、ということを心のどこかに留めていただきたく、ここで筆を置きたいと思います。 ご清聴ありがとうございました。 追伸AWS 公式のドキュメントだと init() で err を握り潰している！https://docs.aws.amazon.com/ja_jp/lambda/latest/dg/golang-handler.html エラーが発生する場合のテストどうするつもりだろう？絶対エラー起きないんで！って言われるかな〜","link":"/2021/01/31/2021-02-01-avoid-go-init-osexit/"},{"title":"terraform-provider-aws 3.26.0 で ElastiCache ClusterMode でエラーになる件","text":"ToC 備忘録です。 terraform-provider-aws 3.26.0 で 以下設定ではエラーになります。 12345678910resource &quot;aws_elasticache_replication_group&quot; &quot;cache_store&quot; { automatic_failover_enabled = true # NOTE: 商用環境以外でコストを抑えるべく必要最低限のリソースの起動にする cluster_mode { replicas_per_node_group = 0 num_node_groups = 1 } ...} 1234Error: if automatic_failover_enabled is true, number_cache_clusters must be greater than 1 on redis_cache.tf line 12, in resource \"aws_elasticache_replication_group\" \"cache_store\": 12: resource \"aws_elasticache_replication_group\" \"cache_store\" { 以下に修正する必要がある。 123456789resource &quot;aws_elasticache_replication_group&quot; &quot;cache_store&quot; {- automatic_failover_enabled = true+ automatic_failover_enabled = false cluster_mode { replicas_per_node_group = 0 num_node_groups = 1 } ...} 以上です。","link":"/2021/02/01/2021-02-02-avoid-elasticache-cluster-mode-error-in-terraform-provider-aws-3-26-0/"},{"title":"Ansible のターゲットホストで Python3 を指定し pip install する","text":"ToC 概要 OS バージョン情報は以下の通り サムネイルが全てですが、一応 概要Raspberry PI Zero WH で、 python -V すると Python 2.7.16 が返ってきた。 Raspberry PI のセットアップは Ansible で管理しており、Ansible で unlink python して ln -s /usr/bin/python3 /usr/bin/python をすれば良いのかな？と思っていたら、もっとシンプルにできたのでメモ。 OS バージョン情報は以下の通り123456789101112$ cat /etc/os-releasePRETTY_NAME=&quot;Raspbian GNU/Linux 10 (buster)&quot;NAME=&quot;Raspbian GNU/Linux&quot;VERSION_ID=&quot;10&quot;VERSION=&quot;10 (buster)&quot;VERSION_CODENAME=busterID=raspbianID_LIKE=debianHOME_URL=&quot;http://www.raspbian.org/&quot;SUPPORT_URL=&quot;http://www.raspbian.org/RaspbianForums&quot;BUG_REPORT_URL=&quot;http://www.raspbian.org/RaspbianBugs&quot; サムネイルが全てですが、一応以下の ansible.cfg の設定が全てです。 ansible.cfg 12[defaults]interpreter_python=/usr/bin/python3 tasks/main.yml12345# NOTE: pip3 として利用する- name: pip3 install packages pip: name: - mh_z19 ansible-playbook 実行後、 123456789101112131415161718// /usr/bin/python には python2 がリンクされている$ ls -al /usr/bin/pythonlrwxrwxrwx 1 root root 7 Mar 6 15:22 /usr/bin/python -&gt; python2$ python -VPython 2.7.16// python2 にモジュールがインストールされていないことを確認// pip list でも良いが、メッセージがわかりやすいのでこの記述にしている$ sudo python -m mh_z19/usr/bin/python: No module named mh_z19$ python3 -VPython 3.7.3// python3 でモジュールがインストールされていることを確認$ sudo python3 -m mh_z19{&quot;co2&quot;: 695} わざわざリンクされている python を切り替えなくてもシンプルに Python バージョンを指定して利用できた！ 以上参考になれば幸いです。","link":"/2021/03/06/2021-03-07-use-python3-by-ansible/"},{"title":"Nginx on Fargate で発生した Resource temporarily unavailable エラーを調査した","text":"Nginx を Fargate で起動しているが、検証中にすぐにリクエストが詰まってしまう事象に悩まされました。その際に調査したことをまとめます。 ToC エラーログレベルを debug に設定 net.core.somaxconn を調べてみる Dockerfile で変更できるか試してみる Fargate の設定でできないものか？ 結論 エラーログレベルを debug に設定元々 info にしていたが、それらしいログが確認できなかった為、nginx の設定ファイルでエラーログレベルを debug に設定しログを確認してみます。 1error_log stderr debug; すると以下ログが確認されました。 1accept() not ready (11: Resource temporarily unavailable) accept() が一時的なリソース不足で準備できてない、とのこと。 Nginx の通信の仕組み上、 accept() が担当している箇所は以下図がわかりやすいです。 net.core.somaxconn を調べてみるnet.core.somaxconn は TCP ソケットが受け付けたリクエストを格納する、キューの最大長です。 受け入れ可能なリクエスト数が小さいのでは？という仮説を立てました。 Nginx コンテナに入って以下実行します。 12$ sysctl net.core.somaxconnnet.core.somaxconn = 128 非常に少ない！ ここの設定を変えたい！ Dockerfile で変更できるか試してみる1RUN sysctl -w net.core.somaxconn=1024 エラーになった (TへT) 1#21 0.370 sysctl: error setting key 'net.core.somaxconn': Read-only file system Fargate でできないものか？ 1docker run --sysctl net.core.somaxconn=65535 ... Fargate の設定でできないものか？【週刊 Ask An Expert #04】AWS Loft Tokyo で受けた質問まとめ #AWSLoft Q: Fargate で net.core.somaxconn を変更したい現時点では Fargate では変更できないので、タスクを多く起動して頂くことで解決して下さい。 結論現状、Nginx で発生する Resource temporarily unavailable の解決は、素直にタスク数を増やすことで対応するしかない！ AWS のコンテナロードマップには issue として上がっている。 いずれ Fargate でも対応される日は近い… はず！ https://github.com/aws/containers-roadmap/issues/623","link":"/2021/04/15/2021-04-16-nginx-on-fargate-somaxconn/"},{"title":"Fargate のタスクサイズ による Nginx の起動プロセス数","text":"備忘録です。 Nginx の設定で、プロセス数を auto にして CPU コア数に委ねる設定があります。 1worker_processes auto; Fargate のタスクサイズだとプロセス数はどの程度になるか調査しました。 結果発表 cpu mem nginx worker process 数 256 512 2 1024 2048 2 2048 4096 4 タスクサイズを上げれば、ちゃんと auto で worker プロセス数が増えてくれた。 ちなみにプロセス数が増えても net.maxsoconn 値は Fargate で変更できない のでリクエストをより捌く様にするには、タスク数を増やした方が良いです。 ちなみに、以下で同額のコスト♪ cpu=256, mem=512 × 8 タスク cpu=2048, mem=4096 × 1 タスク タスク数を増やそう！","link":"/2021/04/29/2021-04-30-nginx-process-count-on-fargate/"},{"title":"NLB+Fargate でクライアントIP を Fargate に送り届ける","text":"ToC 原因 対策 TargetGroup の設定 Nginx の設定 結果 NLB 配下の TargetGroup では、アクセス元のクライアントIP を変更することなく、Target に渡すことができる機能があります。 上記を利用した IP 制御の実装が可能です。 ですが、NLB + Fargate の構成の場合、Fargate に渡る IP が NLB のプライベート IP になっていることを確認しました。 （EC2 の場合は問題なくクライアントIPが渡っていた） Nginx で以下の様な設定はしているけど… Why ?? 123set_real_ip_from 10.10.0.0/16;real_ip_header X-Forwarded-For;real_ip_recursive on; 原因https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html#client-ip-preservation If the target group protocol is TCP or TLS, client IP preservation is disabled by default. NLB は Target Group のプロトコルが TCP or TLS の場合、 クライアント IP はデフォルトで無効化される仕様でした。 上記を補足するとFargate は TargetGroup でターゲットタイプ = IP を指定する為、デフォルトでクライアントIPは保持されない、となります。 EC2 は TargetGroup でインスタンスIDを指定しており、その場合、クライアントIPの保持は有効化されます。 対策以下 AWS ドキュメントにある通り、 proxy protocol v2 を有効化します。 https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html#proxy-protocol TargetGroup の設定123456resource &quot;aws_lb_target_group&quot; &quot;app_https&quot; { ... target_type = &quot;ip&quot; proxy_protocol_v2 = true # proxy protocol を有効化する ...} Nginx の設定proxy_protocol リクエストを受け付ける様にする。 123server { listen 443 ssl proxy_protocol; server_name _; 結果無事 Fargate にクライアントIPを送り届けることができました！ ですが、Nginx のエラーログレベルを debug にするとproxy protocol v2 をサポートしてないと言っているのが確認できた。 https://github.com/nginx/nginx/commit/9207cc84b21e94283478cee7a953b1859c4434cb を見る限り、問題なく対応していそう。 Nginx 1.13.11 以降なら対応しているとオフィシャルも言っていて、問題はなさそう。 https://docs.nginx.com/nginx/admin-guide/load-balancer/using-proxy-protocol/ To accept the PROXY protocol v2, NGINX Plus R16 and later or NGINX Open Source 1.13.11 and later ログの内容こそ気になるものの、LB 的には Fargate 宛に client ip を届けており、 nginx で解釈できている様です。","link":"/2021/04/29/2021-04-30-send-clientip-nlb-nginx-on-fargate/"},{"title":"Nginx で ELB のヘルスチェックのログを出力させない","text":"概要ELB のヘルスチェック時の User-Agent ($http_user_agent) が ELB-HealthChecker の場合に、ログをオフにする設定の備忘録です。 123456789http { ... map $http_user_agent $loggable { ~ELB-HealthChecker 0; default 1; } access_log /var/log/nginx/access.log ltsv if=$loggable;} access_log のパラメータ if で条件を指定でき、アクセスログの出力の on/off が可能です。 ToC 設定例 設定例 conf/nginx.conf 123456789101112131415161718192021222324252627282930http { ... # access log を ltsv 形式にする log_format ltsv 'domain:$host\\t' 'host:$remote_addr\\t' 'user:$remote_user\\t' 'time:$time_local\\t' 'method:$request_method\\t' 'path:$request_uri\\t' 'protocol:$server_protocol\\t' 'status:$status\\t' 'size:$body_bytes_sent\\t' 'referer:$http_referer\\t' 'agent:$http_user_agent\\t' 'response_time:$request_time\\t' 'cookie:$http_cookie\\t' 'set_cookie:$sent_http_set_cookie\\t' 'upstream_addr:$upstream_addr\\t' 'upstream_cache_status:$upstream_cache_status\\t' 'upstream_response_time:$upstream_response_time'; map $http_user_agent $loggable { ~ELB-HealthChecker 0; default 1; } access_log /var/log/nginx/access.log ltsv if=$loggable; include /etc/nginx/conf/conf.d/*.conf;} conf/conf.d/default.conf 12345678910server { listen 80; listen [::]:80; # ELB のヘルスチェッカーの場合、 200 を返す if ($http_user_agent ~* ELB-HealthChecker) { return 200; } ...} 以上参考になれば幸いです。","link":"/2021/05/19/2021-05-20-nginx-no-logging-at-elb-healthcheck/"},{"title":"日本の祝日判定 Go ライブラリ shukujitsu を作った","text":"ToC 概要 祝日判定の方法 自動化はどう実現したか？ Google Calendar API を採用した理由 総評 概要2021年の夏は五輪特措法で海の日が 7/19 から 7/22 に移動されています。祝日判定ライブラリの元となるデータセットが更新されておらず、困ったことから祝日のデータセットを自動更新する仕組みを持った Go ライブラリ kenzo0107/shukujitsu を作るに至った話です。 祝日判定の方法祝日判定する際には主に2つの方法があります。 Google Calendar API を利用する 内閣府ホームページで提供される shukujitsu.csv を元に判定する 上記 2 案を元にした OSS も多数あり、そちらを利用することで祝日判定ができます。 ですが、2021年7月15日時点で以下理由で見送りました。 https://github.com/holiday-jp/holiday_jp 内閣府HP提供 syukujitsu.csv をベースとしているが、自動更新していない https://github.com/holiday-jp/holiday_jp/pull/110 のコメントを見る限り、自動化していないと判断しました。 https://holidays-jp.github.io/ 自動更新されていそう。2021-07-22 が祝日であることが確認できた為。 祝日名が取れず、あくまで祝日かどうかを返す API だった。 祝日だった場合 → “holiday”、そうでない場合 → “else” が返る。 http://s-proj.com/utils/holiday.html Google Calendar API を利用しているが、更新頻度が明記されておらず不明だった。 気づかぬうちに祝日移動してた、ということを回避したく、祝日データが自動更新される仕組みを持った祝日判定ライブラリが欲しかったので自作するに至りました。 kenzo0107/shukujitsu 自動化はどう実現したか？GitHub Actions で Google Calendar API を利用し月2回実行し、祝日情報を取得し、 auto commit で祝日データを更新します。 実際に見ていただけるとわかりやすいかと思います。 https://github.com/kenzo0107/shukujitsu/blob/main/.github/workflows/auto_update_shukujitsu.yml 最後の Slack 通知はどの程度更新されるものか、確認したく、自身の Slack チャンネルに通知する様にしました。 Google Calendar API を採用した理由内閣府 HP 提供の syukujitsu.csv は実際に csv ファイルを開いて見るとわかりますが、sjis でフォーマットがやや雑な印象です。 1955/1/1 は 1955-01-01 にして欲しい気持ち。 これは気づかぬ内にしれっと変わるのでは？という危惧から利用を避けました。 その点、 Google Calendar API もレスポンス内容が変わる可能性こそありますが、syukujitsu.csv ほどではないのでは？という推察の元、Google Calendar API を採用しました。 尚、取得可能な祝日は以下の様になります。 Google Calendar API 去年・今年・翌年の 3年分 syukujitsu.csv 1955年から現在まで syukujitsu.csv の方が、範囲が広く正確に過去分まで取得したい場合は、syukujits.csv が良いです。 現状、自身の管理するシステムで利用する分には Google Calendar API の守備範囲で問題ない為、 Google Calendar API にしています。 syukujitsu.csv が今後、数年このフォーマットを維持していただけるなら、ファイルサイズも小さいですし、syukujitsu.csv への乗り換えを検討しようと思います。 以上です。ご利用いただけますと幸いです。 総評ゆくゆくはデジタル庁から祝日判定用の API が公開されたり、ということがあるのでは？と期待しています。 リポジトリ名を shukujitsu としましたが、 go ライブラリ感が薄いなぁ〜と反省。。","link":"/2021/07/08/2021-07-09-shukujitsu-go-library/"},{"title":"ecs execute command が失敗した際に調査したこと","text":"ToC 概要 まずは本当に有効化されているか調査する Task Role が権限を所持しているか 他のコンテナで試してみる 対象のコンテナを再度実行 同一タスクで強制デプロイでタスクを入れ替える まとめ 概要execute command を有効化させたが、 ecs execute command を実行した際にエラーが発生した為、その調査方法をまとめました。 1An error occurred (InvalidParameterException) when calling the ExecuteCommand operation: The execute command failed because execute command was not enabled when the task was run or the execute command agent isn't running. Wait and try again or run a new task with execute command enabled and try again. まずは本当に有効化されているか調査するECS Service で enableExecuteCommand = true が返ることを確認する。 123$ aws ecs describe-services --cluster example-cluster --services example-service | jq '.services[].enableExecuteCommand'true 起動 Task で enableExecuteCommand = true が返ることを確認する。 123$ aws ecs describe-tasks --cluster example-cluster --tasks 61cf31d333cd43508a412e1437814e19 | jq '.tasks[].enableExecuteCommand'true どちらも true が返るのに何故？ Task Role が権限を所持しているか大丈夫そう♪ 123456789statement { actions = [ &quot;ssmmessages:CreateControlChannel&quot;, &quot;ssmmessages:CreateDataChannel&quot;, &quot;ssmmessages:OpenControlChannel&quot;, &quot;ssmmessages:OpenDataChannel&quot;, ] resources = [&quot;*&quot;]} 他のコンテナで試してみる12345678$ aws ecs execute-command \\ --cluster example-cluster \\ --task 61cf31d333cd43508a412e1437814e19 \\ --container other_container \\ --interactive \\ --command &quot;ps aux&quot;他コンテナだと ecs execute-command を実行できた ここまでくると対象のコンテナが怪しい 対象のコンテナを再度実行rails コンテナだと the execute command agent isn't running. が発生していた。 12345678$ aws ecs execute-command \\ --cluster example-cluster \\ --task 61cf31d333cd43508a412e1437814e19 \\ --container rails \\ --interactive \\ --command &quot;ps aux&quot;An error occurred (InvalidParameterException) when calling the ExecuteCommand operation: The execute command failed because execute command was not enabled when the task was run or the execute command agent isn't running. Wait and try again or run a new task with execute command enabled and try again. rails c をよく利用するから OOM でやられたか??? 同一タスクで強制デプロイでタスクを入れ替える同一タスクで強制デプロイ実行後、 ecs execute-command 実行すると成功した！ 一時的な問題だった様でした。 まとめThe execute command failed because execute command was not enabled when the task was run or the execute command agent isn't running. が発生した場合は、各コンテナの都合で発生する場合もあるので要注意です。 同様のエラーが発生した際に、この記事のリンクをぽんっと貼ってもらえたら何よりです。 以上参考になれば幸いです。","link":"/2021/07/26/2021-07-27-ecs-execute-command-agent-not-running/"},{"title":"Nginx access_log $host が _ になる件調査","text":"Nginx access_log で指定している $host が _ となるケースがあるので調査したことをまとめます。 ToC $host とは？ アクセスログ設定 Host ヘッダーを空指定してリクエストしてみる 結論 $host とは？https://www.nginx.com/resources/wiki/#-24host $host This variable is equal to line Host in the header of request or name of the server processing the request if the Host header is not available. This variable may have a different value from $http_host in such cases: 1) when the Host input header is absent or has an empty value, $host equals to the value of server_name directive; 2) when the value of Host contains port number, $host doesn’t include that port number. $host’s value is always lowercase since 0.8.17. リクエストヘッダーの Host と同じ、もしくは、 ホストヘッダーが利用できない場合、リクエスト処理をするサーバ名になる。 アクセスログ設定123log_format main '$remote_addr - $remote_user [$time_local] ' '&quot;$request_method https://$host$request_uri $server_protocol&quot; $status $body_bytes_sent ' '&quot;$http_referer&quot; &quot;$http_user_agent&quot;'; &quot;$request&quot; でなく、 &quot;$request_method https://$host$request_uri $server_protocol&quot; を使用しているのは、$request の場合、ホスト情報が記載されない為です。 $host を利用したかったのは、複数のホスト名を扱えるサイトを構築した為です。 Host ヘッダーを空指定してリクエストしてみる123456789curl -H &quot;Host: &quot; &quot;https://example.com/?test=kenzo.tanaka&quot;&lt;html&gt;&lt;head&gt;&lt;title&gt;400 Bad Request&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;center&gt;&lt;h1&gt;400 Bad Request&lt;/h1&gt;&lt;/center&gt;&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;&lt;/body&gt;&lt;/html&gt; ログ確認してみると再現できました。 1xxx.xxx.xxx.xxx - - [07/Sep/2021:11:16:16 +0900] &quot;GET https://_/?test=kenzo.tanaka HTTP/1.1&quot; 400 150 &quot;-&quot; &quot;-&quot; curl で実行したので User Agent もない。 結論アクセス元 IP が海外が多いのと User-Agent 等がないことから Bot なのでは？と推察しています。通常ブラウザ操作でヘッダーのホスト情報を指定せずリクエストする様なことはないと思うので、一般ユーザには影響はないかと思いました。 但し、 Bot で攻撃頻度が高く、サーバリソースを逼迫させる様な場合は、IP遮断等、 WAF の設定が必須です。 以上です。ご参考になれば幸いです。","link":"/2021/09/06/2021-09-07-nginx_access_log_host_empty/"},{"title":"DynamoDB Scan ではなく Query を使おう！ ~GSI の設定には気をつけようの巻~","text":"ToC 【例題】 どうやって Query を実行する？ GSI を指定する 余談 参考: https://docs.aws.amazon.com/ja_jp/amazondynamodb/latest/developerguide/bp-query-scan.html 応答時間を短縮するには、アプリケーションが Scan ではなく Query を使用できるようにテーブルおよびインデックスを設計します。 AWS ベストプラクティス にもある通り、Scan による全件捜査はデータ量が増えるとパフォーマンスが劣化する為、極力 Query を利用することを推奨しています。 【例題】 どうやって Query を実行する？以下の様な worriors テーブルがあるとします。 id category name score 1 a tanahashi 88 2 a choshu 70 3 a maeda 77 4 b sayama 90 5 b antonio 100 Key Schema: id : （数値） パーティションキー score: （数値）ソートキー ここで category = a &amp; score &gt; 70 の name リストを取得したい場合、どの様に DynamoDB に処理を実行すれば良いでしょうか？ Scan を利用した場合 1234567891011121314151617181920212223242526272829303132$ aws dynamodb scan \\ --table-name worriors \\ --projection-expression '#NM' \\ --filter-expression '#CTGRY = :g AND #SCR &gt; :scr' \\ --expression-attribute-values '{ \":g\": {\"S\":\"a\"}, \":scr\": {\"N\":\"70\"} }' \\ --expression-attribute-names '{ \"#CTGRY\": \"category\", \"#SCR\": \"score\", \"#NM\": \"name\" }'// response{ \"Items\": [ { \"name\": { \"S\": \"maeda\" } }, { \"name\": { \"S\": \"tanahashi\" } } ], \"Count\": 2, \"ScannedCount\": 5, \"ConsumedCapacity\": null} ですが、 Scan は前述した通り、データ量増加でパフォーマンスが劣化します。Query を使ってみたいと思います。 12345678910111213141516$ aws dynamodb query \\ --table-name worriors \\ --key-condition-expression \"#CTGRY = :g\" \\ --filter-expression '#SCR &gt; :scr' \\ --projection-expression \"#NM\" \\ --expression-attribute-names '{ \"#CTGRY\": \"category\", \"#SCR\": \"score\", \"#NM\": \"name\" }' \\ --expression-attribute-values '{ \":g\": {\"S\": \"a\"}, \":scr\": {\"N\": \"70\"} }'An error occurred (ValidationException) when calling the Query operation: Query condition missed key schema element: id Key Schema で設定したパーティションキー id が指定されていないというエラーが出ます。 DynamoDB コンソール上でもクエリ検索時はパーティションキーの指定は必須です。 Query で score の比較だけで検索できないのでしょうか？ GSI を指定するGlobal Secondary Index を設定し、Query 実行時に指定することで意図した処理が可能です。 GSI は指定したキーで新たなテーブルを作るイメージです。 GSI のパーティションキー category を設定します。 12345678910111213141516171819202122232425262728293031323334$ aws dynamodb query \\ --table-name worriors \\ --index-name category-index \\ --key-condition-expression \"#CTGRY = :g\" \\ --filter-expression '#SCR &gt; :scr' \\ --projection-expression \"#NM\" \\ --expression-attribute-names '{ \"#CTGRY\": \"category\", \"#SCR\": \"score\", \"#NM\": \"name\" }' \\ --expression-attribute-values '{ \":g\": {\"S\": \"a\"}, \":scr\": {\"N\": \"70\"} }'// response{ \"Items\": [ { \"name\": { \"S\": \"maeda\" } }, { \"name\": { \"S\": \"tanahashi\" } } ], \"Count\": 2, \"ScannedCount\": 3, \"ConsumedCapacity\": null} 余談score を GSI のソートキーを指定したい所ですが、既にテーブルのキーとして利用しているので利用できません。 テーブル構築時に利用想定が読めない場合は極力キーの設定を絞っておく方が良さそうです。","link":"/2021/09/20/2021-09-21-dynamodb-query-better-than-scan/"},{"title":"AWS KMS キーを設定する意味を考える","text":"aws_rds_cluster の kms_key_id は Optional (任意) ですが、これを設定するとどんなメリットがあるか考えたいと思います。 ToC kms_key_id とは？ ストレージを暗号化する理由 廃棄証明にもなる まとめ 考えるに至った経緯 123456789101112resource &quot;aws_kms_key&quot; &quot;rds&quot; { enable_key_rotation = true}resource &quot;aws_kms_alias&quot; &quot;rds&quot; { name = &quot;alias/${local.prefix}-rds&quot; target_key_id = aws_kms_key.rds.key_id}resource &quot;aws_rds_cluster&quot; &quot;db&quot; { kms_key_id = aws_kms_key.rds.arn} kms_key_id とは？kms_key_id は terraform の公式ドキュメントにもある通り、ストレージを暗号化する際に必須です。 https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/rds_cluster#kms_key_id (Optional) The ARN for the KMS encryption key. When specifying kms_key_id, storage_encrypted needs to be set to true. ストレージを暗号化する理由ストレージを暗号化する理由は、仮に物理的にストレージを持ち出されても、設定している KMS キーがないと復号できず、データの保護ができます。 廃棄証明にもなる以下 AWS のドキュメントを読むとこんな一文があります。https://aws.amazon.com/jp/blogs/news/data_disposal/ 暗号化を活用したデータの保護と廃棄記録 ストレージを独自 KMS キーで暗号化しておくことで、その鍵へのアクセスをポリシーで制御・記録でき、さらに、鍵自体を廃棄することで、データへのアクセス自体をできなくさせる、ということです。 これにより廃棄証明ができます。 まとめ独自管理の KMS キーを設定しておく意味は以下理由がありました。 データ保護 ストレージを暗号化しストレージ持ち出しされても復号できない様にする アクセスの制御・記録 ポリシーでアクセス制御することで、アクセスの記録が取れる 廃棄証明 データへのアクセス自体を禁止できる為、廃棄証明になる 考えるに至った経緯KMS キーの設定について考えるきっかけとなったのは、顧客より「貴社 DB に個人情報が含まれる場合にサービスクローズ時に廃棄証明ができるか？」という質問があったことです。 廃棄証明って DB 自体消すじゃダメ？と思ったのですが、論理上消してるだけな気もするし、AWS には残りそうな気もする…と思い深掘りしてみるとデータ保護以外にも色々な意味合いがあることに気づきました。 以上参考になれば幸いです。","link":"/2021/09/28/2021-09-29-you-should-set-kms-key/"},{"title":"AWS Transfer for sftp + S3 で IP 制限付き sftp サーバ構築","text":"ToC 概要 SFTP 構築方法 比較 アーキテクチャ terraform で対応 Transfer for sftp サーバ構築 Transfer for sftp アクセス可能ユーザ作成 操作ログの追跡 アクセスに必要な情報の整理 総評 概要外部会社とのファイル共有で sftp サーバを構築することとなった。 先方が AWS を利用していれば、 IAM Role を渡して Assume Role で S3 に直接アップロードしていただくことができましたが、sftp を利用したい強い気持ちを感じ、 sftp サーバを構築することとなりました。 SFTP 構築方法 比較 AWS Transfer for SFTP + S3 Pros: 管理コストが低い Cons: 高い :dollar: ¥25,000/月〜 EC2 + s3fs + S3 Pros: 安い ¥6,836/月〜 Cons: 管理コストが高い EC2 定期メンテ AMI・ミドルウェア更新 リソースの利用コストこそ高いが、管理コストの低さから AWS Transfer for SFTP を採用することとしました。 エンジニアの採用コストより低い！ アーキテクチャ公開鍵認証で構築します。極力管理コストを下げ、セキュアにすべく公開鍵認証としました。 ※ パスワード認証は Lambda で対応するっぽい Route53 で AWS Transfer for SFTP エンドポイントの名前解決 VPC エンドポイント経由でアクセスする セキュリティグループで特定 IP のみ許可する SSH 公開鍵認証で AWS Transfer for SFTP にアクセス ユーザ毎に IAM Role or IAM Policy で権限を制限し、S3 へアクセス terraform で対応構築は terraform で実施しました。 Transfer for sftp サーバ構築12345678910111213141516171819202122232425262728resource &quot;aws_transfer_server&quot; &quot;this&quot; { # VPC エンドポイントを指定し、セキュリティグループで IP 制限する endpoint_type = &quot;VPC&quot; endpoint_details { vpc_id = aws_vpc.main.id subnet_ids = aws_subnet.transfer[*].id address_allocation_ids = aws_eip.transfer[*].id security_group_ids = [aws_security_group.sftp.id] } # FIPS 準拠でない最新のポリシーを指定 # 東京リージョンは FIPS 準拠対応していない security_policy_name = &quot;TransferSecurityPolicy-2020-06&quot; # NOTE: ロギング用の IAM Role を指定 logging_role = aws_iam_role.transfer_logging_access.arn tags = { # NOTE: サーバエンドポイントを Route53 レコードと紐づけるには以下のタグ指定が必要です。 # &quot;aws:transfer:customHostname&quot; = local.domain_sftp # &quot;aws:transfer:route53HostedZoneId&quot; = &quot;/hostedzone/${aws_route53_zone.main.zone_id}&quot; # # ですが、`aws:` で始まるタグキーは指定できず、 terraform では現状対応していない為、 # コンソール上で Route53 DNS エイリアスの設定をします。 # issue: https://github.com/hashicorp/terraform-provider-aws/issues/18077 }} endpoint_type = &quot;VPC&quot; を指定し endpoint_details で以下設定することで VPC エンドポイント経由でアクセスする様、設定できます。 12345678endpoint_type = &quot;VPC&quot;endpoint_details { vpc_id = aws_vpc.main.id subnet_ids = aws_subnet.transfer[*].id address_allocation_ids = aws_eip.transfer[*].id security_group_ids = [aws_security_group.sftp.id]} logging_role = aws_iam_role.transfer_logging_access.arn で CloudWatch Logs へログ出力する為の IAM Role を指定する必要があります。 注意点として、現時点でサーバエンドポイントの Route53 DNS エイリアスの指定を terraform-provider-aws がサポートしていません。 以下 Issue があります。aws_transfer_server custom hostname via alternate mechanism コード中に provider がサポートしていない旨を明記し、 AWS Console 上で設定しました。 Transfer for sftp アクセス可能ユーザ作成123456789101112131415161718192021222324252627282930locals { users_map = { # ユーザ名 = SSH 公開鍵 # 公開鍵なので、ベタ書きして問題ない &quot;kenzo.tanaka&quot; = &quot;ssh-rsa AAAAB3Nxxxxxxx=&quot; }}resource &quot;aws_transfer_user&quot; &quot;this&quot; { for_each = local.users_map server_id = aws_transfer_server.this.id user_name = each.key # 権限管理 role = aws_iam_role.this.arn # sftp 接続時のデフォルトのアクセス先を S3 のルートディレクトリとする # S3 をコンソールや `aws-cli` でオブジェクトのリストを見る際に見やすい為です。 home_directory_type = &quot;PATH&quot; home_directory = &quot;/${aws_s3_bucket.this.id}&quot;}resource &quot;aws_transfer_ssh_key&quot; &quot;this&quot; { for_each = local.users_map server_id = aws_transfer_server.this.id user_name = aws_transfer_user.this[each.key].user_name body = each.value # 公開鍵情報} ユーザ名と公開鍵を Transfer ユーザとしてサーバに登録しアクセス許可されます。 S3 への操作権限管理は IAM Role で実施しています。 12# 権限管理role = aws_iam_role.this.arn ホームディレクトリはどのユーザでも共通してルートパスにしています。個別にホームディレクトリを設定することも可能です。 12home_directory_type = &quot;PATH&quot;home_directory = &quot;/${aws_s3_bucket.this.id}&quot; 操作ログの追跡CloudWatch Logs /aws/transfer/SFTPサーバーID に操作ログが記録されます。 get, put, rm 等ログが記録されることを確認しています。 アクセスに必要な情報の整理接続希望者が提出する情報 接続元 IP セキュリティグループで許可する必要がある ユーザ名 ユーザー名は 3～100 文字にする必要があります。有効な文字は a～z、A～Z、0～9、アンダースコア、ハイフン、アットマーク、ピリオドです。ハイフン、アットマーク、ピリオドで始めることはできません。 SSH 公開鍵 サーバ管理者が提出する情報 サーバホスト名 総評やや高い気がしないでもないですが、マネージドサービスとして AWS にお任せできる部分が多く、管理コストの低い sftp サーバの構築ができました。 アップロードされたファイルは S3 Bucket へ蓄積されるのでオブジェクトサイズが肥大化→コスト増を防止する為にも Glacier へ移行する等、ライフサイクルルールの設定が必要と思います。 参考になれば幸いです。","link":"/2021/10/19/2021-10-20-aws-transfer-for-sftp/"},{"title":"curl で Datadog ユーザ ID リスト取得","text":"ToC 備忘録です。 Datadog ユーザをコンソール上で登録して、後追いで terraform 管理せねば！という時がありました。 後追いで terraform datadog_user で管理するには以下実行する必要があります。 1terraform import datadog_user.this &lt;ユーザ ID&gt; ユーザ ID ってどこに書いてあるんだ？！ ダッシュボード上からソースから眺めてもない。。 API で ユーザ ID を以下の様に取得できました。 12345$ curl -s -X GET \"https://api.datadoghq.com/api/v2/users\" -H \"Content-Type: application/json\" -H \"DD-API-KEY: xxx\" -H \"DD-APPLICATION-KEY: yyy\" | jq -r '.data[] | .id +\" \"+ .attributes.handle'// responsexxxxxxxx-yyyy-zzzz-aaaa-bbbbbbbbbbbb kenzo.tanaka@example.comxxxxxxxx-yyyy-zzzz-aaaa-bbbbbbbbbbbb hoge.moge@example.com ユーザ ID 長っ！","link":"/2021/10/19/2021-10-20-datadogapi-get-user-id/"},{"title":"ログ保存用 S3 Bucket の ACL で CloudFront や他 S3 Bucket のアクセスログを保存許可する","text":"ToC CloudFront Log Delivery Canonical User ID が data ソースで取れる様になった！ まとめ terraform 的には以下の様に設定します。 12345678910111213141516171819202122232425resource &quot;aws_s3_bucket&quot; &quot;logs&quot; { # NOTE: S3 logging を有効化する為、 S3 group Log Delivery に権限を付与する # https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html#grant-log-delivery-permissions-general grant { permissions = [ &quot;READ_ACP&quot;, &quot;WRITE&quot;, ] type = &quot;Group&quot; uri = &quot;http://acs.amazonaws.com/groups/s3/LogDelivery&quot; } # NOTE: CloudFront からログ保存できる様、 CloudFront Log Delivery Canonical User に権限を付与する grant { id = data.aws_cloudfront_log_delivery_canonical_user_id.current.id permissions = [&quot;FULL_CONTROL&quot;] type = &quot;CanonicalUser&quot; } # S3 Bucket 所有者に権限付与する grant { id = data.aws_canonical_user_id.current.id permissions = [&quot;FULL_CONTROL&quot;] type = &quot;CanonicalUser&quot; } CloudFront Log Delivery Canonical User ID が data ソースで取れる様になった！以下 issue を追っていたところ、対応されていました！https://github.com/hashicorp/terraform-provider-aws/issues/12512 これまで data ソースがなかった為、直接、文字列指定していたので、有り難い！ Data Source: aws_cloudfront_log_delivery_canonical_user_idhttps://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/cloudfront_log_delivery_canonical_user_id まとめ設定自体は知っていればさくっと終わりますが、知らないと、いざログを見ようとして気付くという落とし穴があります。 複数 AWS アカウントを terraform で管理している場合は、 module 化して展開するのも設定漏れを防ぐ為に有効です。 以上参考になれば幸いです。","link":"/2021/11/03/2021-11-04-cloudfront-logdelivery/"},{"title":"公開された S3 Objcet を探せ！","text":"ToC スクリプトで調査する CloudTrail を Athena で検索する AWS075: S3 Access block should restrict public bucket to limit accesshttps://tfsec.dev/docs/aws/s3/no-public-buckets/ tfsec でパブリックアクセスが制限されていない場合に指摘される様になりました。 terraform では以下の様に aws_s3_bucket_public_access_block リソースを利用することで対応できます。 123456789101112resource &quot;aws_s3_bucket&quot; &quot;this&quot; { ...}resource &quot;aws_s3_bucket_public_access_block&quot; &quot;this&quot; { bucket = aws_s3_bucket.this.id ignore_public_acls = true restrict_public_buckets = true block_public_acls = true block_public_policy = true} ですが、 tfsec で指摘されたので直ちに対応して良いか、というと勿論そうではありません。まず、現状のユーザ影響があるかどうかを調査する必要があります。 スクリプトで調査する2つ以上 permission がついている S3 Object を探索するスクリプトです。 12345678910#!/bin/bashfunction acl() { if $(aws s3api get-object-acl --bucket $1 --key \"$2\" | jq '.Grants | length != 1'); then echo $2 fi}export -f aclaws s3 ls s3://$BUCKET --recursive | awk '{print $4}' | xargs -P4 -I{} bash -c \"acl ${BUCKET} {}\" 通常、所有者のみアクセス権限がありますが、 public-read が付与されていると2つ以上になるという算段です。 1234567891011121314151617181920$ aws s3api get-object-acl --bucket tanaka.no.bucket --key t.txt | jq '.Grants'// result[ { \"Grantee\": { \"DisplayName\": \"tanaka+administer\", \"ID\": \"xxxxxxxxxxxxxxxxxxxxxxx\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"FULL_CONTROL\" }, { \"Grantee\": { \"Type\": \"Group\", \"URI\": \"http://acs.amazonaws.com/groups/global/AllUsers\" }, \"Permission\": \"READ\" }] コンソール上でわかりやすく、以下の様な設定になっていると検知できます。 但し、 public-read だけ付与されていると1つだけになってしまうので、このスクリプトでは検知できません。 CloudTrail を Athena で検索するCloudTrail を有効化し、 Athena 連携している場合、以下の様に検索が可能です。 12345SELECT *FROM cloudtrail_logsWHERE eventName = 'PutObject' AND eventsource = 's3.amazonaws.com' AND (requestParameters LIKE '%x-amz-acl%public-read%' OR requestParameters LIKE '%x-amz-acl%authenticated-read%') 結果 12eventTime: 2021-11-01T07:18:36ZrequestParameters: {... ,&quot;bucketName&quot;:&quot;tanaka.no.bucket&quot;, ..., &quot;x-amz-acl&quot;:&quot;public-read&quot;, ... ,&quot;key&quot;:&quot;t.txt&quot;, ...} ※ 上記は、一旦 S3 上にファイルをアップし、その後、公開設定した場合でも実行結果に含まれることを確認しています。 public-read のみならず、 authenticated-read も検索できます。 そして何より、データ量にもよりますが、スクリプトよりはるかに速いです。日付で3ヶ月以内くらいに絞ってみるとより早くなるのでそこは調整してください。 これでパブリックアクセスを許可する S3 Bucket が特定でき、安心して tfsec のパブリックアクセスのブロックが設定できる様になりました。 以上参考になれば幸いです。","link":"/2021/11/04/2021-11-05-find-public-s3-objects/"},{"title":"DIY 初心者の工具選び","text":"ToC 最近作ったもの 大まかな工程 1. 設計図を書く 2. 木材調達 3. 木材カット Black &amp; Decker ジグソー利用時の注意点 4. 木材を運ぶ 5. 研磨 makita オービットサンダー Black &amp; Decker サンダー スポンジ &amp; サンドシート 迷ったら 6. 着色 7. ニス塗り ニス塗り失敗した経験 8. 組み立て 食器棚 Before / After 端材で壁掛け 水平器は iPhone で まとめ DIY 初心者の自分がいくつか DIY をしてみて必要な工具や材料が何だったかを振り返ってみたいと思います。 これから始めるという方に幾分でも参考になる点があると幸いです。 もちろん作りたいものにもよるかと思いますので、今回作ったものを参考にします。 最近作ったもの食器棚 洗濯機上棚 大まかな工程完成までの大まかな工程です。 設計図を書く 木材調達 木材カット 木材を運ぶ 研磨 着色 ニス塗り 組み立て 各工程で必要だなと思ったこと、工具や材料について以下にまとめます。 1. 設計図を書く購入する木材の選定にも関わるのでざっくりでもまず設計図を書きました。 ミリ単位で書いた方が調整しやすかったです。 食器棚は電子レンジと電子ジャーを模した紙を貼ってみてイメージを深めました。 設計図を家族に見せ、完成時のイメージの齟齬を極力減らすことができたと思います。（イメージが微妙だったら反対されてたかも） 2. 木材調達 全ての必要な木材が分かって木材購入に出向いたわけではありません。 設計図を描いた後、何度かホームセンターに足を運び、設計図を満たしそうな木材を片っ端から記録しカット後に費用が一番安くなる様に選定しました。 3. 木材カット近所のロイヤルホームやスーパービバホーム等の木材加工の条件を確認しましたが、以下条件でカット OK でした。 基本直線のみ対応で、曲線は不可 カット時には 3mm 程バッファが必要 100mm 幅を 50mm でカットしたら、残り幅が 100-50-3 = 47 mm になる見込み 貸し出し可能な工房で研磨不可 粉が舞い、他のお客さんへご迷惑をお掛けするからとのこと。 以下木材を利用しました。 2x4材 パイン集成材 基本一直線のカットであれば、ホームセンターで安く（1カット10~15円）、丁寧にカットしてもらえます。 完成後に端材で棚を作ろうと思い立ったのですが、その際に端材をカットする為、ジグソーを使いました。 ジグソーは 100年の老舗 Black &amp; Decker のマルチツールを購入！ ブラックアンドデッカー EVO185E1 18Vリチウム コードレス マルチツール エキスパート セット &nbsp; 楽天市場でチェック Amazonでチェック 主にドリルドライバーが目的で、サンダーやジグソーはおまけ程度に考えていたのですが、 ¥20,000 台を切っており、DIY 初心者は手を出しやすい！というところで購入しました。 バッテリーが2つ入っていて、作業の切れ間がなくできるのも良いです。 Black &amp; Decker ジグソー利用時の注意点作業音がかなり大きいです。マンション住まいの為、フルパワーでの利用は気が引けます。ベランダはもちろん部屋の中でもガタガタ大きな音がします。その為、近所の河原に自転車で向かい作業しました。 不審者でないと思われたい。 作業場を選ぶ為、木材購入時にホームセンターの工房を借りることを検討するのが良さそうです。 4. 木材を運ぶ 車を持たない我が家はホームセンターのレンタルトラックで運びました。マンションの前で下ろして、マンションにある台車を借りて、部屋まで運びました。 ホームセンターで配送サービスもあり、「運ぶのは面倒だ」という場合は利用すると良いです。 今回の材料だと運べそうだったので、コストカットを優先しました。 5. 研磨この工程が他の何より重要でした。 研磨すると、表面のざらつきがなくなり、スッと指通りが良くなり、着色のノリが良いです。 完成後に実際に利用する際に水拭きする際にも引っ掛かりがなくなり、埃もつきにくくなります。 #120 → #240 → #320 → #800と徐々に細かくすると、より繊細になっていくのがわかります。 実際にどのくらいのツルツル感を求めるかにもよりますが、#120 → #400 でも自分の場合は満足するツルツル感は出せました。 以下は makita のオービットサンダーによるサンディングの説明ですが、サンディングする意味や仕方の指南が非常に分かりやすかったです。 以下、研磨で利用する工具についての比較です。 makita オービットサンダー マキタ(Makita) 充電式ランダムオービットサンダ (バッテリー・充電器付) BO180DRF &nbsp; 楽天市場でチェック Amazonでチェック メリット かなり綺麗に研磨できる Black &amp; Decker に比べ静音 吸引口があり、削った粉が飛びにくい。 飛ばないということはなく、細かい粉はどうしても飛ぶので室内は避けたい所 3段階のパワー調整 デメリット 高い ¥32,000 程度 初心者には必要な工具か判定し辛く、手出しづらい 3段階の一番下でも木の削り音が幾分出る Black &amp; Decker ほどではない Black &amp; Decker サンダー既出の Black &amp; Decker のサンダーです。 ブラックアンドデッカー EVO185E1 18Vリチウム コードレス マルチツール エキスパート セット &nbsp; 楽天市場でチェック Amazonでチェック メリット 安い ¥18,000 でドリルドライバーと共に 3 way で購入も可能 先が尖っているので makita オービットサンダーで削りにくい様な狭い箇所等での研磨に向いてる デメリット ハンドリングが難しい makita オービットサンダーに比べ、上下の動きが激しく、板から離れ易く、研磨効率が悪い印象。 作業音が大きい スポンジ &amp; サンドシート 松永トイシ FSマジックスポンジファイル &nbsp; 楽天市場でチェック Amazonでチェック メリット とにかく安い！ 静か 気を削る音だけで、機械音がない 集合住宅でも然程気にならない（我が家の場合） Black &amp; Decker サンダーより仕上がりもよい印象 デメリット やや時間がかかる とはいえ、オービットサンダーの倍ほど掛かる様な印象ではない 長くやっていると腕が疲れる DIY 初心者の自分にとってはまずこの方式で十分でした。 木の側面のざらつきを取る際や着色後の研磨にも向いていますし、電動工具とは別に細かい調整が効く為、必須のアイテムでした。 迷ったら まずマジックスポンジで研磨する レンタル工具で試してみる 有用性を感じ、長く使いそうなら買い 私自身は、着色で失敗した経緯（いきなり着色ニスだけ塗ってしまった汗）から makita オービットサンダーでリカバリできないかと考え、購入を決意しました。 DIY 初心者ですが、本当に購入して良かったと思います。 6. 着色 和信ペイント 水性ポアーステイン 抜群の着色と希釈自在 オーク 130ml &nbsp; 楽天市場でチェック Amazonでチェック 和信 水性ポアーステイン オーク色を選びました。 薄めに塗ると赤めの発色が良くなり、濃く塗ると濃い赤紫の様な色味になる 水性なので、水に溶かして濃度を変えられる 小さいサイズしかなくてそこだけ難点 和信ペイント 水性オイルステイン 木目鮮明・微臭・安全 オーク 300ml &nbsp; 楽天市場でチェック Amazonでチェック 同じ和信のオーク色でも 水性オイルステインは色味が全然違うので注意してください。 刷毛はホームセンターで売っていた水性ステインや水性ニスを塗る為、3本セットを購入 アサヒペン ペイント刷毛 お得用上級多用途用ハケ3本セット OTJ-3P &nbsp; 楽天市場でチェック Amazonでチェック 刷毛に水性ステインをたっぷりつけてしまうと、濃くなり過ぎてしまう為、あまり刷毛につけ過ぎず、水も控えつつ、程よく滑る感じで塗れると仕上がりがよかったです。 着色後も表面にプツプツとしたざらつきが出るので#400~ のサンドシートで研磨し、さらに重ね塗りしました。 研磨 → 着色 → 研磨 → 着色 → 研磨 という感じで 研磨で着色を挟むイメージです。 2回目以降の研磨は弱目にかけてます。擦り過ぎると色落ちしてしまうので、あくまで表面のぷつぷつを取る様にしました。 ここでの研磨は軽めにしたいのでスポンジ &amp; サンドシートは必要になります。 ターナー色彩 水性ペイント アイアンペイント アイアンブラック IR200009 200ml &nbsp; 楽天市場でチェック Amazonでチェック 基本 2x4材はアイアンペイントで着色しています。 子供たちと一緒に塗ってみました。5分だけ (TへT) 刷毛で塗ると、毛が抜けて付着し固まってしまう為、スポンジに付けて塗りました。 ターナー色彩 水性ペイント アイアンペイント マルチプライマー IR200901 200ml &nbsp; 楽天市場でチェック Amazonでチェック 金属部分やプラスチック部分はマルチプライマーを塗って、その上からアイアンペイントで着色します。 マルチプライマーを塗った箇所は糊がついている様な感じでベタベタとした感じになり、そこにアイアンペイント貼り合わせるイメージです。 以下動画が参考になりました。 7. ニス塗り 和信ペイント 水性ウレタンニス つや消しクリヤー 0.7L 屋内木部用 ウレタン樹脂配合 低臭・速乾 &nbsp; 楽天市場でチェック Amazonでチェック 和信水性ウレタンニスでニス塗りで仕上げました。 ニス塗り後も表面にプツプツできるので、また既出のスポンジ &amp; サンドシートで研磨します。 ニス塗り失敗した経験以前 DIY で watoco で着色だけして、ニスを塗らずに棚を作りましたが、水拭きすると埃が水分を吸収し固まってしまい、爪でカリカリしても取れないほどになってしまいました。 ↑ わかりづらいですが、埃でコーティングされてます。 これも研磨し直し、 和信水性ステインで着色し直し、水性ニスを塗るとツルツルに仕上がりました。 着色したらニスでコーティングの大事さを身をもって覚えました。 8. 組み立てドリル必須です。手の力だけでネジを木材に入れるのは不可能だと思います。 自分の場合は、コスパの良い Black &amp; Decker を選びました。 makita のドリルドライバーを利用したことがありますが、今回の利用用途ではそこまで差がわからなかったです。厳密には回転の速さとか円滑さ等のパフォーマンスの差があるのだと思いますが、作業に影響はなかったです。 マキタ ドリル ドライバー ブラシレスモーター 18V 充電式 MAKITA XFD13Z 緑 純正 &nbsp; 楽天市場でチェック Amazonでチェック 2x4材の固定には以下ジャッキを購入しました。 ウォリスト突っぱりジャッキ 黒 WAT-001 &nbsp; 楽天市場でチェック Amazonでチェック 高さ調整がネジでできるので、ドリルドライバーを使って容易にできます。 以上工程で完成しました。 食器棚 Before / After Before After 作業スペースを確保できる様にしました。片付けしやすい様に扉をなくし、オープンな棚にしました。 入れ替えする際に改めて、不要なものは捨てて、すっきりしました。これも家具リプレイス効果♪ 端材で壁掛け 端材で適当な長さにジグソーでカットし来年小学校の息子用の壁掛けを作りました。 平安伸銅工業 LABRICO DIY収納パーツ ナゲシレール ブラケット KXO-210 ホワイト 奥行1.1×高さ8×幅2cm ねじx8、ピンx12(予備x2) 2個入 &nbsp; 楽天市場でチェック Amazonでチェック 食器棚と同じ水性ステインで薄めに塗って明るく仕上がりました。 水平器は iPhone でiPhone の計測アプリでデフォルトで入っていたものを使いました。https://support.apple.com/ja-jp/guide/iphone/iphbd435673d/ios シビアに出してくれるので助かります。 まとめDIY 初心者として必要な電動工具は以下で始めるのがコスト的にも抑えられて良いと思いました。 ドリルドライバー サンドスポンジとサンドシート これから長くやるか分からないし、でもちょっと手を出してみたいなという時にはスモールスタートでまずはレンタル工具で始めるのも良いと思います。 以上参考になれば幸いです。","link":"/2021/11/10/2021-11-11-diy-for-beginner/"},{"title":"no-cache, no-store の違い","text":"ToC no-cache の仕組み 使い所 利用時の注意点 no-store 使い所 厳密にキャッシュさせたくない場合 手厚い防御をしている理由 総評 参考 キャッシュ関連のディレクティブは多々ありますが、その中で間違えやすい no-cache, no-store を取り上げます。 no-cache の仕組み「キャッシュしない」ではない！キャッシュが有効期限内であっても毎回キャッシュが最新かどうか検証します。 タワレコよろしく、No Validation, No Cacheと覚えましょう。 使い所更新のあるコンテンツに対して、更新に追随しつつもキャッシュを利用したい場合に有効です。 利用時の注意点ブラウザバック時に (disk cache) が返ってしまい、コンテンツが更新されない場合があります。 参考: Chrome の Back button を押した際に「意図しない Cache」が利用されて、期待と違うページが表示される問題について調査した サポートするブラウザの仕様によって意図しない挙動をする場合があるので、その場合はキャッシュをそもそもさせない no-store が対策の一手。 no-storeキャッシュを保存せず、毎回 Origin にリクエストします。 使い所その命名である「no-store = ストアしない」という意味合いからキャッシュストレージの容量を奪わない様にする場合に利用します。 CloudFront 等 CDN サービスではキャッシュストレージ容量への懸念が小さいですが、CDN を DIY するときにはストレージ容量は有限なので懸念がある為です。 厳密にキャッシュさせたくない場合1Cache-Control: private, no-store, no-cache, must-revalidate private: Proxy や CDN の経路上のキャッシュをしない no-store: キャッシュを保存しない no-cache: 再検証なしではキャッシュしない must-revalidate: キャッシュ期限切れ時に再検証を強制 Stale (期限切れ) キャッシュを利用させない Origin がダウンしていたら 504 Gateway Timeout を返す max-age, no-cache と共存不可 手厚い防御をしている理由Proxy, CDN の互換性を軽減する為です。 総評キャッシュの設定を細かく制御する際はブラウザ, Proxy, CDN の互換性を勘案し検証を細かく実施する必要があることがわかりました。 参考 Web配信の技術―HTTPキャッシュ・リバースプロキシ・CDNを活用する &nbsp; 楽天市場でチェック Amazonでチェック 同僚に勧められた本！ Web における配信の最適化・高速化について解説いただいてます。 自分自身が AWS CloudFront 使っていた程度でしたがVarnish の設定がそもそも分かりやすく記述されており入りやすかったです。 キャッシュに対しての理解が非常に深まりました。 この場を借りて感謝申し上げます。 テストをどの様に行うか、開発時の本番環境の再現性等々、まだまだ知りたいことがいっぱいです。 続編求！","link":"/2021/12/29/2021-12-30-web-delivery/"},{"title":"warning: reattach-to-user-namespace: unsupported new OS, trying as if it were 10.10","text":"macOS を Catalina から BigSur にアップグレードしたところ、ターミナル起動で以下 warning が見られる様になりました。 1warning: reattach-to-user-namespace: unsupported new OS, trying as if it were 10.10 シンプルにアップグレードしたら解決しました。 1234$ brew upgrade reattach-to-user-namespaceRunning `brew update --preinstall`...... という備忘録でした。 以上、参考になれば幸いです。","link":"/2022/01/03/2022-01-04-rettach-to-user-namespace-unsupported-new-os/"},{"title":"Raspberry PI zero で CO2 濃度測定 &amp; アラートを LINE 通知","text":"ToC 設計図 要点 プロット先を Mackerel にした理由 事前に購入したもの 価格について Raspberry PI OS Ansible で設定 各 Role について Mackerel のカスタムメトリクス Mackerel 監視ルールの設定 工夫点 Mackerel アラート通知先設定 CO2 濃度を測定してみて 総評 リモートワークで部屋に閉じこもることが増え、何気ない眠気に襲われることがあり部屋の CO2 濃度を調査すべく、Raspberry PI で計測しました。 設計図 ブレッドボードを使わず、ピンに直で結線しています。 要点 MH-Z19 CO2 濃度測定 モニター CO2 濃度（リアルタイム ）表示 Raspberry PI MH-Z19 で測定した CO2 濃度を Mackerel にプロット モニターに CO2 濃度表示 Mackerel CO2 濃度をカスタムメトリクスとしてグラフ化 閾値を超えるとアラート発火 （LINE 通知） プロット先を Mackerel にした理由自前サーバを Raspberry PI に立てても良いかなと思いましたが、CO2 濃度が閾値を超えるとアラートを発火させたかったのでその辺まで面倒を見たくないなという気持ちから避けました。 以前 Prometheus で監視したこともありましたが、結構リソース食うので極力リソースを使用したくない意図もありました。 尚、 Datadog は Raspberry PI 用の Agent は明確にサポートしておらず、近しい Agent タイプを利用する必要があります。ですが、何度も失敗しサポートに問い合わせたところ、ログ送って欲しいとなって面倒になってしまいました。 インストールが簡単、監視とカスタムメトリクス、アラート設定まで無料で事足りてしまう Mackerel が心強く、採用に至りました。 念の為、 Mackerel の回し者ではないことは断言しておきます。 事前に購入したもの Raspberry PI zero W 特に種類は問わずです。pico もそのうち試したい！ Raspberry PI zero W &nbsp; 楽天市場でチェック Amazonでチェック MH-Z19C CO2 濃度測定モジュールです。 Raspberry PI や Arduino と接続でき利用できます。 MH-Z19C CO2センサー &nbsp; 楽天市場でチェック Amazonでチェック ジャンパーワイヤー 今回、8 本利用するのみですが、今後も利用する可能性が高そうであれば、買っといて損なしです。 ブレッドボード・ジャンパーワイヤー（メス-メス）（20cm）40本 &nbsp; 楽天市場でチェック Amazonでチェック 0.91 インチ 128 * 32 OLED ディスプレイ 0.91インチ128 * 32 OLEDディスプレイ &nbsp; 楽天市場でチェック Amazonでチェック 価格についてamazon や楽天のリンクを貼りましたが、秋月電子通商やスイッチサイエンスの方が安いかもしれないです。 時勢によるかと思いますが、専門店の方が安く購入できる印象があります。 安さを求めるなら ebay も良いです。但し、海外サイトで配送まで時間がかかる可能性があります。興奮を抑えきれず、すぐ実装したいという場合に不向きです。 実際に MH-Z19C は ebay で購入しましたが、届くまで 1 ヶ月くらい掛かりましたが、特に問題なく利用できています。 Raspberry PI OS12$ cat /etc/issueRaspbian GNU/Linux 10 \\n \\l OS インストールは数多く紹介されておりますので説明は譲ります。初めての方は最後に紹介している Raspberry PI で学ぶ電子工作がおすすめです。 Ansible で設定https://github.com/kenzo0107/raspi-ansible を利用します。 raspberrypi.yml の nodejs は今回利用しないので削除しても構いません。 mackerel api key を秘匿情報として管理していますので更新してください。https://github.com/kenzo0107/raspi-ansible/blob/master/roles/monitoring/vars/secret.yml 1mackerel_apikey=xxxx 各 Role について今回のメインである機能について補足しておきます。 monitoring: mackerel agent インストールし起動 co2sensor: 主に MH-Z19 からデータ取得する設定 MH-Z19 からデータ取得する為に UART を有効化しシリアル通信できる様にします。 I2C はデフォルトで有効化されていた為、Ansible の設定に含んでいません。 もし有効化されていない様であれば /boot/confit.txt に dtparam=i2c_arm=on 追記してください。 python モジュール mh_z19 インストールし、 CO2 濃度を python モジュールで取得できる様にする mackerel-agent に CO2 濃度値をカスタムメトリクスでプロットする設定追加 co2lcd: OLED ディスプレイに CO2 濃度を表示する設定 ディスプレイに表示するためのモジュールをインストール ディスプレイに CO2 濃度表示する python スクリプトを追加 上記スクリプトを systemd 登録し daemon 化 スクリプトは MH-Z19 の値が変更された場合のみディプレイへの表示更新を実施するようにしています。 Mackerel のカスタムメトリクス 無料版なので 1 日だけのメトリクス保存となりますが、おおよそ一日の動きがわかれば特段問題はないです。 季節性や数日間の比較が見たくなったらアップグレードを検討します。 Mackerel 監視ルールの設定 cusotm.co2.raspberrypi というカスタムメトリクス名はhttps://github.com/kenzo0107/raspi-ansible/blob/master/roles/co2sensor/files/mackerel-co2monitoring.sh#L3 でメトリクス名を co2.raspberrypi としている為です。 以下の様に閾値を設定しました。 warn &gt; 1200 ppm critical &gt; 1500 ppm 工夫点 一時的な上昇で通知するとノイズとなるアラートが多かった為、5 回連続発生した場合のみとしました。 たまたま深いため息を MH-Z19 に吹きかけてアラートが発火してしまったことがあった為です。 ため息したら CRITICAL と教えられる様なディストピアは頂けません。 Mackerel アラート通知先設定Mackerel は様々なプラットフォームをサポートしています。プライベートな通知を LINE 通知にまとめていた個人的事情で LINE 通知を採用しました。 プライベート Slack もありますが、業務の連絡と見間違うことがあったので避けておきました。 CO2 濃度を測定してみてCO2 濃度上昇の起因は以下が多かったです。 複数人が一部屋に集まる MH-Z19 に近づいて息をしてしまう 息が吹きかからないような場所に設置することで回避できそう 5 時間程度、同じ部屋からほぼほぼ動かない 基本ドア閉めっぱなし 意外とエアコンで暖房をつけても思ったほど上昇することはなかったです。 ドアを開けて放置するだけでも割と落ち着きます。 冬場で窓を開けるのを避けたい時はドアを開けての換気でも効果がある印象です。 逆によく眠れない時は CO2 濃度を高めると眠りやすいのでは？とも思うきっかけにもなりました。 布団をかぶって寝る なるべく集まって寝る 総評シリアル通信や I2C という聞き慣れない箇所は以下の本でラジコンを作ってたので割とすっと入れました。 以下は Raspberry PI で電子工作を始める初学者にとってうってつけの良書です。 Raspberry PI で学ぶ電子工作 &nbsp; 楽天市場でチェック Amazonでチェック ジャンパワイヤーが剥き出しで子供が触って外れることもある為、ケースも自作してみようと思います ♪ 以上、参考になれば幸いです。","link":"/2022/01/05/2022-01-06-raspberrypi-co2-monitoring/"},{"title":"fluentbit JSON のネストしたキーの取得方法","text":"ToC キー log の中身だけ欲しい場合 キー log の中の b の中身だけ欲しい場合 キー log の b のリストの中身だけ欲しい場合 その 2 lua スクリプトで対応 まとめ ECS Service のコンテナログからネストされたキーの取得をどのように対応したかをまとめました。 実施内容は https://github.com/kenzo0107/sample-fluentbit-get-nested-key にてまとめています。 キー log の中身だけ欲しい場合 ログ 123456{ \"log\": { \"a\": \"1\", \"b\": \"2\" }} sample1.conf 12345678$ docker run --rm -it \\ -v $(PWD)/sample1.conf:/fluent-bit/etc/sample.conf \\ amazon/aws-for-fluent-bit:2.23.0 /fluent-bit/bin/fluent-bit \\ -c /fluent-bit/etc/sample.conf...[0] *-firelens-*: [1648191581.185935500, {\"a\"=&gt;\"1\", \"b\"=&gt;\"2\"}] 抽出できました。 キー log の中の b の中身だけ欲しい場合12345678{ \"log\": { \"a\": \"1\", \"b\": { \"c\": \"2\" } }} sample2.conf 12345678$ docker run --rm -it \\ -v $(PWD)/sample2.conf:/fluent-bit/etc/sample.conf \\ amazon/aws-for-fluent-bit:2.23.0 /fluent-bit/bin/fluent-bit \\ -c /fluent-bit/etc/sample.conf...[0] *-firelens-*: [1648192323.237149000, {\"c\"=&gt;\"2\"}] 抽出できました キー log の b のリストの中身だけ欲しい場合 その 2 ログ 12345678910{ \"log\": { \"a\": \"1\", \"b\": [ { \"c\": \"2\" } ] }} sample3.conf 123456789$ docker run --rm -it \\ -v $(PWD)/sample3.conf:/fluent-bit/etc/sample.conf \\ -v $(PWD)/test.lua:/fluent-bit/etc/test.lua \\ amazon/aws-for-fluent-bit:2.23.0 /fluent-bit/bin/fluent-bit \\ -c /fluent-bit/etc/sample.conf...[filter:nest:nest.1] Value of key 'b' is not a map. Will not attempt to lift from here b は map 型でなく nest では抽出できませんでした。 lua スクリプトで対応以下 lua スクリプトをかませて b キーの中身を取得します。 test.lua 1234567function cb_split(tag, timestamp, record) if record['b'] ~= nil then return 2, timestamp, record['b'] else return 2, timestamp, record endend sample4.conf 12345[Filter] Name lua Match *-firelens-* script test.lua call cb_split 123456789$ docker run --rm -it \\ -v $(PWD)/sample4.conf:/fluent-bit/etc/sample.conf \\ -v $(PWD)/test.lua:/fluent-bit/etc/test.lua \\ amazon/aws-for-fluent-bit:2.23.0 /fluent-bit/bin/fluent-bit \\ -c /fluent-bit/etc/sample.conf...[0] *-firelens-*: [1648192853.650025200, {\"c\"=&gt;\"2\"}] 抽出できました。 まとめValue of key 'xxx' is not a map. Will not attempt to lift from here エラーについてfluentbit の公式マニュアルにある Lua script で非常にシンプルなスクリプトで対応できました。 以上参考になれば幸いです。","link":"/2022/03/24/2022-03-25-fluentbit-get-key-from-nested-list/"},{"title":"GitHub Actions で特定ブランチが削除された場合に実行する","text":"ToC 結論 on: delete と if でブランチ名指定 デフォルトブランチに push して初めて on.delete は利用できる まとめ GitHub Actions で特定ブランチが削除された場合にトリガーし実行する設定を試してみたのでまとめます。 以下設定サンプルです。https://github.com/kenzo0107/tutorial-delete-event-on-github-actions/blob/develop/.github/workflows/delete.yml 結論以下のような設定で実装しました。 .github/workflows/delete.yml 12345678910on: deletejobs: run: if: ${{ (github.event.ref_type == 'branch' &amp;&amp; startsWith(github.event.ref, 'tmp/')) }} runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - run: echo \"delete event ${{ github.event.ref }}\" 以下ポイントをまとめます。 on: delete と if でブランチ名指定ブランチやタグが削除された際にトリガーされます。 on.delete は 以下 on.push の様に branches によるブランチのフィルターは現状利用できません。 1234on: push: branches: - tmp/** その為、 on.delete で削除されたものが branch であり、且つ、その branch 名が tmp/ で始まる (ex. tmp/dummy) 場合のみ以下実行される設定としています。 123jobs: run: if: ${{ (github.event.ref_type == 'branch' &amp;&amp; startsWith(github.event.ref, 'tmp/')) }} 上記設定の為、毎回ブランチ削除時 (merge して削除等）に delete トリガーが実行されますが、実質 1 秒程度で停止されました。 ちりも積もれば…となるのでこれを避けたい場合は GitHub の Webhook で Lambda の無料枠使えれば…という感じで根本的な対策がないという印象です。 on.delete に branches フィルタ希望です（切に） デフォルトブランチに push して初めて on.delete は利用できるon.push は任意のブランチで初めて push した場合でもトリガーできます。 on.delete はデフォルトブランチに push して初めて利用できます。なかなかトリガーされないなーと焦ってたら公式ドキュメントにちゃんと載ってました。 https://docs.github.com/ja/actions/using-workflows/events-that-trigger-workflows#delete ノート: このイベントは、ワークフローファイルがデフォルトブランチにある場合にのみワークフローの実行をトリガーします。 まとめ on.delete では branches フィルターが利用できない on.delete はデフォルトブランチに利用するワークフローファイルがないとトリガーしない 以上参考になれば幸いです。","link":"/2022/04/05/2022-04-06-github-actions-delete-event/"},{"title":"おうち k8s 構築","text":"ToC 目的 購入したものリスト Raspberry Pi Imager で OS 書き込み cgroup の有効化 swap 無効化 IP 固定 Docker インストール Cgroup Driver が systemd を使用するように設定されていることを確認 kubeadm インストール iptables がブリッジを通過するトラフィックを処理できるようにする iptables が nftables バックエンドを使用しないようにする kubeadm、kubelet、kubectl のインストール Kubernetes クラスター構築 worker ノード登録 token の期限が切れたら クラスタで worker node が登録されてるか確認する label 付けする ローカルのマシンで kubectl で操作できる様にする Metal LB インストール Raspberry PI 以外のマシンからアクセスできない場合 総評 参考 Raspberry PI で Kubernetes クラスタを構築しました。 会社の「テックサポート制度」により、 Raspberry PI がサポート対象となり、遺憾無くお金を使わせてもらうことができる様になりました。その甲斐あって、兼ねてより家で k8s cluster を飼う、というエンジニア冥利に尽きる所作を味わいたくチャレンジさせていただきました。 普段は AWS で Fargate を利用するケースが多く、 kubernetes を利用するシーンがなく、興味もあり、学びを広げたい意図があります。 目的Raspberry PI のようなベアメタル環境で OS をインストールし k8s 関連パッケージを構築し、構築に必要な大まかな流れを理解する、です。 以下を実施していきます。 Raspberry PI OS インストール kubernetes cluster 構築 Metal LB 構築 非常に学びとハマりポイントが多かったので、以下に記していきたいと思います。 購入したものリスト2022 年 4 月下旬、Raspberry PI 単体でなく、スターターキットでの取り扱いが多かったです。スターターキットは単体に比べやや高くなりますが、その辺は会社のサポート制度の力を存分にお借りしました ♪ Raspberry PI 4B 4GB スターターキット &nbsp; 楽天市場でチェック Amazonでチェック LAN ケーブル CAT6 フラット ホワイト 5本 0.15m &nbsp; 楽天市場でチェック Amazonでチェック エレコム スイッチングハブ ギガビット 5ポート &nbsp; 楽天市場でチェック Amazonでチェック GeeekPi Raspberry Pi4クラスターケース冷却ファンとRaspberryPi4ヒートシンク付きRaspberryPi4ケースアクリルケース &nbsp; 楽天市場でチェック Amazonでチェック Raspberry Pi Imager で OS 書き込み 2022-04-26 時点で最新の Raspberry PI OS Lite (32-bit) Bullseye を選択しました。今回の要件に GUI は不要で極力軽めのイメージを利用したかった為です。 設定でホスト名や Wifi の設定をしておくと後が楽です。 SD カードにイメージを書き込みし、 Raspberry PI に差し込み、起動します。 cgroup の有効化Docker を利用すべく、 cgroup を有効化します。 123$ sudo nano /boot/cmdline.txtcgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1 nano or vi が入ってたのですが、 vim 使いたい場合はこちら 参考 cgroup への理解は以下参考になりました。 Kubernetes で cgroup がどう利用されているか - VA Linux エンジニアブログ cgroup とは、Linux カーネルの機能の1つであり、プロセスやスレッドが利用するリソースの制限や分離を行うための機能で、Linux コンテナの根幹を成す技術の1つでもあります。本記事では、Kubernetes のいくつかの機能を例に挙げ、cgroup がどう利用されている… swap 無効化123$ sudo swapoff --all$ sudo systemctl stop dphys-swapfile$ sudo systemctl disable dphys-swapfile swap を無効化する理由は公式ドキュメントで言及されています。 https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/_print/#始める前に Swapがオフであること。kubeletが正常に動作するためにはswapは必ずオフでなければなりません。 IP 固定ルーターで IP 固定しておくと再起動時に変更なく楽です。 ルーターでなく、 Raspberry PI 側で /etc/dhcpcd.conf を編集し固定する方法は以下参考になります。Raspberry Pi の IP アドレスを固定にするには？ ここで一旦 reboot し諸々を反映しておきます。 1$ sudo reboot Docker インストールCRI のインストール を参考にインストールしました。 1234567891011121314151617// パッケージのリポジトリ情報更新時に必要な公開鍵を取得。ないと GPG error が発生$ curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg// armhf debian 用の docker 安定版のリポジトリを登録$ echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list// リポジトリ更新$ sudo apt-get update// docker インストール$ sudo apt-get -y install docker-ce docker-ce-cli containerd.io$ sudo systemctl enable docker// pi ユーザを docker グループに追加し docker を操作できる様にする$ sudo usermod pi -aG docker Cgroup Driver が systemd を使用するように設定されていることを確認1234$ sudo docker info | grep Cgroup Cgroup Driver: systemd Cgroup Version: 2 以下公式で systemd を使用を推奨しています。 コンテナランタイムと kubelet が cgroup ドライバーとして systemd を使用するように設定を変更することでシステムは安定します。 以下の Docker 設定の native.cgroupdriver=systemd オプションに注意してください。 kubeadm インストール公式に沿って以下実行していきます。 iptables がブリッジを通過するトラフィックを処理できるようにする公式ドキュメント参考 br_netfilter がロードされているか確認する 12345$ lsmod | grep br_netfilterbr_netfilter 32768 0bridge 180224 1 br_netfilteripv6 520192 28 br_netfilter,bridge 何も表示されない場合、 br_netfilter がロードされていない為、以下実行し明示的にロードしておきます。 1$ modprobe br_netfilter 12345$ cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF$ sudo sysctl --system iptables が nftables バックエンドを使用しないようにする公式 nftables バックエンドは現在の kubeadm パッケージと互換性がありません。(ファイアウォールルールが重複し、kube-proxy を破壊するためです。) 公式の説明にある通り、 iptables が nftables を使うことで kubernetes が正常に動作しないことがある為、 iptables をレガシーバージョンに切り替えます。 12345678// レガシーバイナリがインストールされていることを確認してください$ sudo apt-get install -y iptables arptables ebtables// レガシーバージョンに切り替えてください。$ sudo update-alternatives --set iptables /usr/sbin/iptables-legacy$ sudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy$ sudo update-alternatives --set arptables /usr/sbin/arptables-legacy$ sudo update-alternatives --set ebtables /usr/sbin/ebtables-legacy 参考: nftables 入門 kubeadm、kubelet、kubectl のインストールいよいよ kubeadm インストールです。 公式 2022/04/30 時点の最新バージョン 1.23.6 では kubelet が起動失敗するエラーが発生した為、バージョンは 1.22 系を選択します。 123456789101112$ sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https curl$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -$ cat &lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.listdeb https://apt.kubernetes.io/ kubernetes-xenial mainEOF$ sudo apt-get update// 1.22 系をインストール$ sudo apt-get install -y kubelet=1.22.7-00 kubeadm=1.22.7-00 kubectl=1.22.7-00// バージョン固定$ sudo apt-mark hold kubelet kubeadm kubectl Kubernetes クラスター構築公式 1234567891011121314151617181920212223242526272829// flannel をクラスター初期化処理を実装すべく 10.244.0.0/16 を指定している// see: https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml$ sudo kubeadm init --pod-network-cidr=10.244.0.0/16...// 最後の1行をコピーしておくkubeadm join &lt;master node ip&gt;:6443 --token yyyy \\ --discovery-token-ca-cert-hash sha256:xxxxxxxx// 上記作成時に出力されるクラスター開始時の設定$ mkdir -p $HOME/.kube$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ sudo chown $(id -u):$(id -g) $HOME/.kube/config// flannel 構築$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml// 起動してく様子がわかる♪$ kubectl get pod --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-78fcd69978-sv52p 0/1 Pending 0 111skube-system coredns-78fcd69978-t5glm 0/1 Pending 0 111skube-system etcd-pikube01 1/1 Running 0 2mkube-system kube-apiserver-pikube01 1/1 Running 0 2m4skube-system kube-controller-manager-pikube01 1/1 Running 0 2mkube-system kube-flannel-ds-w2bqt 0/1 Init:0/2 0 9skube-system kube-proxy-kpm8w 1/1 Running 0 111skube-system kube-scheduler-pikube01 1/1 Running 0 2m flannel はコンテナの相互疎通等ネットワーク構築に有用で k8s との相性がよいです。 worker ノード登録master node でクラスター作成時に出力されたコマンドを実行します。以下 worker node で実施します。 123456789$ sudo kubeadm join &lt;master node ip&gt;:6443 --token xxx \\ --discovery-token-ca-cert-hash sha256:yyy...This node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.* The Kubelet was informed of the new secure connection details.Run 'kubectl get nodes' on the control-plane to see this node join the cluster. token は期限付きなのでご注意ください。 token の期限は master node で確認できます。 1234master$ kubeadm token listTOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPSxxx 23h 2022-04-28T13:12:39Z authentication,signing &lt;none&gt; system:bootstrappers:kubeadm:default-node-token token の期限が切れたらmaster node で再発行してください。トークンを再発行し、且つ、worker node で kubeadm join する為のコマンドを出力してくれます。 1master$ kubeadm token create --print-join-command クラスタで worker node が登録されてるか確認する123456master$ kubectl get nodesNAME STATUS ROLES AGE VERSIONpikube01 Ready control-plane,master 32h v1.22.7pikube02 Ready &lt;none&gt; 32m v1.22.7pikube03 NotReady &lt;none&gt; 18s v1.22.7 label 付けする12master$ kubectl label node pikube02 node-role.kubernetes.io/worker=master$ kubectl label node pikube03 node-role.kubernetes.io/worker= 再度 node 一覧を表示すると ROLES にラベル付けされているのが確認できます。 123456$ kubectl get nodesNAME STATUS ROLES AGE VERSIONpikube01 Ready control-plane,master 32h v1.22.7pikube02 Ready worker 39m v1.22.7pikube03 Ready worker 7m8s v1.22.7 ローカルのマシンで kubectl で操作できる様にする1234567// 出力結果をコピーするmaster$ kubectl config view --rawmacOS$ vi ~/.kube/config// 上記コピーを貼り付け保存macOS$ kubectl get nodes Metal LB インストール参考: 【手順あり】MetalLB の使い方から動きまで解説します MetalLB &gt; Installation Step by Step slow guide — Kubernetes Cluster on Raspberry Pi 4B — Part 3 123// 全インターフェースで IPv4 パケットの転送が有効化する$ sudo sysctl net.ipv4.conf.all.forwarding=1$ sudo iptables -P FORWARD ACCEPT MetalLB &gt; Installation にある設定通りに進めます。 1234567891011$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/namespace.yaml$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/metallb.yaml// metallb 関連の pod の起動確認$ kubectl get -n metallb-system podsNAME READY STATUS RESTARTS AGEcontroller-66445f859d-qg8cz 1/1 Running 0 30sspeaker-bzzzc 1/1 Running 0 30sspeaker-vbhdf 1/1 Running 0 30sspeaker-vslj8 1/1 Running 0 30s addresses: 192.168.11.200-192.168.11.220 は DHCP で取得可能なレンジを指定します。 1234567891011121314151617// metallb を layer2 モードで起動$ cat &lt;EOF&gt; metallb-config.yamlapiVersion: v1kind: ConfigMapmetadata: namespace: metallb-system name: configdata: config: | address-pools: - name: default protocol: layer2 addresses: - 192.168.11.200-192.168.11.220EOF$ kubectl apply -f metallb-config.yaml nginx を type: LoadBalancer でデプロイし、 metallb が IP を割り当てていることを確認します。 1234567891011121314151617181920212223242526272829303132333435$ cat &lt;EOF&gt; nginx.deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploymentspec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80---apiVersion: v1kind: Servicemetadata: name: nginx-servicespec: selector: app: nginx ports: - port: 80 targetPort: 80 type: LoadBalancerEOF$ kubectl apply -f nginx.deployment.yml 割り当てられた IP が 192.168.11.200 となっており、外部からアクセスできることを確認します。 12345$ kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 148mnginx-service LoadBalancer 10.109.50.62 192.168.11.200 80:31270/TCP 57m 123$ curl 192.168.11.200// Welcome to nginx! が表示される Raspberry PI 以外のマシンからアクセスできない場合自身の環境でRaspberry PI の設定をしている MacOS から EXTERNAL-IP に nginx 起動直後はアクセスできましたが、数分後、アクセスできなくなる事象が発生しました。 以下参考に解決しました。 参考: LoadBalancer using Metallb on bare metal RPI cluster not working after installation MetalLB layer2 モードは、プロミスキャスモードが有効でない限り、ブロードキャストパケットを受信しません。そのため、以下ブロードキャストパケットを受信できる様にすることでmacOS –&gt; metalLB の疎通が確認できました。 1$ sudo ifconfig wlan0 promisc promisc: “promiscuous” で「見境のない」という意味で全ての通信を読み込むモードにする、という意味です。 サーバ再起動で消えてしまう設定なので crontab に設定しておくと良い。 1234$ sudo crontab -e// 以下最終行に追記@reboot sudo ifconfig wlan0 promisc 総評ハマりポイントは以下でした。 kubeadm, kubelet が最新 1.23 系で動作せず マイナーバージョン単位でダウングレードし対応 MetalLB の吐き出す External IP に接続できなかった プロミスキャスモードを有効化することで対応 今後は以下に焦点を当てつつ、実際にサービスを作っていきます ♪ CI/CD 監視 以上参考になれば幸いです。 参考https://qiita.com/reireias/items/0d87de18f43f27a8ed9b","link":"/2022/05/05/2022-05-06-ouchi-kubernetes/"},{"title":"Raspberry PI OS で vim インストール","text":"備忘録です。 Raspberry PI OS (32-bit) Bullseye で vim インストールした際のメモです。 12sudo apt-get --purge remove vim-common vim-tinysudo apt-get install vim","link":"/2022/05/08/2022-05-09-install_vim_on_raspberrypi_os/"},{"title":"tfsec aws-vpc-add-description-to-security-group 対応","text":"ToC aws-cli でセキュリティグループの description 更新はできない AWS コンソール上でも変更はできない かくなる上は terraform で AWS Security Group リソースは以下の場合、 aws-vpc-add-description-to-security-group ルールで違反が指摘されます。 description がない description = &quot;Managed by Terraform&quot; description を変更すればルールを回避できますが、その際にセキュリティグループが再作成となります。 123456$ terraform plan... # aws_security_group.this must be replace-/+ resource &quot;aws_security_group&quot; &quot;this&quot; { tfsec:ignore:aws-vpc-add-description-to-security-group で暫定的に回避することは可能です。 123resource &quot;aws_security_group&quot; &quot;this&quot; { description = &quot;Managed by Terraform&quot; # tfsec:ignore:aws-vpc-add-description-to-security-group 変更時に再作成となる為} aws-cli でセキュリティグループの description 更新はできない2022-05-13 時点、aws-cli でセキュリティグループの description の更新コマンドはありませんでした。 ※セキュリティグループルールの description 更新コマンドはあります。 時折、 terraform だと再作成になるが、 aws-cli であれば更新できるケースがありますが、セキュリティグループの description は無理でした。 AWS コンソール上でも変更はできないAWS コンソール上で変更ができませんでした。 かくなる上はterraform で既存リソースのコピーを作成しアタッチし直すのが良さそうです。 12345678910111213141516resource &quot;aws_security_group&quot; &quot;this&quot; { description = &quot;Managed by Terraform&quot;}# 別途作成するresource &quot;aws_security_group&quot; &quot;this_v2&quot; { description = &quot;&quot;}resource &quot;aws_lb&quot; &quot;app&quot; { name = &quot;${local.prefix}-app-lb&quot; security_groups = [ aws_security_group.this.id, aws_security_group.this_v2.id, # 追加 ] AWS コンソールのセキュリティグループを選択し Actions &gt; Copy to new security group をクリックする手と同じです。 ただ、対応コストとリスクを鑑みると、優先順位が高くなく、セキュリティグループルールの description の変更は可能なので、そちらを適宜対応していくのが良いと感じました。 セキュリティグループリソース作成時に意識できる様にしていきたい所です。 以上参考になれば幸いです。","link":"/2022/05/12/2022-05-13-fix-tfsec-aws-vpc-add-description-to-security-group/"},{"title":"kubeadm init で発生したエラー「unknown service runtime.v1alpha2.RuntimeService」対応","text":"ToC 結論 検証環境 kubeadm init 時に以下エラーが発生した際の対処についてまとめます。 1level=fatal msg=&quot;getting status of runtime failed: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService&quot; 結論以下実行することで解決します。 12sudo rm /etc/containerd/config.tomlsudo systemctl restart containerd 以下参考 Kubeadm unknown service runtime.v1alpha2.RuntimeService · Issue #4581 · containerd/containerdProblem Following Kubernetes official installation instruction for containerd and kubeadm init will fail with unknown service runtime.v1alph… 検証環境 kubeadm 1.22.7-00 kubelet 1.22.7-00 kubectl 1.22.7-00 以上参考になれば幸いです。","link":"/2022/05/13/2022-05-14-fix-unknown-service-runtime.v1alpha2.runtimeservice/"},{"title":"RaspberryPI に Go をインストールする","text":"RPi に Go をインストールする手順です。2022.05.15 時点最新 1.18.2 をインストールします。 12345678wget https://golang.org/dl/go1.18.2.linux-armv6l.tar.gzsudo tar -C /usr/local -xzf go1.18.2.linux-armv6l.tar.gzecho 'export PATH=$PATH:/usr/local/go/bin' &gt;&gt; ~/.bashrcecho 'export PATH=$HOME/go/bin:$PATH' &gt;&gt; ~/.bashrcsource ~/.bashrc// バージョン確認go version ※ Raspberry PI OS Bullseye 2022.04.04 リリースで検証しました。","link":"/2022/05/14/2022-05-15-install-golang-on-raspberrypios/"},{"title":"k8s リソースをディスプレイに表示する","text":"ToC 手順 Pod metrics-server 追加 sampler ビルド/実行 いざ sampler 起動 ちょっとハマりポイント 参考 おうち k8s 構築の続きです。 おうち k8s 構築ToC 目的 購入したものリスト Raspberry Pi Imager で OS 書き込み cgroup の有効化 swap 無効化 IP 固定 Docker インストール Cgroup Driver が systemd を使用するように設定されていることを確認 k… k8s cluster 各ノードの CPU, Memory を取得し、ディスプレイに表示します。 手順Pod metrics-server 追加12345678910111213141516171819202122232425262728// 2022.05.14 時点最新 commit abacf42babf4b4f623e992ff65761cd3902d0994 を参照しています。$ wget https://github.com/kubernetes-sigs/metrics-server/releases/download/metrics-server-helm-chart-3.8.2/components.yaml -O metrics-server-components.yaml$ vim metrics-server-components.yaml// 以下編集 spec: containers: - args: ... - --kubelet-insecure-tls # 追加$ kubectl apply -f metrics-server-components.yaml// metrics-server 関連 pod 起動確認$ kubectl get pod metrics-server -n kube-system...// 以下のように表示されれば OKmetrics-server-8bb87844c-jvfnz 1/1 Running 0 31s// k8s cluster 各ノードの CPU/Memory 表示$ kubectl top nodeNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%pikube01 466m 11% 1309Mi 35% # master nodepikube02 171m 4% 1615Mi 43%pikube03 576m 14% 1525Mi 40% sampler ビルド/実行RPi の master node (pikube01) で以下作業を進めます。 RPi に Go をインストールする必要があります。以下参考まで RaspberryPI に Go をインストールするRPi に Go をインストールする手順です。2022.05.15 時点最新 1.18.2 をインストールします。 12345678wget https:&amp;#x2F;&amp;#x2F;golang.org&amp;#x2F;dl&amp;#x2F;go1.18.2.linux-armv6l.tar.g… 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990// see: https://github.com/greghesp/assistant-relay/issues/49$ sudo apt-get install libasound2-dev$ git clone https://github.com/sqshq/sampler$ cd sampler$ GOOS=linux GOARCH=arm GOARM=7 go build$ sudo mv sampler /usr/bin$ cd $HOME$ cat &lt;&lt;'EOF'&gt; sampler-config.ymlgauges: - title: pikube01 CPU position: [[0, 0], [40, 6]] rate-ms: 30000 color: 10 percent-only: true cur: sample: cat /tmp/kube-node | grep pikube01 | awk '{print $3}' | tr -d \"%\" max: sample: echo 100 min: sample: echo 0 - title: pikube02 CPU position: [[0, 7], [40, 6]] rate-ms: 30000 color: 13 percent-only: true cur: sample: cat /tmp/kube-node | grep pikube02 | awk '{print $3}' | tr -d \"%\" max: sample: echo 100 min: sample: echo 0 - title: pikube03 CPU position: [[0, 13], [40, 6]] rate-ms: 30000 color: 14 percent-only: true cur: sample: cat /tmp/kube-node | grep pikube03 | awk '{print $3}' | tr -d \"%\" max: sample: echo 100 min: sample: echo 0 - title: pikube01 Mem position: [[40, 0], [40, 6]] rate-ms: 30000 color: 10 cur: sample: cat /tmp/kube-node | grep pikube01 | awk '{print $4}' | tr -d \"Mi\" max: sample: echo 4096 min: sample: echo 0 - title: pikube02 Mem position: [[40, 7], [40, 6]] rate-ms: 30000 color: 13 cur: sample: cat /tmp/kube-node | grep pikube02 | awk '{print $4}' | tr -d \"Mi\" max: sample: echo 4096 min: sample: echo 0 - title: pikube03 Mem position: [[40, 13], [40, 6]] rate-ms: 30000 color: 14 cur: sample: cat /tmp/kube-node | grep pikube03 | awk '{print $4}' | tr -d \"Mi\" max: sample: echo 4096 min: sample: echo 0textboxes: - title: Status position: [[0, 19], [80, 23]] rate-ms: 30000 sample: &gt;- kubectl top node &gt; /tmp/kube-node; kubectl get all --all-namespaces &gt; /tmp/kube-all; echo \"Pod:$(cat /tmp/kube-all | grep pod/ | grep 'Running' | wc -l)\" \"Service:$(cat /tmp/kube-all | grep service/ | wc -l)\" \"Daemonset:$(cat /tmp/kube-all | grep daemonset.apps/ | wc -l)\" \"Deployment:$(cat /tmp/kube-all | grep deployment.apps/ | wc -l)\" \"Replicaset:$(cat /tmp/kube-all | grep replicaset.apps/ | wc -l)\"; echo \"\"; echo \"Service\"; kubectl get svc --no-headers | grep -v ClusterIP | awk '{print $1, $4, $5}' | column -t;EOF いざ sampler 起動master node にキーボードを直接接続し、コンソール上で sampler を起動します。 1sampler -c sampler-config.yml かっこいい！ ちょっとハマりポイント Quimat 3.5インチタッチスクリーン HDMIモニタTFT LCDディスプレイ &nbsp; 楽天市場でチェック Amazonでチェック ディスプレイに 「QUIMAT 3.5 インチタッチスクリーン HDMI モニタ TFT LCD ディスプレイ」を使いましたが、HDMI 接続で解像度の調整が難しく、手を焼きました。 /boot/config.txt の修正をして再起動したら、 RPi が起動しなくなったり。。。 ssh もできなくなり、RPi にキーボード接続しても操作できなくなり、強制停止（電源引っこ抜き）し、 SD カードを別マシンで読み込んで /boot/config.txt を元に戻して事なきを得ました。 RPi4B の /boot/config.txt は 公式ドキュメント を参考に最終的に以下のような編集をしました。 1234567891011121314151617181920212223242526// 以下追加# コンソールフレームバッファの幅framebuffer_width=480# コンソールフレームバッファの高さframebuffer_height=320# 480x320 の解像度の設定がない為、カスタムCVTモードを定義する# hdmi_cvt=&lt;width&gt; &lt;height&gt; &lt;framerate&gt; &lt;aspect&gt; &lt;margins&gt; &lt;interlace&gt; &lt;rb&gt;hdmi_cvt=480 320 60 6 0 0 0# HDMI が接続されていない状態で RPi を起動するとコンポジットに切り替わるのを防ぐ# ディスプレイを HDMI に接続すると表示される様にするhdmi_force_hotplug=1# DMT (Display Monitor Timings、通常モニターで使用される規格) に設定# 今回利用するディスプレイに対応hdmi_group=2# hdmi_group=2 にない hdmi_mode でカスタムモードを利用したい場合に設定するhdmi_mode=87# 通常の HDMI モードhdmi_drive=2# 以下コメントアウト: この設定があると解像度が変わってしまい、文字サイズが小さくなってしまう#dtoverlay=vc4-kms-v3d#max_framebuffers=2 以上参考になれば幸いです。 参考https://qiita.com/reireias/items/0d87de18f43f27a8ed9b","link":"/2022/05/17/2022-05-18-k8s-monitoring-metric/"},{"title":"black「ImportError: cannot import name _unicodefun from click」 エラー対応","text":"python の自動整形ツール black で以下エラーが発生した為、その時の対応をまとめます。 123456$ black . --check --skip-string-normalizationTraceback (most recent call last):...ImportError: cannot import name '_unicodefun' from 'click' ...... 以下参考に black 最新バージョン 22.3.0 にアップグレードすることで対応できたことを確認しました。 根本原因は black の内部モジュールが click (自身の環境ではバージョン 8.1.3) をインストールしており、そのバグによるものです。 以上参考になれば幸いです。","link":"/2022/05/24/2022-05-25-black-fix-cannot-import-name-unicodefun-from-click/"},{"title":"PrivacyPolicy","text":"個人情報の利用目的アクセス解析ツールについて当サイトでは、Googleによるアクセス解析ツール「Googleアナリティクス」を利用しております。このGoogleアナリティクスはトラフィックデータの収集のためにCookieを使用しています。このトラフィックデータは匿名で収集されており、個人を特定するものではありません。この機能はCookieを無効にすることで収集を拒否することが出来ますので、お使いのブラウザの設定をご確認ください。この規約に関して、詳しくはこちらをご確認ください。 免責事項当サイトのコンテンツ・情報につきまして、可能な限り正確な情報を掲載するよう努めておりますが、必ずしも正確性・信頼性等を保証するものではありません。 当サイトに掲載された内容によって生じた損害等の一切の責任を負いかねますのでご了承ください。当サイトからリンクやバナーなどによって他のサイトに移動された場合、移動先サイトで提供される情報、サービス等について一切の責任を負いません。","link":"/2013/12/31/PrivacyPolicy/"},{"title":"AWS ALB&#x2F;CloudFront でのメンテナンス切り替え方法","text":"ToC ALB の場合 var.maintenance_mode をメンテモード切り替えに利用している理由 ALB のデフォルトアクションを 404 にしている理由 CloudFront の場合 デフォルトアクションを allow {} にしている理由 まとめ ALB, CloudFront がインターフェースにある場合のメンテナンス切り替え方法をまとめました。 ALB の場合123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119resource &quot;aws_lb&quot; &quot;app&quot; { ...}resource &quot;aws_lb_target_group&quot; &quot;app&quot; { # CodeDeploy で ECS Service をデプロイする際に blue/green を用意しています。 for_each = toset([&quot;blue&quot;, &quot;green&quot;]) ...}resource &quot;aws_lb_listener&quot; &quot;app&quot; { load_balancer_arn = aws_lb.app.arn ... # デフォルトアクションは 404 エラー default_action { type = &quot;fixed-response&quot; fixed_response { content_type = &quot;text/plain&quot; status_code = &quot;404&quot; } }}# NOTE: メンテナンスモード時は社内 IP のみルーティングするresource &quot;aws_alb_listener_rule&quot; &quot;app_routing_in_maintenance&quot; { count = var.maintenance_mode ? 1 : 0 listener_arn = aws_alb_listener.app.arn priority = 10 action { # ECS Service に紐づけている Target Group ARN を指定する target_group_arn = element(aws_ecs_service.app.load_balancer[*].target_group_arn, 0) type = &quot;forward&quot; } condition { # NOTE: ドメイン名で制御する host_header { values = [ &quot;example.com&quot;, ] } } # 社内 IP のみメンテ時でもアクセスできるようにしておく condition { source_ip { values = office_ips } } # NOTE: CodeDeployによるBlue/Green Deploymentで変更される箇所を無視 lifecycle { ignore_changes = [ action, ] }}# NOTE: メンテナンスモード時は503の固定レスポンスを返す。resource &quot;aws_alb_listener_rule&quot; &quot;app_listener_https_maintenance&quot; { count = var.maintenance_mode ? 1 : 0 listener_arn = aws_alb_listener.app.arn priority = 20 action { type = &quot;fixed-response&quot; fixed_response { status_code = &quot;503&quot; content_type = &quot;application/json&quot; message_body = jsonencode( { code = &quot;service_unavailable&quot; hint = &quot;現在メンテナンス中です。&quot; } ) } } condition { # NOTE: ドメイン名で制御する host_header { values = [ &quot;example.com&quot;, ] } }}resource &quot;aws_lb_listener_rule&quot; &quot;app_listener_https_host&quot; { listener_arn = aws_lb_listener.app_https.arn priority = 100 action { target_group_arn = aws_lb_target_group.app[&quot;blue&quot;].arn type = &quot;forward&quot; } condition { host_header { # NOTE: ドメイン名で制御する values = [ &quot;example.com&quot;, ] } } # NOTE: CodeDeployによるBlue/Green Deploymentで変更される箇所を無視 lifecycle { ignore_changes = [ action, ] }} メンテモード有効時に社内 IP のみ、サーバへ接続できるようにし、その他リクエストを 503 で返します。 メンテ無効時には上記ルールは削除されます。 メンテ時のレスポンスを application/json で返しているのはネイティブアプリケーションとの通信を想定しています。 text/html 等選択可能です。 参考: https://docs.aws.amazon.com/ja_jp/elasticloadbalancing/latest/application/load-balancer-limits.html var.maintenance_mode をメンテモード切り替えに利用している理由ALB リソースを管理する terraform 外で maintenace_mode を管理するとメンテ切り替えとリソースの更新を別のライフサイクルで管理できるメリットがある為です。 例えば、 以下のような利用想定をしています。 Terraform Cloud 上で mainteance_mode = true にし、 terraform apply を実行しメンテナンスモードにする インフラの変更を terraform で実施する ALB のデフォルトアクションを 404 にしている理由メンテモードの話と逸れますが、指定ドメイン以外のリクエストを許可しないようにしたい為です。 IP 直指定や ALB ドメイン名指定のアクセスを許可してるとセキュリティ上も SEO 上もよろしくない認識です。 CloudFront の場合1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859resource &quot;aws_wafv2_web_acl&quot; &quot;cloudfront_app&quot; { ... scope = &quot;CLOUDFRONT&quot; custom_response_body { content = jsonencode( { code = &quot;service_unavailable&quot; hint = &quot;現在システムのメンテナンス中です。&quot; } ) content_type = &quot;APPLICATION_JSON&quot; key = &quot;maintenance&quot; } default_action { allow {} } # アクセス許可 IP にマッチしない場合 block する dynamic &quot;rule&quot; { for_each = var.maintenance_mode ? [&quot;1&quot;] : [] content { ... priority = 10 # ブロック時にメンテナンス時のレスポンスを返す action { block { custom_response { custom_response_body_key = &quot;maintenance&quot; response_code = 503 } } } statement { not_statement { statement { ip_set_reference_statement { arn = aws_wafv2_ip_set.allow_ips.arn } } } } ... } } ...}resource &quot;aws_cloudfront_distribution&quot; &quot;app&quot; { ... web_acl_id = aws_wafv2_web_acl.cloudfront_app.arn ...} CloudFront にアタッチする WAF でリクエスト許可 IP 以外は 503 メンテ用レスポンスを返します。 ※ custom_response_body は、terraform-provider-aws&gt;=3.67.0 でサポートしています。 デフォルトアクションを allow {} にしている理由12345678910111213141516default_action { allow {}}dynamic &quot;rule&quot; { ... content { statement { not_statement { statement { ip_set_reference_statement { arn = aws_wafv2_ip_set.allow_ips.arn } } } } メンテモードの話とは逸れますが、本番環境・ステージング環境でデフォルトのアクションを統一したい為です。 時折見受けられるのは以下の様な設定 本番環境 → default_action { allow {} } ステージング環境 → default_action { block {} } デフォルトアクションが環境毎に異なることで追加したルールによってリクエストが意図せずにブロックしたり・許可してしまったりする事故が起きる可能性があり、その防止策の一環として統一しておくほうが良いという判断です。 まとめそれぞれのリソースの特性を活かしたメンテ方法が実装できました。 ALB はリスナールール CloudFront は WAF メンテページをよりリッチにしたい場合は S3 にルーティングしても良いと思います。 以上参考になれば幸いです。","link":"/2022/03/12/2022-03-13-maintenance-mode-for-alb-or-cloudfront/"},{"title":"terraform で CloudFront マネージドプリフィックスリストを利用したアクセス制限","text":"ToC 早速 terraform で実装してみた CloudFront のプレフィックスリストの取得方法について よりセキュアに 総評 https://aws.amazon.com/jp/about-aws/whats-new/2022/02/amazon-cloudfront-managed-prefix-list/ VPC が CloudFront のマネージドプリフィックスリストをサポートするようになりました。これにより CloudFront からのアクセスをセキュリティグループルールベースでアクセス制限できるようになります。 早速 terraform で実装してみた以下の構成を想定しています。 1CloudFront--&gt;ALB ALB にアタッチするセキュリティグループで CloudFront のマネージドプリフィックスリスト ID のみ許可するようにします。 12345678910111213141516171819202122# CloudFront のマネージドプリフィックスリスト取得data &quot;aws_ec2_managed_prefix_list&quot; &quot;cloudfront&quot; { name = &quot;com.amazonaws.global.cloudfront.origin-facing&quot;}resource &quot;aws_security_group&quot; &quot;lb_app&quot; { ...}resource &quot;aws_security_group_rule&quot; &quot;lb_app_https&quot; { type = &quot;ingress&quot; from_port = 443 to_port = 443 protocol = &quot;tcp&quot; prefix_list_ids = [data.aws_ec2_managed_prefix_list.cloudfront.id] security_group_id = aws_security_group.lb_app.id}resource &quot;aws_lb&quot; &quot;app&quot; { security_groups = [aws_security_group.lb_app.id] ...} セキュリティグループルールベースで CloudFront からのアクセスを許可できました。 CloudFront のプレフィックスリストの取得方法についてプレフィックスリストの取得方法でどんな実装が正しいのか試行錯誤しました。 結論から言うと data “ec2_managed_prefix_list” を採用しました。 全てのリージョンで com.amazonaws.global.cloudfront.origin-facing で指定されており、リージョン変更に柔軟な構成にしました。 123data &quot;aws_ec2_managed_prefix_list&quot; &quot;cloudfront&quot; { name = &quot;com.amazonaws.global.cloudfront.origin-facing&quot;} 以下 data &quot;aws_prefix_list&quot; で ID 直指定でも取得できましたがリージョン毎に値が異なり、リージョン変更した場合に利用できません。 123data &quot;aws_prefix_list&quot; &quot;cloudfront&quot; { prefix_list_id = &quot;pl-58a04531&quot;} 以下、 name 指定の場合、エラーになりました。 123data &quot;aws_prefix_list&quot; &quot;cloudfront&quot; { name = &quot;com.amazonaws.global.cloudfront.origin-facing&quot;} エラー内容 1Error: no matching prefix list found; the prefix list ID or name may be invalid or not exist in the current region 参考: https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/prefix_list#name Filter 指定する場合、ステップ数も増えます。 シンプルな実装で済む data &quot;aws_ec2_managed_prefix_list&quot; が最適と判断しました。https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/prefix_list#filter よりセキュアにこれは CloudFront のマネージドプリフィックリストでの制御とは話が逸れるところですが、WAF or ALB のリスナールールでドメイン名で制限しておくとよりセキュアです。 1 つのリポジトリで app, admin と双方のコードを書いていて、ルーティングに特に制限をかけていない場合、以下のようにインターネット越しにアクセスできてしまう可能性があります。 1curl -H &quot;Host: admin.example.com&quot; https://example.com/login 上記のような処理を防止すべく、以下処理では、ALB リスナールールでデフォルト処理がメンテページ表示にし指定したドメイン名の場合のみバックエンドへルーティングさせています。 12345678910111213141516171819202122232425262728293031323334353637resource &quot;aws_cloudfront_distribution&quot; &quot;app&quot; { aliases = [ &quot;example.com&quot;, ] ...}resource &quot;aws_lb_listener&quot; &quot;app_https&quot; { load_balancer_arn = aws_lb.app.arn ... # メンテナンス画面 default_action { type = &quot;fixed-response&quot; fixed_response { content_type = &quot;text/html&quot; message_body = file(&quot;${path.module}/files/503.html&quot;) status_code = &quot;503&quot; } } depends_on = [aws_lb_target_group.app]}resource &quot;aws_lb_listener_rule&quot; &quot;app_listener_https_host&quot; { listener_arn = aws_lb_listener.app_https.arn ... condition { host_header { # NOTE: ドメイン名で制御する values = [ &quot;example.com&quot;, ] } }} CloudFront 経由でないとアクセスできないようにする、に加えて、さらにドメイン名によるアクセス制限を加えることでより意図しない CloudFront からのルーティングを回避できます。 総評既存では CloudFront のカスタムヘッダーに値を入れてALB の WAF でその値を照合するような仕組みを採用していましたが、よりシンプルな実装にできました。 実質、セキュリティグループルールベースでの許可となるのでALB → CloudFront + ALB への構成への移行もしやすくなったと感じました。 これは有難いアップデートです ♪ 以上参考になれば幸いです。","link":"/2022/02/09/2022-02-10-cloudfront-managed-prefix/"},{"title":"2022-01-15 昼頃に発生した AWS ElastiCache CPU 使用率の異常上昇について","text":"2022-01-15 11:40 頃 AWS ElastiCache (ap-northeast-1) のいくつかの node で CPU 使用率が 100 % を優に超える値を記録する事象が確認されました。 状況整理し特段ユーザ影響がないことを確認しています。 CPU 使用率上昇時の状況整理 Service Health Dashboard には特に記録なし キャッシュヒット率が一時的に92% → 78% (-14%) 程度下がった アプリケーションの Redis 利用箇所で 5xx エラーはなし Redis を利用する worker のジョブも影響なし エンジン CPU 使用率 (=Redis エンジンスレッドの CPU 使用率) が低いことから Redis の処理自体の影響はないと思われる CPU 使用率 (Redis 以外のホスト全体の CPU 使用率) が高騰したことから、 AWS 側でホストに対する更新があったと思われる この辺の影響だろうか？ https://aws.amazon.com/jp/about-aws/whats-new/2022/01/amazon-elasticache-streaming-storing-redis-engine-logs/ 参考: https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/CacheMetrics.Redis.html TODO: サポート問い合わせし、追記します。 AWS サポートに問い合わせた結果メトリクスのバグとのことでした。（ほっ）","link":"/2022/01/14/2022-01-15-aws-elasticache-redis-cpu-utilization-unnormally-up/"}],"tags":[{"name":"Vagrant","slug":"Vagrant","link":"/tags/Vagrant/"},{"name":"Chef","slug":"Chef","link":"/tags/Chef/"},{"name":"Android","slug":"Android","link":"/tags/Android/"},{"name":"GooglePlay","slug":"GooglePlay","link":"/tags/GooglePlay/"},{"name":"wget","slug":"wget","link":"/tags/wget/"},{"name":"SonarQube","slug":"SonarQube","link":"/tags/SonarQube/"},{"name":"Unity","slug":"Unity","link":"/tags/Unity/"},{"name":"Apache","slug":"Apache","link":"/tags/Apache/"},{"name":"OpenSSL","slug":"OpenSSL","link":"/tags/OpenSSL/"},{"name":"Ruby","slug":"Ruby","link":"/tags/Ruby/"},{"name":"Go","slug":"Go","link":"/tags/Go/"},{"name":"kibana","slug":"kibana","link":"/tags/kibana/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"Fluentd","slug":"Fluentd","link":"/tags/Fluentd/"},{"name":"Nginx","slug":"Nginx","link":"/tags/Nginx/"},{"name":"Slack","slug":"Slack","link":"/tags/Slack/"},{"name":"Kibana","slug":"Kibana","link":"/tags/Kibana/"},{"name":"Elasticsearch","slug":"Elasticsearch","link":"/tags/Elasticsearch/"},{"name":"JavaScript","slug":"JavaScript","link":"/tags/JavaScript/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Monitoring","slug":"Monitoring","link":"/tags/Monitoring/"},{"name":"Twilio","slug":"Twilio","link":"/tags/Twilio/"},{"name":"yum","slug":"yum","link":"/tags/yum/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"macos","slug":"macos","link":"/tags/macos/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"Outlook","slug":"Outlook","link":"/tags/Outlook/"},{"name":"Email","slug":"Email","link":"/tags/Email/"},{"name":"ping","slug":"ping","link":"/tags/ping/"},{"name":"iptable","slug":"iptable","link":"/tags/iptable/"},{"name":"AWS","slug":"AWS","link":"/tags/AWS/"},{"name":"Pacemaker","slug":"Pacemaker","link":"/tags/Pacemaker/"},{"name":"Corosync","slug":"Corosync","link":"/tags/Corosync/"},{"name":"Swift","slug":"Swift","link":"/tags/Swift/"},{"name":"Hubot","slug":"Hubot","link":"/tags/Hubot/"},{"name":"no-ip","slug":"no-ip","link":"/tags/no-ip/"},{"name":"Security","slug":"Security","link":"/tags/Security/"},{"name":"AntiVirus","slug":"AntiVirus","link":"/tags/AntiVirus/"},{"name":"PHP","slug":"PHP","link":"/tags/PHP/"},{"name":"SSL","slug":"SSL","link":"/tags/SSL/"},{"name":"StatsBot","slug":"StatsBot","link":"/tags/StatsBot/"},{"name":"ipinfo","slug":"ipinfo","link":"/tags/ipinfo/"},{"name":"awk","slug":"awk","link":"/tags/awk/"},{"name":"csv","slug":"csv","link":"/tags/csv/"},{"name":".htaccess","slug":"htaccess","link":"/tags/htaccess/"},{"name":"ssh","slug":"ssh","link":"/tags/ssh/"},{"name":"Zabbix","slug":"Zabbix","link":"/tags/Zabbix/"},{"name":"sftp","slug":"sftp","link":"/tags/sftp/"},{"name":"Jenkins","slug":"Jenkins","link":"/tags/Jenkins/"},{"name":"Prometheus","slug":"Prometheus","link":"/tags/Prometheus/"},{"name":"Ansible","slug":"Ansible","link":"/tags/Ansible/"},{"name":"pip","slug":"pip","link":"/tags/pip/"},{"name":"RaspberryPI","slug":"RaspberryPI","link":"/tags/RaspberryPI/"},{"name":"Reactio","slug":"Reactio","link":"/tags/Reactio/"},{"name":"MachineLearning","slug":"MachineLearning","link":"/tags/MachineLearning/"},{"name":"Cookie","slug":"Cookie","link":"/tags/Cookie/"},{"name":"LINE Notify","slug":"LINE-Notify","link":"/tags/LINE-Notify/"},{"name":"Let&#39;s encrypt","slug":"Let-s-encrypt","link":"/tags/Let-s-encrypt/"},{"name":"vim","slug":"vim","link":"/tags/vim/"},{"name":"zsh","slug":"zsh","link":"/tags/zsh/"},{"name":"spam","slug":"spam","link":"/tags/spam/"},{"name":"Terraform","slug":"Terraform","link":"/tags/Terraform/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"GKE","slug":"GKE","link":"/tags/GKE/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"WAF","slug":"WAF","link":"/tags/WAF/"},{"name":"casperjs","slug":"casperjs","link":"/tags/casperjs/"},{"name":"flask","slug":"flask","link":"/tags/flask/"},{"name":"iftop","slug":"iftop","link":"/tags/iftop/"},{"name":"Datadog","slug":"Datadog","link":"/tags/Datadog/"},{"name":"Rails","slug":"Rails","link":"/tags/Rails/"},{"name":"ECR","slug":"ECR","link":"/tags/ECR/"},{"name":"FTPS","slug":"FTPS","link":"/tags/FTPS/"},{"name":"Vault","slug":"Vault","link":"/tags/Vault/"},{"name":"ECS","slug":"ECS","link":"/tags/ECS/"},{"name":"ElastiCache","slug":"ElastiCache","link":"/tags/ElastiCache/"},{"name":"cpu","slug":"cpu","link":"/tags/cpu/"},{"name":"S3","slug":"S3","link":"/tags/S3/"},{"name":"ALB","slug":"ALB","link":"/tags/ALB/"},{"name":"i-node","slug":"i-node","link":"/tags/i-node/"},{"name":"reCAPTCHA","slug":"reCAPTCHA","link":"/tags/reCAPTCHA/"},{"name":"SendGrid","slug":"SendGrid","link":"/tags/SendGrid/"},{"name":"ProxySQL","slug":"ProxySQL","link":"/tags/ProxySQL/"},{"name":"Puppeteer","slug":"Puppeteer","link":"/tags/Puppeteer/"},{"name":"GitHub Actions","slug":"GitHub-Actions","link":"/tags/GitHub-Actions/"},{"name":"Lambda","slug":"Lambda","link":"/tags/Lambda/"},{"name":"CodeBuild","slug":"CodeBuild","link":"/tags/CodeBuild/"}],"categories":[{"name":"Go","slug":"Go","link":"/categories/Go/"},{"name":"Terraform","slug":"Terraform","link":"/categories/Terraform/"},{"name":"AWS","slug":"AWS","link":"/categories/AWS/"},{"name":"RaspberryPI","slug":"RaspberryPI","link":"/categories/RaspberryPI/"},{"name":"DIY","slug":"DIY","link":"/categories/DIY/"},{"name":"AWS","slug":"Terraform/AWS","link":"/categories/Terraform/AWS/"}]}